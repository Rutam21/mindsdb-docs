{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","social":[{"link":"https://github.com/mindsdb/mindsdb","type":"github"},{"link":"https://twitter.com/mindsdb","type":"twitter"},{"link":"https://www.mindsdb.com","type":"link"}]},"docs":[{"location":"","text":"What is MindsDB? MindsDB enables advanced predictive capabilities directly in your Database. This puts sophisticated machine learning techniques into the hands of anyone who knows SQL (data analysts, developers and business intelligence users) without the need for a new tool or significant training. Data is the single most important ingredient in machine learning, and your data lives in a database. So why do machine learning anywhere else? The Vision: A world where people use intelligent databases to make better data-driven decisions. ...and we believe the best way to make these decisions is to enable this capability within existing databases without the effort of creating a new one. (Despite the name we are not actually a database!) From Database Tables to Machine Learning Models As a quick example, let's consider a database that stores the SqFt and Price of Home Rentals: SELECT sqft , price FROM home_rentals_table ; You query the database for information in this table and if your search criteria generates a match: you get results: SELECT sqft , price FROM home_rentals_table WHERE sqft = 900 ; If your search criteria do not generate a match - your results will be empty: SELECT sqft , price FROM home_rentals_table WHERE sqft = 800 ; An ML model can be fitted to the data in the home rentals table. CREATE PREDICTOR home_rentals_model FROM home_rentals_table PREDICT price ; An ML model can provide approximate answers for searches where there is no exact match in the income table: SELECT sqft , price FROM home_rentals_model WHERE sqft = 800 ; Getting Started To start using MindsDB, check out our Getting Started Guide","title":"What is MindsDB"},{"location":"#what-is-mindsdb","text":"MindsDB enables advanced predictive capabilities directly in your Database. This puts sophisticated machine learning techniques into the hands of anyone who knows SQL (data analysts, developers and business intelligence users) without the need for a new tool or significant training. Data is the single most important ingredient in machine learning, and your data lives in a database. So why do machine learning anywhere else?","title":"What is MindsDB?"},{"location":"#the-vision","text":"A world where people use intelligent databases to make better data-driven decisions. ...and we believe the best way to make these decisions is to enable this capability within existing databases without the effort of creating a new one. (Despite the name we are not actually a database!)","title":"The Vision:"},{"location":"#from-database-tables-to-machine-learning-models","text":"As a quick example, let's consider a database that stores the SqFt and Price of Home Rentals: SELECT sqft , price FROM home_rentals_table ; You query the database for information in this table and if your search criteria generates a match: you get results: SELECT sqft , price FROM home_rentals_table WHERE sqft = 900 ; If your search criteria do not generate a match - your results will be empty: SELECT sqft , price FROM home_rentals_table WHERE sqft = 800 ; An ML model can be fitted to the data in the home rentals table. CREATE PREDICTOR home_rentals_model FROM home_rentals_table PREDICT price ; An ML model can provide approximate answers for searches where there is no exact match in the income table: SELECT sqft , price FROM home_rentals_model WHERE sqft = 800 ;","title":"From Database Tables to Machine Learning Models"},{"location":"#getting-started","text":"To start using MindsDB, check out our Getting Started Guide","title":"Getting Started"},{"location":"FunctionalInterface/","text":"This section goes into detail about each of the methods exposed by Functional and each of the arguments they work with. All of the Functional methods act as utilities for Predictors. Get Models F.get_models() Takes no argument, returns a list with all the models and some information about them. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet Get Model Data F.get_model_data(model_name='model_name') Returns all the data we have about a given model. This is a rather complex python dictionary meant to be interpreted by the Scout GUI. We recommend looking at the training logs mindsdb_native gives to see some of these insights in an easy to read format. model_name -- Required argument, the name of the model to return data about. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet Export Model F.export_predictor(model_name='model_name') Exports this predictor's data (or the data of another predictor) to a zip file inside the CONFIG.MINDSDB_STORAGE_PATH directory. model_name -- The name of the model to export (defaults to the name of the current Predictor). Rename Model F.rename_model(old_model_name='old_name', new_model_name='new_name') Renames the created model. old_model_name: the name of the model you want to rename new_model_name: the new name of the model Export Storage F.export_storage(mindsdb_storage_dir='mindsdb_storage') Exports mindsdb's storage directory to a zip file. mindsdb_storage_dir -- The location where you want to save the mindsdb storage directory. Load Model F.import_model(model_archive_path='path/to/predictor.zip') Loads a predictor that was previously exported into the current mindsdb_native storage path so you can use it later. model_archive_path -- full_path that contains your mindsdb_native predictor zip file. Delete Model F.delete_model(model_name='blah') Deletes a given predictor from the storage path mindsdb_native is currently operating with. model_name -- The name of the model to delete (defaults to the name of the current Predictor). Analyse dataset F.analyse_dataset(from_data=the_data_source, sample_settings={}) Analyse the dataset inside the data source, file, ulr or pandas dataframe. This runs all the steps prior to actually training a predictive model. from_data -- the data that you want to analyse, this can be either a file, a pandas data frame, a url or a mindsdb data source. sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function .","title":"Functional interface"},{"location":"FunctionalInterface/#get-models","text":"F.get_models() Takes no argument, returns a list with all the models and some information about them. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet","title":"Get Models"},{"location":"FunctionalInterface/#get-model-data","text":"F.get_model_data(model_name='model_name') Returns all the data we have about a given model. This is a rather complex python dictionary meant to be interpreted by the Scout GUI. We recommend looking at the training logs mindsdb_native gives to see some of these insights in an easy to read format. model_name -- Required argument, the name of the model to return data about. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet","title":"Get Model Data"},{"location":"FunctionalInterface/#export-model","text":"F.export_predictor(model_name='model_name') Exports this predictor's data (or the data of another predictor) to a zip file inside the CONFIG.MINDSDB_STORAGE_PATH directory. model_name -- The name of the model to export (defaults to the name of the current Predictor).","title":"Export Model"},{"location":"FunctionalInterface/#rename-model","text":"F.rename_model(old_model_name='old_name', new_model_name='new_name') Renames the created model. old_model_name: the name of the model you want to rename new_model_name: the new name of the model","title":"Rename Model"},{"location":"FunctionalInterface/#export-storage","text":"F.export_storage(mindsdb_storage_dir='mindsdb_storage') Exports mindsdb's storage directory to a zip file. mindsdb_storage_dir -- The location where you want to save the mindsdb storage directory.","title":"Export Storage"},{"location":"FunctionalInterface/#load-model","text":"F.import_model(model_archive_path='path/to/predictor.zip') Loads a predictor that was previously exported into the current mindsdb_native storage path so you can use it later. model_archive_path -- full_path that contains your mindsdb_native predictor zip file.","title":"Load Model"},{"location":"FunctionalInterface/#delete-model","text":"F.delete_model(model_name='blah') Deletes a given predictor from the storage path mindsdb_native is currently operating with. model_name -- The name of the model to delete (defaults to the name of the current Predictor).","title":"Delete Model"},{"location":"FunctionalInterface/#analyse-dataset","text":"F.analyse_dataset(from_data=the_data_source, sample_settings={}) Analyse the dataset inside the data source, file, ulr or pandas dataframe. This runs all the steps prior to actually training a predictive model. from_data -- the data that you want to analyse, this can be either a file, a pandas data frame, a url or a mindsdb data source. sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function .","title":"Analyse dataset"},{"location":"InsideMindsDB/","text":"Different transactions PREDICT, CREATE MODEL etc, require different steps/phases, however they may share some of these phases, in order to make this process modular we keep the variables in the Transaction controller (the data bus) as the communication interface, as such, the implementation of a given phase can change, so long as the expected variables in the bus prevail. (We will describe in more detail some of the Phase Modules in the next section). The MindsDB Stack DataExtractor It deals with extracting inputs from various data-sources such as files, directories and SQL compatible databases. If input is a query, it builds the joins with all implied tables (if any). StatsLoader : There is some transaction such as PREDICT where it's assumed that the statistical information is already known, all we have to do is make sure we load the right statistics to the transaction BUS. At the moment we don't support loading database from {char}svs that don't have headers or have incomplete headers. NOTE : That as of now mindsDB requires that the full dataset can be loaded into memory, in the future we might look into supporting very large datasets using something like apache drill to query a FS or db for the chunks of data we need in order to train and generate our statistical analysis . StatsGenerator Once the data is pulled and aggregated from the various data sources, MindsDB runs an analysis of each of the columns of the corpus. The purpose of the stats generator is two fold: To provide various data quality scores in order to determine the overall quality of a column (e.g. variance, some correlation metrics between columns, amount of duplicates). To provide properties about the columns which have to be used in the following steps and in order to rain the model. (e.g. histogram, data type) After all stats are computed, we warn the user of any interesting insights we found about his data and (if web logs are enabled), use the generated values to plot some interesting information about the data (e.g. data type distribution, outliers, histogram). Finally, the various stats are passed on as part of the metadata, so that further phases and the model itself can use them. Model Interface Train mode : When calling learn ,the model interface will feed the data to a machine learning framework which does the training in order to build a model. Predict mode : When calling predict , the model interface will feed the data to the model built by learn in order to generate a prediction. Data adaption : The ModelInterface phase is simply a lightweight wrapper over the model backends which handle adapting the data frame used by mindsdb into a format they can work with. During this process additional metadata for the machine learning libraries/frameworks is generated based on the results of the Stats Generator phase. Learning backend : The learning backends as the ensemble learning libraries used by mindsdb to train the model that will generate the predictions. Currently the learning backend we are working on supporting is Lightwood (created by us, based on the pre 1.0 version of mindsdb, work in progress). ModelAnalyzer The model analyzer phase runs after training is done in order to gather insights about the model and gather insights about the data that we can only get post-training. At the moment, it contains the fitting for a probabilistic model which is used to determine the accuracy of future prediction, based on the number of missing features and the bucket in which the predicted value falls.","title":"Inside MindsdDB"},{"location":"InsideMindsDB/#the-mindsdb-stack","text":"","title":"The MindsDB Stack"},{"location":"InsideMindsDB/#dataextractor","text":"It deals with extracting inputs from various data-sources such as files, directories and SQL compatible databases. If input is a query, it builds the joins with all implied tables (if any). StatsLoader : There is some transaction such as PREDICT where it's assumed that the statistical information is already known, all we have to do is make sure we load the right statistics to the transaction BUS. At the moment we don't support loading database from {char}svs that don't have headers or have incomplete headers. NOTE : That as of now mindsDB requires that the full dataset can be loaded into memory, in the future we might look into supporting very large datasets using something like apache drill to query a FS or db for the chunks of data we need in order to train and generate our statistical analysis .","title":"DataExtractor"},{"location":"InsideMindsDB/#statsgenerator","text":"Once the data is pulled and aggregated from the various data sources, MindsDB runs an analysis of each of the columns of the corpus. The purpose of the stats generator is two fold: To provide various data quality scores in order to determine the overall quality of a column (e.g. variance, some correlation metrics between columns, amount of duplicates). To provide properties about the columns which have to be used in the following steps and in order to rain the model. (e.g. histogram, data type) After all stats are computed, we warn the user of any interesting insights we found about his data and (if web logs are enabled), use the generated values to plot some interesting information about the data (e.g. data type distribution, outliers, histogram). Finally, the various stats are passed on as part of the metadata, so that further phases and the model itself can use them.","title":"StatsGenerator"},{"location":"InsideMindsDB/#model-interface","text":"Train mode : When calling learn ,the model interface will feed the data to a machine learning framework which does the training in order to build a model. Predict mode : When calling predict , the model interface will feed the data to the model built by learn in order to generate a prediction. Data adaption : The ModelInterface phase is simply a lightweight wrapper over the model backends which handle adapting the data frame used by mindsdb into a format they can work with. During this process additional metadata for the machine learning libraries/frameworks is generated based on the results of the Stats Generator phase. Learning backend : The learning backends as the ensemble learning libraries used by mindsdb to train the model that will generate the predictions. Currently the learning backend we are working on supporting is Lightwood (created by us, based on the pre 1.0 version of mindsdb, work in progress).","title":"Model Interface"},{"location":"InsideMindsDB/#modelanalyzer","text":"The model analyzer phase runs after training is done in order to gather insights about the model and gather insights about the data that we can only get post-training. At the moment, it contains the fitting for a probabilistic model which is used to determine the accuracy of future prediction, based on the number of missing features and the bucket in which the predicted value falls.","title":"ModelAnalyzer"},{"location":"PredictorInterface/","text":"This section goes into detail about each of the methods exposed by Predictor and each of the arguments they work with. Predictor Note: The Predictor in MindsDB's words means Machine Learning Model. Constructor predictor = Predictor(name='weather_forecast') Constructs a new mindsdb predictor name -- Required argument, the name of the predictor, used for saving the predictor, creating a new predictor with the same name as a previous one loads the data from the old predictor root_folder -- The directory (also known as folder) where the predictor information should be saved log_level -- The log level that the predictor should use, number from 0 to 50, with 0 meaning log everything and 50 meaning log only fatal errors: DEBUG_LOG_LEVEL = 10 INFO_LOG_LEVEL = 20 WARNING_LOG_LEVEL = 30 ERROR_LOG_LEVEL = 40 NO_LOGS_LOG_LEVEL = 50 Learn predictor.learn(from_data=a_data_source, to_predict='a_column') Teach the predictor to make predictions on a given dataset, extract information about the dataset and extract information about the resulting machine learning model making the predictions. This is the \"main\" functionality of mindsdb_native together with the \"predict\" method. from_data -- the data that you want to use for training, this can be either a file, a pandas data frame, a url or a mindsdb data source. to_predict -- The columns/keys to be predicted (aka the targets, output columns, target variables), can be either a string (when specifying a single column) or a list of strings (when specifying multiple columns). test_from_data -- Specify a different data source on which mindsdb should test the machine learning model, by default mindsdb_native takes testing and validation samples from your dataset to use during training and analysis, and only trains the predictive model on ~80% of the data. This might seem sub-optimal if you're not used to machine learning, but trust us, it allows us to have much better confidence in the model we give you. timeseries specific parameters : group_by, window_size, order_by -- For more information on how to use these, please see the advanced examples section dealing with timeseries . Please note, these are currently subject to change, though they will remain backwards compatible until v2.0. ignore_columns -- Ignore certain columns from your data entirely. stop_training_in_x_seconds -- Stop training the model after this amount of seconds, note, the full amount it takes for mindsdb_native to run might be up to twice the amount you specify in this value. Thought, usually, the model training constitutes ~70-90% of the total mindsdb_native runtime. stop_training_in_accuracy -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. rebuild_model -- Defaults to True , if this is set to False the model will be loaded and the model analysis and data analysis will be re-run, but a new model won't be trained. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. equal_accuracy_for_all_output_categories -- When you have unbalanced target variable values, this will treat all of them as equally important when training the model. To see more information about this and an example, please see advanced section . output_categories_importance_dictionary -- A dictionary containing a number representing the importance for each (or some) values from the column to be predicted. An example of how his can be used (assume the column we are predicting is called is_true and takes two falues): {'is_true': {'True':0.6, 'False':1}} . The bigger the number (maximum value is one), the more important will it be for the model to predict that specific value correctly (usually at the cost of predicting other values correctly and getting more false positives for that value). advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache , force_categorical_encoding , handle_foreign_keys , use_selfaware_model , deduplicate_data . sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function . If you are interested in using advanced_args or sample_settings but you are unsure of how they work, please shot us an email or create a github issue and we will help you. Predict predict(self, when_data = None, update_cached_model = False, use_gpu=False, advanced_args={}, backend=None, run_confidence_variation_analysis=False): predictor.predict(from_data=the_data_source) Make a prediction about a given dataset. when_data -- the data that you want to make the predictions for, this can be either a file, a pandas data frame, a url, a dictionary used for single prediction(column name: value) or a mindsdb data source. update_cached_model -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache . backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. Note, you shouldn't use a different backend than the one you used to train the model, this will result in undefined behavior in the worst case scenario and most likely lead to a weird error. This defaults to whatever backend was last used when calling learn on this predictor. run_confidence_variation_analysis -- Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via when_data . It provides some more in-depth analysis of a given prediction, by specifying how the confidence would increase/decrease based on which of the columns in the prediction were not present (had null, None or empty values). Test predictor.test(when_data=data, accuracy_score_functions='r2_score', score_using='predicted_value', predict_args={'use_gpu': True}): Test the overall confidence of the predictor e.g {'rental_price_accuracy': 0.95}. when_data -- Use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from. accuracy_score_functions -- A single function or a dictionary for the form {f'{target_name}': acc_func} when multiple targets are used. score_using -- What values from the explanation of the target to be used in the score function. predict_args -- Dictionary of arguments to be passed to predict (same arguments that predict accepts), e.g: predict_args={'use_gpu': True} . Predictor Quality Prediction Quality DataSources Mindsdb exposes a number of data sources that can be used with the predictor, you can find more details in the datasources section . Constants and Configuration For the constants and configuration options exposed by mindsdb_native at large please refer to this section .","title":"Predictor interface"},{"location":"PredictorInterface/#predictor","text":"Note: The Predictor in MindsDB's words means Machine Learning Model.","title":"Predictor"},{"location":"PredictorInterface/#constructor","text":"predictor = Predictor(name='weather_forecast') Constructs a new mindsdb predictor name -- Required argument, the name of the predictor, used for saving the predictor, creating a new predictor with the same name as a previous one loads the data from the old predictor root_folder -- The directory (also known as folder) where the predictor information should be saved log_level -- The log level that the predictor should use, number from 0 to 50, with 0 meaning log everything and 50 meaning log only fatal errors: DEBUG_LOG_LEVEL = 10 INFO_LOG_LEVEL = 20 WARNING_LOG_LEVEL = 30 ERROR_LOG_LEVEL = 40 NO_LOGS_LOG_LEVEL = 50","title":"Constructor"},{"location":"PredictorInterface/#learn","text":"predictor.learn(from_data=a_data_source, to_predict='a_column') Teach the predictor to make predictions on a given dataset, extract information about the dataset and extract information about the resulting machine learning model making the predictions. This is the \"main\" functionality of mindsdb_native together with the \"predict\" method. from_data -- the data that you want to use for training, this can be either a file, a pandas data frame, a url or a mindsdb data source. to_predict -- The columns/keys to be predicted (aka the targets, output columns, target variables), can be either a string (when specifying a single column) or a list of strings (when specifying multiple columns). test_from_data -- Specify a different data source on which mindsdb should test the machine learning model, by default mindsdb_native takes testing and validation samples from your dataset to use during training and analysis, and only trains the predictive model on ~80% of the data. This might seem sub-optimal if you're not used to machine learning, but trust us, it allows us to have much better confidence in the model we give you. timeseries specific parameters : group_by, window_size, order_by -- For more information on how to use these, please see the advanced examples section dealing with timeseries . Please note, these are currently subject to change, though they will remain backwards compatible until v2.0. ignore_columns -- Ignore certain columns from your data entirely. stop_training_in_x_seconds -- Stop training the model after this amount of seconds, note, the full amount it takes for mindsdb_native to run might be up to twice the amount you specify in this value. Thought, usually, the model training constitutes ~70-90% of the total mindsdb_native runtime. stop_training_in_accuracy -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. rebuild_model -- Defaults to True , if this is set to False the model will be loaded and the model analysis and data analysis will be re-run, but a new model won't be trained. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. equal_accuracy_for_all_output_categories -- When you have unbalanced target variable values, this will treat all of them as equally important when training the model. To see more information about this and an example, please see advanced section . output_categories_importance_dictionary -- A dictionary containing a number representing the importance for each (or some) values from the column to be predicted. An example of how his can be used (assume the column we are predicting is called is_true and takes two falues): {'is_true': {'True':0.6, 'False':1}} . The bigger the number (maximum value is one), the more important will it be for the model to predict that specific value correctly (usually at the cost of predicting other values correctly and getting more false positives for that value). advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache , force_categorical_encoding , handle_foreign_keys , use_selfaware_model , deduplicate_data . sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function . If you are interested in using advanced_args or sample_settings but you are unsure of how they work, please shot us an email or create a github issue and we will help you.","title":"Learn"},{"location":"PredictorInterface/#predict","text":"predict(self, when_data = None, update_cached_model = False, use_gpu=False, advanced_args={}, backend=None, run_confidence_variation_analysis=False): predictor.predict(from_data=the_data_source) Make a prediction about a given dataset. when_data -- the data that you want to make the predictions for, this can be either a file, a pandas data frame, a url, a dictionary used for single prediction(column name: value) or a mindsdb data source. update_cached_model -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache . backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. Note, you shouldn't use a different backend than the one you used to train the model, this will result in undefined behavior in the worst case scenario and most likely lead to a weird error. This defaults to whatever backend was last used when calling learn on this predictor. run_confidence_variation_analysis -- Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via when_data . It provides some more in-depth analysis of a given prediction, by specifying how the confidence would increase/decrease based on which of the columns in the prediction were not present (had null, None or empty values).","title":"Predict"},{"location":"PredictorInterface/#test","text":"predictor.test(when_data=data, accuracy_score_functions='r2_score', score_using='predicted_value', predict_args={'use_gpu': True}): Test the overall confidence of the predictor e.g {'rental_price_accuracy': 0.95}. when_data -- Use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from. accuracy_score_functions -- A single function or a dictionary for the form {f'{target_name}': acc_func} when multiple targets are used. score_using -- What values from the explanation of the target to be used in the score function. predict_args -- Dictionary of arguments to be passed to predict (same arguments that predict accepts), e.g: predict_args={'use_gpu': True} .","title":"Test"},{"location":"PredictorInterface/#predictor-quality","text":"","title":"Predictor Quality"},{"location":"PredictorInterface/#prediction-quality","text":"","title":"Prediction Quality"},{"location":"PredictorInterface/#datasources","text":"Mindsdb exposes a number of data sources that can be used with the predictor, you can find more details in the datasources section .","title":"DataSources"},{"location":"PredictorInterface/#constants-and-configuration","text":"For the constants and configuration options exposed by mindsdb_native at large please refer to this section .","title":"Constants and Configuration"},{"location":"community/","text":"Join our community If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace . MindsDB newsletter To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter . Become a MindsDB Beta tester If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community . Talk to our engineers If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button. Get in touch for collaboration Contact us by submitting this form .","title":"Join our community"},{"location":"community/#join-our-community","text":"If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace .","title":"Join our community"},{"location":"community/#mindsdb-newsletter","text":"To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter .","title":"MindsDB newsletter"},{"location":"community/#become-a-mindsdb-beta-tester","text":"If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community .","title":"Become a MindsDB Beta tester"},{"location":"community/#talk-to-our-engineers","text":"If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button.","title":"Talk to our engineers"},{"location":"community/#get-in-touch-for-collaboration","text":"Contact us by submitting this form .","title":"Get in touch for collaboration"},{"location":"connect/","text":"Connect your data Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Plug your data"},{"location":"connect/#connect-your-data","text":"","title":"Connect your data"},{"location":"connect/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Connect to database"},{"location":"contribute/","text":"Contribute to MindsDB Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes. Report issue We use GitHub issues to track bugs and features. Report them by opening a new issue and complete out all of the required inputs: Your Python version , MindsDB version , Describe the bug and Steps to reproduce . Documentation We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the MindsDB Docs repository and help us. Easy contribution to issues Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA. Write for us Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"How to contribute"},{"location":"contribute/#contribute-to-mindsdb","text":"Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes.","title":"Contribute to MindsDB"},{"location":"contribute/#report-issue","text":"We use GitHub issues to track bugs and features. Report them by opening a new issue and complete out all of the required inputs: Your Python version , MindsDB version , Describe the bug and Steps to reproduce .","title":"Report issue"},{"location":"contribute/#documentation","text":"We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the MindsDB Docs repository and help us.","title":"Documentation"},{"location":"contribute/#easy-contribution-to-issues","text":"Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA.","title":"Easy contribution to issues"},{"location":"contribute/#write-for-us","text":"Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"Write for us"},{"location":"faq/","text":"What is the roadmap? The MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation What type of data can MindsDB learn and predict from? We support tabular data formats as a CSV, Excel, JSON, text files also pandas data frame, URLs, s3 files. We support the following database integration: SQL NoSQL Streams Data Warehouse - - - - - - - - - - - - - - - - - - - How can I help? You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions . Why is it called MindsDB? Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part? Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded. What is the difference between AI and Machine Learning? What is XAI?","title":"FAQ"},{"location":"faq/#what-is-the-roadmap","text":"The MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation","title":"What is the roadmap?"},{"location":"faq/#what-type-of-data-can-mindsdb-learn-and-predict-from","text":"We support tabular data formats as a CSV, Excel, JSON, text files also pandas data frame, URLs, s3 files. We support the following database integration: SQL NoSQL Streams Data Warehouse - - - - - - - - - - - - - - - - - - -","title":"What type of data can MindsDB learn and predict from?"},{"location":"faq/#how-can-i-help","text":"You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions .","title":"How can I help?"},{"location":"faq/#why-is-it-called-mindsdb","text":"Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part? Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded.","title":"Why is it called MindsDB?"},{"location":"faq/#what-is-the-difference-between-ai-and-machine-learning","text":"","title":"What is the difference between AI and Machine Learning?"},{"location":"faq/#what-is-xai","text":"","title":"What is XAI?"},{"location":"info/","text":"Follow the below steps to get up and running with MindsDB. Steps: Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Getting Started"},{"location":"info/#steps","text":"Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Steps:"},{"location":"bi-tools/tableau/","text":"looker","title":"looker"},{"location":"bi-tools/tableau/#looker","text":"","title":"looker"},{"location":"databases/","text":"AI Tables - Add Automated Machine Learning capabilities into databases You can make Machine Learning predictions directly inside the database by using MindsDB AI-Tables with the most popular database management systems like MySQL, MariaDB, PostgreSQL, ClickHouse. Any database user can now create, train and deploy machine learning models with the same knowledge they have of SQL. AI Tables benefits AI tables help to accelerate development speed and reduce complexity of machine learning workflows: Automated model training and deployment Predictions done at the Data Layer Saves time of moving to production Greater insight into model accuracy Easy to manage and use Open-source based Learn more how AI Tables work from the video below. Try it by installing MindsDB and following the tutorials . AITables example AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example. The used car price example Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. The data is persistend in your database inside a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn't it be nice if you could simply tell your database server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked directly to your database are here to do exactly that. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called 'used_cars_model'. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated 'used_cars_model' AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions you need and what data you want your ML to learn from to make such predictions. Currently, we are supporting the integration with: MySQL PostgreSQL MariaDB ClickHouse","title":"AI Tables - Add Automated Machine Learning capabilities into databases"},{"location":"databases/#ai-tables-add-automated-machine-learning-capabilities-into-databases","text":"You can make Machine Learning predictions directly inside the database by using MindsDB AI-Tables with the most popular database management systems like MySQL, MariaDB, PostgreSQL, ClickHouse. Any database user can now create, train and deploy machine learning models with the same knowledge they have of SQL.","title":"AI Tables - Add Automated Machine Learning capabilities into databases"},{"location":"databases/#ai-tables-benefits","text":"AI tables help to accelerate development speed and reduce complexity of machine learning workflows: Automated model training and deployment Predictions done at the Data Layer Saves time of moving to production Greater insight into model accuracy Easy to manage and use Open-source based Learn more how AI Tables work from the video below. Try it by installing MindsDB and following the tutorials .","title":"AI Tables benefits"},{"location":"databases/#aitables-example","text":"AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example.","title":"AITables example"},{"location":"databases/#the-used-car-price-example","text":"Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. The data is persistend in your database inside a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn't it be nice if you could simply tell your database server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked directly to your database are here to do exactly that. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called 'used_cars_model'. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated 'used_cars_model' AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions you need and what data you want your ML to learn from to make such predictions. Currently, we are supporting the integration with: MySQL PostgreSQL MariaDB ClickHouse","title":"The used car price example"},{"location":"databases/Clickhouse/","text":"AI Tables in ClickHouse Now, you can train machine learning models straight from the database by using MindsDB and ClickHouse . Prerequisite You will need MindsDB version >= 2.0.0 and ClickHouse installed. Install MindsDB Install ClickHouse Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default default) - The ClickHouse user name. host(default localhost) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 8123) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 8123 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM data.pollution_measurement' , { \"option\" : \"value\" } ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example SO2. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: price predicted info 0.001156540079952395 0.9869 Check JSON below { \"predicted_value\" : 0.001156540079952395 , \"confidence\" : 0.9869 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.003184904620383531 , 0.013975553923630717 ], \"important_missing_information\" : [ \"Station code\" , \"Latitude\" , \"O3\" ], \"confidence_composition\" : { \"CO\" : 0.006 }, \"extra_insights\" : { \"if_missing\" : [{ \"NO2\" : 0.007549311956155897 }, { \"CO\" : 0.005459383721227349 }, { \"PM10\" : 0.003870252306568623 }] } } Delete the model If you want to delete the predictor that you have previously created run: INSERT INTO mindsdb . commands values ( 'DELETE predictor airq_predictor' ); To get additional information about the integration check out Machine Learning Models as Tables with ClickHouse tutorial.","title":"AI Tables in ClickHouse"},{"location":"databases/Clickhouse/#ai-tables-in-clickhouse","text":"Now, you can train machine learning models straight from the database by using MindsDB and ClickHouse .","title":"AI Tables in ClickHouse"},{"location":"databases/Clickhouse/#prerequisite","text":"You will need MindsDB version >= 2.0.0 and ClickHouse installed. Install MindsDB Install ClickHouse","title":"Prerequisite"},{"location":"databases/Clickhouse/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default default) - The ClickHouse user name. host(default localhost) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 8123) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 8123 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" }","title":"Configuration"},{"location":"databases/Clickhouse/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/Clickhouse/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM data.pollution_measurement' , { \"option\" : \"value\" } ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example SO2. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/Clickhouse/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: price predicted info 0.001156540079952395 0.9869 Check JSON below { \"predicted_value\" : 0.001156540079952395 , \"confidence\" : 0.9869 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.003184904620383531 , 0.013975553923630717 ], \"important_missing_information\" : [ \"Station code\" , \"Latitude\" , \"O3\" ], \"confidence_composition\" : { \"CO\" : 0.006 }, \"extra_insights\" : { \"if_missing\" : [{ \"NO2\" : 0.007549311956155897 }, { \"CO\" : 0.005459383721227349 }, { \"PM10\" : 0.003870252306568623 }] } }","title":"Query the model"},{"location":"databases/Clickhouse/#delete-the-model","text":"If you want to delete the predictor that you have previously created run: INSERT INTO mindsdb . commands values ( 'DELETE predictor airq_predictor' ); To get additional information about the integration check out Machine Learning Models as Tables with ClickHouse tutorial.","title":"Delete the model"},{"location":"databases/MariaDB/","text":"AI Tables in MariaDB Now, you can train machine learning models straight from the database by using MindsDB and MariaDB . Prerequisite You will need MindsDB version >= 2.0.0 and MariaDB installed: Install MindsDB Install MariaDB Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters creata a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default localhost) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 3306) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" , \"database\" : \"mindsdb\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install CONNECT Storage Engine Also you need to install the CONNECT Storage Engine to access external local data. Checkout MariaDB docs on how to do that. Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example price. To predict multiple features include a comma separated string e.g 'price,year'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price predicted info 13111 0.9921 Check JSON below { \"predicted_value\" : 13111 , \"confidence\" : 0.9921 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10792 , 32749 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.009 , \"year\" : 0.013 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 12962 }, { \"year\" : 12137 }, { \"transmission\" : 2136 }, { \"mileage\" : 22706 }, { \"fuelType\" : 7134 }, { \"tax\" : 13210 }, { \"mpg\" : 27409 }, { \"engineSize\" : 13111 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statement while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'used_cars_model' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the price and a year : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price,year' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" } ); And query it using the select_data_query : SELECT price AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT year FROM price_data' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. If you want to follow along with a tutorial check out AI Tables in MariaDB tutorial.","title":"AI Tables in MariaDB"},{"location":"databases/MariaDB/#ai-tables-in-mariadb","text":"Now, you can train machine learning models straight from the database by using MindsDB and MariaDB .","title":"AI Tables in MariaDB"},{"location":"databases/MariaDB/#prerequisite","text":"You will need MindsDB version >= 2.0.0 and MariaDB installed: Install MindsDB Install MariaDB","title":"Prerequisite"},{"location":"databases/MariaDB/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters creata a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default localhost) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 3306) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" , \"database\" : \"mindsdb\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install CONNECT Storage Engine Also you need to install the CONNECT Storage Engine to access external local data. Checkout MariaDB docs on how to do that.","title":"Configuration"},{"location":"databases/MariaDB/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/MariaDB/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example price. To predict multiple features include a comma separated string e.g 'price,year'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/MariaDB/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price predicted info 13111 0.9921 Check JSON below { \"predicted_value\" : 13111 , \"confidence\" : 0.9921 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10792 , 32749 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.009 , \"year\" : 0.013 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 12962 }, { \"year\" : 12137 }, { \"transmission\" : 2136 }, { \"mileage\" : 22706 }, { \"fuelType\" : 7134 }, { \"tax\" : 13210 }, { \"mpg\" : 27409 }, { \"engineSize\" : 13111 }] } }","title":"Query the model"},{"location":"databases/MariaDB/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statement while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'used_cars_model'","title":"Delete the model"},{"location":"databases/MariaDB/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the price and a year : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price,year' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" } ); And query it using the select_data_query : SELECT price AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT year FROM price_data' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. If you want to follow along with a tutorial check out AI Tables in MariaDB tutorial.","title":"Train and predict multiple features"},{"location":"databases/MySQL/","text":"AI Tables in MySQL Now, you can train machine learning models straight from the database by using MindsDB and MySQL . Prerequisite You will need MindsDB version >= 2.3.0 and MySQL installed: Install MindsDB Install MySQL Enable FEDERATED Storage Engine Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. enabled(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Enable FEDERATED storage engine The FEDERATED storage engine is not enabled by default in the running server; to enable FEDERATED, you must start the MySQL server binary using the --federated option. Check official docs for more info. Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple features include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption predicted info 1.252233223 0.923 Check JSON below { \"predicted_value\" : 1.252233223 , \"confidence\" : 0.923 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statement while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . us_consumption WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in MySQL tutorial .","title":"AI Tables in MySQL"},{"location":"databases/MySQL/#ai-tables-in-mysql","text":"Now, you can train machine learning models straight from the database by using MindsDB and MySQL .","title":"AI Tables in MySQL"},{"location":"databases/MySQL/#prerequisite","text":"You will need MindsDB version >= 2.3.0 and MySQL installed: Install MindsDB Install MySQL Enable FEDERATED Storage Engine","title":"Prerequisite"},{"location":"databases/MySQL/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. enabled(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Enable FEDERATED storage engine The FEDERATED storage engine is not enabled by default in the running server; to enable FEDERATED, you must start the MySQL server binary using the --federated option. Check official docs for more info.","title":"Configuration"},{"location":"databases/MySQL/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/MySQL/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple features include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/MySQL/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption predicted info 1.252233223 0.923 Check JSON below { \"predicted_value\" : 1.252233223 , \"confidence\" : 0.923 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } }","title":"Query the model"},{"location":"databases/MySQL/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statement while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption'","title":"Delete the model"},{"location":"databases/MySQL/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . us_consumption WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in MySQL tutorial .","title":"Train and predict multiple features"},{"location":"databases/PostgreSQL/","text":"AI Tables in PostgreSQL Now, you can train machine learning models straight from the database by using MindsDB and PostgreSQL . Prerequisite You will need MindsDB version >= 2.3.0 and PostgreSQL installed: Install MindsDB Install PostgreSQL Install PostgreSQL foreign data wrapper for MySQL Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install PostgreSQL foreign data wrapper for MySQL The Foreign Data Wrapper (mysql_fwd) can be installed from the EnterpriseDB repo . Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' , '{\"timeseries_settings\":{\"order_by\": [\"t\"], \"window\":20}}' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple features include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . If you are using timeseries data check the Timeseries settings . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE income = 1 . 182497938 AND production = 5 . 854555956 AND savings = 3 . 183292657 AND unemployment = 0 . 1 You should get a similar response from MindsDB as: consumption predicted info 1.4979682087292199 0.9475 Check JSON below { \"predicted_value\" : 1.4979682087292199 , \"confidence\" : 0.9475 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statement while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in PostgreSQL tutorail .","title":"AI Tables in PostgreSQL"},{"location":"databases/PostgreSQL/#ai-tables-in-postgresql","text":"Now, you can train machine learning models straight from the database by using MindsDB and PostgreSQL .","title":"AI Tables in PostgreSQL"},{"location":"databases/PostgreSQL/#prerequisite","text":"You will need MindsDB version >= 2.3.0 and PostgreSQL installed: Install MindsDB Install PostgreSQL Install PostgreSQL foreign data wrapper for MySQL","title":"Prerequisite"},{"location":"databases/PostgreSQL/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install PostgreSQL foreign data wrapper for MySQL The Foreign Data Wrapper (mysql_fwd) can be installed from the EnterpriseDB repo .","title":"Configuration"},{"location":"databases/PostgreSQL/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/PostgreSQL/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' , '{\"timeseries_settings\":{\"order_by\": [\"t\"], \"window\":20}}' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple features include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . If you are using timeseries data check the Timeseries settings .","title":"Train new model"},{"location":"databases/PostgreSQL/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE income = 1 . 182497938 AND production = 5 . 854555956 AND savings = 3 . 183292657 AND unemployment = 0 . 1 You should get a similar response from MindsDB as: consumption predicted info 1.4979682087292199 0.9475 Check JSON below { \"predicted_value\" : 1.4979682087292199 , \"confidence\" : 0.9475 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } }","title":"Query the model"},{"location":"databases/PostgreSQL/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statement while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption'","title":"Delete the model"},{"location":"databases/PostgreSQL/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in PostgreSQL tutorail .","title":"Train and predict multiple features"},{"location":"datasources/clickhouse/","text":"Connect to ClickHouse database Connecting MindsDB to your ClickHouse database can be done in two ways: Using MindsDB Studio . Using ClickHouse client . MindsDB Studio Using MindsDB Studio, you can connect to the ClickHouse database with a few clicks. Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select ClickHouse as Supported Database. Add the Database name. Add the Hostname. Add Port. Add ClickHouse user. Add Password for ClickHouse user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to ClickHouse from MindsDB Studio. The next step is to train the Machine Learning model . ClickHouse client How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example . After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your ClickHouse database, it will create a new database mindsdb and new table predictors . After starting the server, you can run a SELECT query from your ClickHouse client to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and ClickHouse. The next step is to train the Machine Learning model .","title":"Connect to ClickHouse database"},{"location":"datasources/clickhouse/#connect-to-clickhouse-database","text":"Connecting MindsDB to your ClickHouse database can be done in two ways: Using MindsDB Studio . Using ClickHouse client .","title":"Connect to ClickHouse database"},{"location":"datasources/clickhouse/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the ClickHouse database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/clickhouse/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select ClickHouse as Supported Database. Add the Database name. Add the Hostname. Add Port. Add ClickHouse user. Add Password for ClickHouse user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/clickhouse/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to ClickHouse from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/clickhouse/#clickhouse-client","text":"How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example . After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your ClickHouse database, it will create a new database mindsdb and new table predictors . After starting the server, you can run a SELECT query from your ClickHouse client to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and ClickHouse. The next step is to train the Machine Learning model .","title":"ClickHouse client"},{"location":"datasources/configuration/","text":"Extending default configuration Before using SQL clients to connect MindsDB and databases, you will need to create additional configuration before starting MindsDB Server. Create a new config.json file and send the file location as parameteter to --config before starting the server. Check the examples below to preview the configuration required for each database. Required vs Optional means required config key means optional config key PostgreSQL configuration config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type, in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. publish(true|false) - Enable PostgreSQL integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration. MySQL configuration config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the available settings that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type -- in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1, don't use localhost here) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. MariaDB configuration config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 3306 , \"publish\" : true , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mariadb'] -- This key specifies the integration type, in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default 127.0.0.1) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MariaDB integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration. Clickhouse configuration config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"database\" : \"default\" , \"published\" : true , \"type\" : \"clickhouse\" , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"user\" : \"default\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default is default user) - The ClickHouse user name. host(default 127.0.0.1) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 8123) - The TCP/IP port number to use for the connection. publish(true|false) - Enable ClickHouse integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration. SQL Server configuration config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"database\" : \"mindsdb\" , \"password\" : \"123456\" , \"port\" : \"47335\" , \"ssl\" : true , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mssql\" : { \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 1433 , \"publish\" : true , \"type\" : \"mssql\" , \"user\" : \"sa\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default 123456) - Required to have a password, since Microsoft SQL will use default user pass. This is the password for MindsDB MySQL API. database - The name of the server that mindsdb will start. ssl(default true) -- Use SSL true/false. host(default 127.0.0.1). port(default 47335). integrations['default_mssql'] -- This key specifies the integration type, in this case default_mssql . The required keys are: user(default sa) - The Microsoft SQL Server user name. host(default 127.0.0.1) - Connect to the Microsoft SQL Server on the given host. password - The password of the Microsoft SQL Server user. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 1433) - The TCP/IP port number to use for the connection. publish(true|false) - Enable Microsoft SQL Server integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration. MongoDB Configuration config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : {} \"mongodb\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47336\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : {}, \"storage_dir\" : \"/mindsdb_storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP API by providing: host(default 127.0.0.1) - MindsDB server address. port(default 47334) - MindsDB server port. api['mongodb'] -- This key is used for starting MindsDB Mongo API by providing: host(default 127.0.0.1) - MindsDB Mongo API address. port(default 47335) - MindsDB Mongo API port. api['mysql] -- This key is used for starting MindsDB MySQL API. Leave it empty if you work only with MongoDB. config_version(latest 1.4) - The version of config.json file. debug(true|false) integrations[''] -- This key specifies the integration options with other SQL databases. Leave it empty if you work only with MongoDB. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration files.","title":"Configuration"},{"location":"datasources/configuration/#extending-default-configuration","text":"Before using SQL clients to connect MindsDB and databases, you will need to create additional configuration before starting MindsDB Server. Create a new config.json file and send the file location as parameteter to --config before starting the server. Check the examples below to preview the configuration required for each database. Required vs Optional means required config key means optional config key","title":"Extending default configuration"},{"location":"datasources/configuration/#postgresql-configuration","text":"config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type, in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. publish(true|false) - Enable PostgreSQL integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration.","title":"PostgreSQL configuration"},{"location":"datasources/configuration/#mysql-configuration","text":"config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the available settings that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type -- in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1, don't use localhost here) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration.","title":"MySQL configuration"},{"location":"datasources/configuration/#mariadb-configuration","text":"config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 3306 , \"publish\" : true , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mariadb'] -- This key specifies the integration type, in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default 127.0.0.1) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MariaDB integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration.","title":"MariaDB configuration"},{"location":"datasources/configuration/#clickhouse-configuration","text":"config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"pass\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"database\" : \"default\" , \"published\" : true , \"type\" : \"clickhouse\" , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"user\" : \"default\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default is default user) - The ClickHouse user name. host(default 127.0.0.1) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 8123) - The TCP/IP port number to use for the connection. publish(true|false) - Enable ClickHouse integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration.","title":"Clickhouse configuration"},{"location":"datasources/configuration/#sql-server-configuration","text":"config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"database\" : \"mindsdb\" , \"password\" : \"123456\" , \"port\" : \"47335\" , \"ssl\" : true , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mssql\" : { \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 1433 , \"publish\" : true , \"type\" : \"mssql\" , \"user\" : \"sa\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default 123456) - Required to have a password, since Microsoft SQL will use default user pass. This is the password for MindsDB MySQL API. database - The name of the server that mindsdb will start. ssl(default true) -- Use SSL true/false. host(default 127.0.0.1). port(default 47335). integrations['default_mssql'] -- This key specifies the integration type, in this case default_mssql . The required keys are: user(default sa) - The Microsoft SQL Server user name. host(default 127.0.0.1) - Connect to the Microsoft SQL Server on the given host. password - The password of the Microsoft SQL Server user. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 1433) - The TCP/IP port number to use for the connection. publish(true|false) - Enable Microsoft SQL Server integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration.","title":"SQL Server configuration"},{"location":"datasources/configuration/#mongodb-configuration","text":"config.json example (click to expand) { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : {} \"mongodb\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47336\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : {}, \"storage_dir\" : \"/mindsdb_storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP API by providing: host(default 127.0.0.1) - MindsDB server address. port(default 47334) - MindsDB server port. api['mongodb'] -- This key is used for starting MindsDB Mongo API by providing: host(default 127.0.0.1) - MindsDB Mongo API address. port(default 47335) - MindsDB Mongo API port. api['mysql] -- This key is used for starting MindsDB MySQL API. Leave it empty if you work only with MongoDB. config_version(latest 1.4) - The version of config.json file. debug(true|false) integrations[''] -- This key specifies the integration options with other SQL databases. Leave it empty if you work only with MongoDB. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsDB will store models and configuration files.","title":"MongoDB Configuration"},{"location":"datasources/local/","text":"Connect to local dataset Uploading local dataset to MindsDB can be easily done with a few clicks. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML Upload local dataset From the left navigation menu, select the Data dashboard. Click on the UPLOAD button. In the New datasource modal window: Click on the File explorer. Choose the dataset you want to upload. Add the name of the datasource. Click on UPLOAD . That's it You have successfully uploaded your local dataset from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Connect to local dataset"},{"location":"datasources/local/#connect-to-local-dataset","text":"Uploading local dataset to MindsDB can be easily done with a few clicks. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML","title":"Connect to local dataset"},{"location":"datasources/local/#upload-local-dataset","text":"From the left navigation menu, select the Data dashboard. Click on the UPLOAD button. In the New datasource modal window: Click on the File explorer. Choose the dataset you want to upload. Add the name of the datasource. Click on UPLOAD . That's it You have successfully uploaded your local dataset from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Upload local dataset"},{"location":"datasources/mariadb/","text":"Connect to MariaDB database Connecting MindsDB to your MariaDB database can be done in two ways: Using MindsDB Studio . Using MariaDB client . Prerequisite To connect to MariaDB from MindsDB, you will need to install the CONNECT Storage Engine. How to install the CONNECT Storage Engine Please check the MariaDB documentation . Read more about the CONNECT Storage Engine . MindsDB Studio Using MindsDB Studio, you can connect to the MariaDB database with a few clicks. Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select MariaDB as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add MariaDB user. Add Password for MariaDB user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to MariaDB from MindsDB Studio. The next step is to train the Machine Learning model . MariaDB client How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example . After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your MariaDB database, it will create a new database mindsdb and new table predictors . After starting the server, you can run a SELECT query from your mariadb-client it to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and MariaDB. The next step is to train the Machine Learning model .","title":"Connect to MariaDB database"},{"location":"datasources/mariadb/#connect-to-mariadb-database","text":"Connecting MindsDB to your MariaDB database can be done in two ways: Using MindsDB Studio . Using MariaDB client .","title":"Connect to MariaDB database"},{"location":"datasources/mariadb/#prerequisite","text":"To connect to MariaDB from MindsDB, you will need to install the CONNECT Storage Engine. How to install the CONNECT Storage Engine Please check the MariaDB documentation . Read more about the CONNECT Storage Engine .","title":"Prerequisite"},{"location":"datasources/mariadb/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the MariaDB database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/mariadb/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select MariaDB as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add MariaDB user. Add Password for MariaDB user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/mariadb/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to MariaDB from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/mariadb/#mariadb-client","text":"How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example . After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your MariaDB database, it will create a new database mindsdb and new table predictors . After starting the server, you can run a SELECT query from your mariadb-client it to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and MariaDB. The next step is to train the Machine Learning model .","title":"MariaDB client"},{"location":"datasources/mongodb/","text":"Connect to MongoDB database Connecting MindsDB to MongoDB can be done in two ways: Using MindsDB Studio . Using Mongo clients . The current integration works by accessing MongoDB through MindsDB MongoDB API as a new datasource. The new version of MindsDB will allow direct integration inside MongoDB. MindsDB Studio Using MindsDB Studio, you can connect to the MongoDB with a few clicks. Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select MongoDB as the Supported Database. Add Host (connection string) Add Port Add Username Add Password Click on CONNECT Create new dataset Click on the NEW DATASET button. In the Datasource from DB integration modal window: Add Datasource Name Add Database name Add Collection name Add find query to select documents from the collection(must be valid JSON format) Click on CREATE . That's it You have successfully connected to MongoDB from MindsDB Studio. The next step is to train the Machine Learning model . Mongo shell How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the steps in configuration example . After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mongodb --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and Mongo. The --config parameter specifies the location of the configuration file. Connect to MongoDB API You can use mongo shell to connect to mindsdb's MongoDB API. As a prerequisite you need mongo shell version greater then 3.6. To connect use the host that you have specified inside the api['mongodb'] key e.g: mongo --host 127.0.0.1 -u \"username\" -p \"password\" To make sure everything works, you should be able to list the predictors collection: use mindsdb show collections That's it You have successfully connected MindsDB Server and MongoDB. The next step is to train the Machine Learning model .","title":"Connect to MongoDB database"},{"location":"datasources/mongodb/#connect-to-mongodb-database","text":"Connecting MindsDB to MongoDB can be done in two ways: Using MindsDB Studio . Using Mongo clients . The current integration works by accessing MongoDB through MindsDB MongoDB API as a new datasource. The new version of MindsDB will allow direct integration inside MongoDB.","title":"Connect to MongoDB database"},{"location":"datasources/mongodb/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the MongoDB with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/mongodb/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select MongoDB as the Supported Database. Add Host (connection string) Add Port Add Username Add Password Click on CONNECT","title":"Connect to database"},{"location":"datasources/mongodb/#create-new-dataset","text":"Click on the NEW DATASET button. In the Datasource from DB integration modal window: Add Datasource Name Add Database name Add Collection name Add find query to select documents from the collection(must be valid JSON format) Click on CREATE . That's it You have successfully connected to MongoDB from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new dataset"},{"location":"datasources/mongodb/#mongo-shell","text":"How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the steps in configuration example . After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mongodb --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and Mongo. The --config parameter specifies the location of the configuration file.","title":"Mongo shell"},{"location":"datasources/mongodb/#connect-to-mongodb-api","text":"You can use mongo shell to connect to mindsdb's MongoDB API. As a prerequisite you need mongo shell version greater then 3.6. To connect use the host that you have specified inside the api['mongodb'] key e.g: mongo --host 127.0.0.1 -u \"username\" -p \"password\" To make sure everything works, you should be able to list the predictors collection: use mindsdb show collections That's it You have successfully connected MindsDB Server and MongoDB. The next step is to train the Machine Learning model .","title":"Connect to MongoDB API"},{"location":"datasources/mssql/","text":"Connect to Microsoft SQL Server Connecting MindsDB to your Microsoft SQL Server can be done in two ways: Using MindsDB Studio . Using mssql client . Prerequisite To connect to your Microsoft SQL Server from MindsDB, you will need to install Microsoft OLEDB Provider driver. How to install Please check that your MSSQL instance have MSDASQL installed . If not, download driver from here . Microsoft OLE DB Provider for ODBC Overview . Install additional dependency Also, you will need to install MSSQL as an additional requirement. Install MSSQL On the right bottom of the screen, select dependencies . Click on Mssql INSTALL . After installation finish, restart or stop/start MindsDB studio. MindsDB Studio Using MindsDB Studio, you can connect to the Microsoft SQL Server with a few clicks. Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select Microsoft SQL Server as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add SQL Server user. Add Password for SQL Server user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to Microsoft SQL Server from MindsDB Studio. The next step is to train the Machine Learning model . MSSQL client How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example . After adding the required configuration to the config.json file, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. After starting the server, you can run a SELECT query from your SQL client to make sure integration has been successful. Please make sure to use exec or openquery as the examples below. exec ( 'SELECT * FROM mindsdb.predictors' ) AT mindsdb_db ; Or, openquery : select * from openquery ( mindsdb , 'select * from mindsdb.predictors' ); Note: mindsdb is the name of the api['mysql]['database'] key from config.json. The default name is mindsdb . That's it You have successfully connected MindsDB Server and Microsoft SQL Server. The next step is to train the Machine Learning model .","title":"Connect to Microsoft SQL Server"},{"location":"datasources/mssql/#connect-to-microsoft-sql-server","text":"Connecting MindsDB to your Microsoft SQL Server can be done in two ways: Using MindsDB Studio . Using mssql client .","title":"Connect to Microsoft SQL Server"},{"location":"datasources/mssql/#prerequisite","text":"To connect to your Microsoft SQL Server from MindsDB, you will need to install Microsoft OLEDB Provider driver. How to install Please check that your MSSQL instance have MSDASQL installed . If not, download driver from here . Microsoft OLE DB Provider for ODBC Overview . Install additional dependency Also, you will need to install MSSQL as an additional requirement.","title":"Prerequisite"},{"location":"datasources/mssql/#install-mssql","text":"On the right bottom of the screen, select dependencies . Click on Mssql INSTALL . After installation finish, restart or stop/start MindsDB studio.","title":"Install MSSQL"},{"location":"datasources/mssql/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the Microsoft SQL Server with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/mssql/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select Microsoft SQL Server as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add SQL Server user. Add Password for SQL Server user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/mssql/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to Microsoft SQL Server from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/mssql/#mssql-client","text":"How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example . After adding the required configuration to the config.json file, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use -- in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. After starting the server, you can run a SELECT query from your SQL client to make sure integration has been successful. Please make sure to use exec or openquery as the examples below. exec ( 'SELECT * FROM mindsdb.predictors' ) AT mindsdb_db ; Or, openquery : select * from openquery ( mindsdb , 'select * from mindsdb.predictors' ); Note: mindsdb is the name of the api['mysql]['database'] key from config.json. The default name is mindsdb . That's it You have successfully connected MindsDB Server and Microsoft SQL Server. The next step is to train the Machine Learning model .","title":"MSSQL client"},{"location":"datasources/mysql/","text":"Connect to MySQL database Connecting MindsDB to your MySQL database can be done in two ways: Using MindsDB Studio . Using MySQL client . Prerequisite To connect to your MySQL Server from MindsDB, you will need to enable the FEDERATED Storage Engine. How to enable the FEDERATED Storage Engine Please check the MySQL documentation . Stackoverflow answers . Read more about the FEDERATED Storage Engine . MindsDB Studio Using MindsDB Studio, you can connect to the MySQL database with a few clicks. Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select MySQL as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add MySQL user. Add Password for MySQL user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to MySQL from MindsDB Studio. The next step is to train the Machine Learning model . MySQL client How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use, in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your MySQL database, it will create a new database mindsdb and new table predictors . After starting the server, you can run a SELECT query from your mysql-client to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and MySQL. The next step is to train the Machine Learning model .","title":"Connect to MySQL database"},{"location":"datasources/mysql/#connect-to-mysql-database","text":"Connecting MindsDB to your MySQL database can be done in two ways: Using MindsDB Studio . Using MySQL client .","title":"Connect to MySQL database"},{"location":"datasources/mysql/#prerequisite","text":"To connect to your MySQL Server from MindsDB, you will need to enable the FEDERATED Storage Engine. How to enable the FEDERATED Storage Engine Please check the MySQL documentation . Stackoverflow answers . Read more about the FEDERATED Storage Engine .","title":"Prerequisite"},{"location":"datasources/mysql/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the MySQL database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/mysql/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select MySQL as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add MySQL user. Add Password for MySQL user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/mysql/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to MySQL from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/mysql/#mysql-client","text":"How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow the configuration example After adding the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use, in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your MySQL database, it will create a new database mindsdb and new table predictors . After starting the server, you can run a SELECT query from your mysql-client to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and MySQL. The next step is to train the Machine Learning model .","title":"MySQL client"},{"location":"datasources/postgresql/","text":"Connect to PostgreSQL database Connecting MindsDB to your PostgreSQL database can be done in two ways: Using MindsDB Studio . Using psql client . Prerequisite To connect to your PostgreSQL Server from MindsDB, you will need to install the MySQL foreign data wrapper for PostgreSQL. How to install the MySQL foreign data wrapper Please check the mysql_fdw documentation . Stackoverflow answers . MindsDB Studio Using MindsDB Studio, you can connect to the PostgreSQL database with a few clicks. Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select PostgreSQL as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add PostgreSQL user. Add Password for PostgreSQL user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to PostgreSQL from MindsDB Studio. The next step is to train the Machine Learning model . PSQL client How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow configuration example . After creating the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use, in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your PostgreSQL database, it will create a new schema mindsdb and new table predictors . After starting the server, you can run a SELECT query from your psql-client to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and PostgreSQL. The next step is to train the Machine Learning model .","title":"Connect to PostgreSQL database"},{"location":"datasources/postgresql/#connect-to-postgresql-database","text":"Connecting MindsDB to your PostgreSQL database can be done in two ways: Using MindsDB Studio . Using psql client .","title":"Connect to PostgreSQL database"},{"location":"datasources/postgresql/#prerequisite","text":"To connect to your PostgreSQL Server from MindsDB, you will need to install the MySQL foreign data wrapper for PostgreSQL. How to install the MySQL foreign data wrapper Please check the mysql_fdw documentation . Stackoverflow answers .","title":"Prerequisite"},{"location":"datasources/postgresql/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the PostgreSQL database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/postgresql/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select PostgreSQL as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add PostgreSQL user. Add Password for PostgreSQL user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/postgresql/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database) Click on CREATE . That's it You have successfully connected to PostgreSQL from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/postgresql/#psql-client","text":"How to extend MindsDB configuration Our suggestion is to always use MindsDB Studio to connect MindsDB to your database. If you still want to extend the configuration without using MindsDB Studio follow configuration example . After creating the required configuration, you will need to start MindsDB and provide the path to the newly created config.json : python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use, in this case HTTP and MySQL. The --config parameter specifies the location of the configuration file. If MindsDB is successfully connected to your PostgreSQL database, it will create a new schema mindsdb and new table predictors . After starting the server, you can run a SELECT query from your psql-client to make sure integration has been successful. SELECT * FROM mindsdb . predictors ; That's it You have successfully connected MindsDB Server and PostgreSQL. The next step is to train the Machine Learning model .","title":"PSQL client"},{"location":"datasources/remote/","text":"Connect to remote URL Connecting MindsDB to your remote data can be easily done with a few clicks. You can connect to your: S3 filestore Raw GitHub files Azure Blob storage files Google cloud storage Or, wherever your files are publicly accessible. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML Upload from remote URL From the left navigation menu, select the Data dashboard. Click on the ADD BY URL button. In the New data source from URL modal window: Add a URL link to your data. Add name for the datasource. Click on UPLOAD . That's it You have successfully connected to your remote dataset from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Connect to remote URL"},{"location":"datasources/remote/#connect-to-remote-url","text":"Connecting MindsDB to your remote data can be easily done with a few clicks. You can connect to your: S3 filestore Raw GitHub files Azure Blob storage files Google cloud storage Or, wherever your files are publicly accessible. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML","title":"Connect to remote URL"},{"location":"datasources/remote/#upload-from-remote-url","text":"From the left navigation menu, select the Data dashboard. Click on the ADD BY URL button. In the New data source from URL modal window: Add a URL link to your data. Add name for the datasource. Click on UPLOAD . That's it You have successfully connected to your remote dataset from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Upload from remote URL"},{"location":"datasources/snowflake/","text":"Connect to Snowflake Data Warehouse Connecting MindsDB to the Snowflake can be easily done with a few clicks. Connect to Snowflake From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select Snowflake as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add Account. Add Warehouse. Add Schema. Add protocol. Add User. Add Password for the user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database). Click on CREATE . That's it You have successfully connected to Snowflake from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Connect to Snowflake Data Warehouse"},{"location":"datasources/snowflake/#connect-to-snowflake-data-warehouse","text":"Connecting MindsDB to the Snowflake can be easily done with a few clicks.","title":"Connect to Snowflake Data Warehouse"},{"location":"datasources/snowflake/#connect-to-snowflake","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select Snowflake as the Supported Database. Add the Database name. Add the Hostname. Add Port. Add Account. Add Warehouse. Add Schema. Add protocol. Add User. Add Password for the user. Click on CONNECT .","title":"Connect to Snowflake"},{"location":"datasources/snowflake/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal window: Add Datasource Name. Add Database name. Add SELECT Query (e.g. SELECT * FROM my_database). Click on CREATE . That's it You have successfully connected to Snowflake from MindsDB Studio. The next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"deployment/cloud/","text":"MindsDB Cloud For a quick jumpstart into using MindsDB you can sign up for our Cloud. MindsDB Cloud is hosted by the MindsDB team and has all of the latest updates. You can start using it immediately for free by following these steps: Create an account by visiting the cloud.mindsdb.com/signup and filling out the sign-up form. After sign up, you will get a confirmation email to validate your account. Once your account is validated, you can login to MindsDB Cloud. Now, you are ready to use MindsDB Cloud.","title":"MindsDB Cloud"},{"location":"deployment/cloud/#mindsdb-cloud","text":"For a quick jumpstart into using MindsDB you can sign up for our Cloud. MindsDB Cloud is hosted by the MindsDB team and has all of the latest updates. You can start using it immediately for free by following these steps: Create an account by visiting the cloud.mindsdb.com/signup and filling out the sign-up form. After sign up, you will get a confirmation email to validate your account. Once your account is validated, you can login to MindsDB Cloud. Now, you are ready to use MindsDB Cloud.","title":"MindsDB Cloud"},{"location":"deployment/docker/","text":"Deploy using Docker To use MindsDB's Docker Container you first need to have Docker installed on your machine. To make sure Docker is successfully installed on your machine, run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check the Get Started documentation. MindsDB container MindsDB images are uploaded to the MindsDB repo on docker hub after each release. For \"Docker for Mac\" Users By default, Docker for Mac allocates 2.00GB of memory . This is insufficient for deploying to docker. We recommend increasing the default memory limit to 4.00GB . Please refer to https://docs.docker.com/desktop/mac/#resources for more information on how to increase the allocated memory. Pull image First, run the below command to pull our latest production image: docker pull mindsdb/mindsdb Or, to try out the latest beta version, pull the beta image: docker pull mindsdb/mindsdb_beta Publish ports By default, when you run the MindsDB container, it does not publish any of its ports. To make the ports available you must run the container by providing -p flag as: -p 47334:47334 - Map 47334 port which is used by the MindsDB GUI and the HTTP API. -p 47335:47335 - Map 47335 to use MindsDB MySQL API. -p 47336:47336 - Map 47336 port to use MindsDB MongoDB API. Start container Next, run the below command to start the container: docker run -p 47334:47334 mindsdb/mindsdb That's it. MindsDB should automatically start the Studio on your default browser. MKL Issues Note. If you experience any issue related to MKL or if your training process does not complete, please add env var or starter Docker with this command: docker run --env MKL_SERVICE_FORCE_INTEL=1 -it -p 47334:47334 mindsdb/mindsdb Extend config.json If you want to extend the default configuration, you will be able to send the config.json value as JSON string argument to the MDB_CONFIG_CONTENT as: docker run -e MDB_CONFIG_CONTENT='{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"47334\"}}}' mindsdb/mindsdb Or, you can pipe < the content of the file to the MDB_CONFIG_CONTENT.","title":"Docker"},{"location":"deployment/docker/#deploy-using-docker","text":"To use MindsDB's Docker Container you first need to have Docker installed on your machine. To make sure Docker is successfully installed on your machine, run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check the Get Started documentation.","title":"Deploy using Docker"},{"location":"deployment/docker/#mindsdb-container","text":"MindsDB images are uploaded to the MindsDB repo on docker hub after each release.","title":"MindsDB container"},{"location":"deployment/docker/#for-docker-for-mac-users","text":"By default, Docker for Mac allocates 2.00GB of memory . This is insufficient for deploying to docker. We recommend increasing the default memory limit to 4.00GB . Please refer to https://docs.docker.com/desktop/mac/#resources for more information on how to increase the allocated memory.","title":"For \"Docker for Mac\" Users"},{"location":"deployment/docker/#pull-image","text":"First, run the below command to pull our latest production image: docker pull mindsdb/mindsdb Or, to try out the latest beta version, pull the beta image: docker pull mindsdb/mindsdb_beta","title":"Pull image"},{"location":"deployment/docker/#publish-ports","text":"By default, when you run the MindsDB container, it does not publish any of its ports. To make the ports available you must run the container by providing -p flag as: -p 47334:47334 - Map 47334 port which is used by the MindsDB GUI and the HTTP API. -p 47335:47335 - Map 47335 to use MindsDB MySQL API. -p 47336:47336 - Map 47336 port to use MindsDB MongoDB API.","title":"Publish ports"},{"location":"deployment/docker/#start-container","text":"Next, run the below command to start the container: docker run -p 47334:47334 mindsdb/mindsdb That's it. MindsDB should automatically start the Studio on your default browser. MKL Issues Note. If you experience any issue related to MKL or if your training process does not complete, please add env var or starter Docker with this command: docker run --env MKL_SERVICE_FORCE_INTEL=1 -it -p 47334:47334 mindsdb/mindsdb","title":"Start container"},{"location":"deployment/docker/#extend-configjson","text":"If you want to extend the default configuration, you will be able to send the config.json value as JSON string argument to the MDB_CONFIG_CONTENT as: docker run -e MDB_CONFIG_CONTENT='{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"47334\"}}}' mindsdb/mindsdb Or, you can pipe < the content of the file to the MDB_CONFIG_CONTENT.","title":"Extend config.json"},{"location":"deployment/linux/","text":"Deploy using pip Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version is >=19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages: Deploy using Anaconda Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list Troubleshooting If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. Installation fail Note that Python 64 bit version is required. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon.","title":"Deploy using pip"},{"location":"deployment/linux/#deploy-using-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version is >=19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages:","title":"Deploy using pip"},{"location":"deployment/linux/#deploy-using-anaconda","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list","title":"Deploy using Anaconda"},{"location":"deployment/linux/#troubleshooting","text":"If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. Installation fail Note that Python 64 bit version is required. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon.","title":"Troubleshooting"},{"location":"deployment/macos/","text":"Deploy using pip Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed, run: pip freeze You should see a list with the names of installed packages: Deploy using Anaconda Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and a Python 64bit version. Then open the Anaconda prompt and: Create a new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed, run: conda list You should see a list with the names of installed packages. Troubleshooting If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. Please downgrade to an older version of Python for now 3.7.x or 3.8.x . We are working on this, and Python 3.9 will be supported soon. Installation fail Note that Python 64 bit version is required. Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies.","title":"Deploy using pip"},{"location":"deployment/macos/#deploy-using-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed, run: pip freeze You should see a list with the names of installed packages:","title":"Deploy using pip"},{"location":"deployment/macos/#deploy-using-anaconda","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and a Python 64bit version. Then open the Anaconda prompt and: Create a new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed, run: conda list You should see a list with the names of installed packages.","title":"Deploy using Anaconda"},{"location":"deployment/macos/#troubleshooting","text":"If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. Please downgrade to an older version of Python for now 3.7.x or 3.8.x . We are working on this, and Python 3.9 will be supported soon. Installation fail Note that Python 64 bit version is required. Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies.","title":"Troubleshooting"},{"location":"deployment/pypi/","text":"Deploy from PyPi The recommended way is to always install inside a virtual environment when using pip . The venv provides an isolated Python environment, which is more practical than installing MindsDB systemwide. Follow below instructions depending on your operating system: Linux Windows MacOS Source code","title":"Pip"},{"location":"deployment/pypi/#deploy-from-pypi","text":"The recommended way is to always install inside a virtual environment when using pip . The venv provides an isolated Python environment, which is more practical than installing MindsDB systemwide. Follow below instructions depending on your operating system: Linux Windows MacOS Source code","title":"Deploy from PyPi"},{"location":"deployment/source/","text":"Deploy from the source code This section describes how to deploy MindsDB from the source code. This is the preferred way to use MindsDB if you want to contribute to our code or simply to debug MindsDB. Prerequisite Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. Python version >=3.7 (64 bit) and pip version >= 19.3. Pip (is usually pre-installed with the latest Python versions). Git . Installation We recommend installing MindsDB inside a virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create a virtual environment and activate it: python3 -m venv mindsdb-venv source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop You're done! To check if everything works, start the MindsDB server: python -m mindsdb To access MindsDB APIs, visit http://127.0.0.1:47334/api . To access MindsDB Studio, visit http://127.0.0.1:47334/ To access MindsDB Studio using mysql: mysql -h 127.0.0.1 --port 3306 -u mindsdb -p Installation troubleshooting No module named mindsdb If you get this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} This type of error can occur if you skipped the 3rd step. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in the starting phase. If the server has started and there is an error displayed, please report it on our GitHub .","title":"Deploy from the source code"},{"location":"deployment/source/#deploy-from-the-source-code","text":"This section describes how to deploy MindsDB from the source code. This is the preferred way to use MindsDB if you want to contribute to our code or simply to debug MindsDB.","title":"Deploy from the source code"},{"location":"deployment/source/#prerequisite","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. Python version >=3.7 (64 bit) and pip version >= 19.3. Pip (is usually pre-installed with the latest Python versions). Git .","title":"Prerequisite"},{"location":"deployment/source/#installation","text":"We recommend installing MindsDB inside a virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create a virtual environment and activate it: python3 -m venv mindsdb-venv source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop You're done! To check if everything works, start the MindsDB server: python -m mindsdb To access MindsDB APIs, visit http://127.0.0.1:47334/api . To access MindsDB Studio, visit http://127.0.0.1:47334/ To access MindsDB Studio using mysql: mysql -h 127.0.0.1 --port 3306 -u mindsdb -p","title":"Installation"},{"location":"deployment/source/#installation-troubleshooting","text":"No module named mindsdb If you get this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} This type of error can occur if you skipped the 3rd step. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in the starting phase. If the server has started and there is an error displayed, please report it on our GitHub .","title":"Installation troubleshooting"},{"location":"deployment/windows/","text":"Deploy using Anaconda Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages. Deploy using pip Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Troubleshooting If the installation fails, don't worry, simply follow the below bellow instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. Installation fail Note that Python 64 bit version is required. pip command not found fail Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Installation fail If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt .","title":"Deploy using Anaconda"},{"location":"deployment/windows/#deploy-using-anaconda","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages.","title":"Deploy using Anaconda"},{"location":"deployment/windows/#deploy-using-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages.","title":"Deploy using pip"},{"location":"deployment/windows/#troubleshooting","text":"If the installation fails, don't worry, simply follow the below bellow instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. Installation fail Note that Python 64 bit version is required. pip command not found fail Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Installation fail If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt .","title":"Troubleshooting"},{"location":"integrations/AwsSageMaker/","text":"In the next sections, you can find general information on how to build MindsDB container image, train models, deploy them on SageMaker and get the predictions. The code can be found inside the mindsdb-sagemaker-container GitHub repository. Training and Inference flow on SageMaker","title":"Amazon SageMaker"},{"location":"integrations/AwsSageMaker/#training-and-inference-flow-on-sagemaker","text":"","title":"Training and Inference flow on SageMaker"},{"location":"integrations/CallSageMakerEndpoint/","text":"To call the SageMaker endpint you can create Jupyter Notebook or use call.py script. Jupyter Notebook import boto3 endpointName = 'mindsdb-impl' # load test dataset with open ( 'diabetest-test.csv' , 'r' ) as reader : payload = reader . read () # Talk to SageMaker client = boto3 . client ( 'sagemaker-runtime' ) response = client . invoke_endpoint ( EndpointName = endpointName , Body = payload , ContentType = text / csv , Accept = 'Accept' ) print ( response [ 'Body' ] . read () . decode ( 'ascii' )) Run the code and you should see the prediction response from the endpoint: { \"prediction\" : \"* We are 96% confident the value of \" Class \" is positive.\" , \"class_confidence\" : [ 0.964147493532568 ] } call.py Script The required arguments are: endpoint - The name of the SageMaker endpoint. dataset - The location of test dataset. content type - The mime type of the data. python3 call . py -- endpoint mindsdb - impl -- dataset test_data / diabetes - test . json -- content - type application / json","title":"Call SageMaker Endpoint"},{"location":"integrations/CallSageMakerEndpoint/#jupyter-notebook","text":"import boto3 endpointName = 'mindsdb-impl' # load test dataset with open ( 'diabetest-test.csv' , 'r' ) as reader : payload = reader . read () # Talk to SageMaker client = boto3 . client ( 'sagemaker-runtime' ) response = client . invoke_endpoint ( EndpointName = endpointName , Body = payload , ContentType = text / csv , Accept = 'Accept' ) print ( response [ 'Body' ] . read () . decode ( 'ascii' )) Run the code and you should see the prediction response from the endpoint: { \"prediction\" : \"* We are 96% confident the value of \" Class \" is positive.\" , \"class_confidence\" : [ 0.964147493532568 ] }","title":"Jupyter Notebook"},{"location":"integrations/CallSageMakerEndpoint/#callpy-script","text":"The required arguments are: endpoint - The name of the SageMaker endpoint. dataset - The location of test dataset. content type - The mime type of the data. python3 call . py -- endpoint mindsdb - impl -- dataset test_data / diabetes - test . json -- content - type application / json","title":"call.py Script"},{"location":"integrations/MindsDBSageContainer/","text":"The general flow is to train and deploy models within SageMaker, create endpoints and take advantage of automated machine learning with MindsDB. The mindsdb_impl Code Structure All of the components we need to package MindsDB for Amazon SageMager are located inside the mindsdb_impl directory: |-- Dockerfile |-- build_and_push.sh `-- mindsdb_impl |-- nginx.conf |-- predictor.py |-- serve |-- train `-- wsgi.py All of the files that will be packaged in the container working directory are inside mindsdb_impl: * nginx.conf - is the configuration file for the nginx . * predictor.py is the program that actually implements the Flask web server and the MindsDB predictions for this app. We have modified this to use the MindsDB Predictor and to accept different types of tabular data for predictions. * serve is the program that is started when the container is started for hosting. * train is the program that is invoked when the container is being run for training. We have modified this program to use MindsDB Predictor interface. * wsgi.py is a small wrapper used to invoke the Flask app. Build Docker Image The docker command for building the image is build and -t parameter provides the name of the image: docker build -t mindsdb-impl . After getting the Successfully built message, we should be able to list the image by running the list command: docker image list Test the Container Locally The local_test directory contains all of the scripts and data samples for testing the built container on the local machine. train_local.sh: Instantiate the container configured for training. serve_local.sh: Instantiate the container configured for serving. predict.sh: Run predictions against a locally instantiated server. test-dir: This directory is mounted in the container. test_data: This directory contains a few tabular format datasets used for getting the predictions. input/data/training/file.csv`: The training data. model: The directory where MindsDB writes the model files. call.py: This cli script can be used for testing the deployed model on the SageMaker endpoint To train the model execute: ./train_local.sh mindsdb-impl Next, start the inference server that will provide an endpoint for getting the predictions. Inside the local_test directory execute serve_local.sh script: ./serve_local.sh mindsdb-impl To run predictions against the invocations endpoint, use predict.sh script: ./predict.sh test_data/diabetest-test.csv text/csv The arguments sent to the script are the test data and content type of the data. Deploy the Image on Amazon ECR (Elastic Container Repository) To push the image use build-and-push.sh script. Note that the script will look for an AWS EC Repository in the default region that you are using, and create a new one if that doesn't exist. ./build-and-push.sh mindsdb-impl After the mindsdb-impl image is deployed to Amazon ECR, you can use it inside SageMaker.","title":"MindsDB container"},{"location":"integrations/MindsDBSageContainer/#the-mindsdb_impl-code-structure","text":"All of the components we need to package MindsDB for Amazon SageMager are located inside the mindsdb_impl directory: |-- Dockerfile |-- build_and_push.sh `-- mindsdb_impl |-- nginx.conf |-- predictor.py |-- serve |-- train `-- wsgi.py All of the files that will be packaged in the container working directory are inside mindsdb_impl: * nginx.conf - is the configuration file for the nginx . * predictor.py is the program that actually implements the Flask web server and the MindsDB predictions for this app. We have modified this to use the MindsDB Predictor and to accept different types of tabular data for predictions. * serve is the program that is started when the container is started for hosting. * train is the program that is invoked when the container is being run for training. We have modified this program to use MindsDB Predictor interface. * wsgi.py is a small wrapper used to invoke the Flask app.","title":"The mindsdb_impl Code Structure"},{"location":"integrations/MindsDBSageContainer/#build-docker-image","text":"The docker command for building the image is build and -t parameter provides the name of the image: docker build -t mindsdb-impl . After getting the Successfully built message, we should be able to list the image by running the list command: docker image list","title":"Build Docker Image"},{"location":"integrations/MindsDBSageContainer/#test-the-container-locally","text":"The local_test directory contains all of the scripts and data samples for testing the built container on the local machine. train_local.sh: Instantiate the container configured for training. serve_local.sh: Instantiate the container configured for serving. predict.sh: Run predictions against a locally instantiated server. test-dir: This directory is mounted in the container. test_data: This directory contains a few tabular format datasets used for getting the predictions. input/data/training/file.csv`: The training data. model: The directory where MindsDB writes the model files. call.py: This cli script can be used for testing the deployed model on the SageMaker endpoint To train the model execute: ./train_local.sh mindsdb-impl Next, start the inference server that will provide an endpoint for getting the predictions. Inside the local_test directory execute serve_local.sh script: ./serve_local.sh mindsdb-impl To run predictions against the invocations endpoint, use predict.sh script: ./predict.sh test_data/diabetest-test.csv text/csv The arguments sent to the script are the test data and content type of the data.","title":"Test the Container Locally"},{"location":"integrations/MindsDBSageContainer/#deploy-the-image-on-amazon-ecr-elastic-container-repository","text":"To push the image use build-and-push.sh script. Note that the script will look for an AWS EC Repository in the default region that you are using, and create a new one if that doesn't exist. ./build-and-push.sh mindsdb-impl After the mindsdb-impl image is deployed to Amazon ECR, you can use it inside SageMaker.","title":"Deploy the Image on Amazon ECR (Elastic Container Repository)"},{"location":"integrations/SageMakerSDK/","text":"The following section explaines how to train and host models using Amazon SageMaker SDK . Add dependencies Install SageMaker SDK : pip install sagemaker First, add IAM Role that have AmazonSageMakerFullAccess Policy. import sagemaker as sage role = \"arn:aws:iam::123213143532:role/service-role/AmazonSageMaker-ExecutionRole-20199\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] Next, provide s3 bucket where the models will be saved, get aws region from session and add URI to MindsDB image in AWS ECR: bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) Start training The required properties for invoking SageMaker training using Estimator are: The image name(str) -- The MindsDB container URI on ECR. The role(str) -- AWS arn with SageMaker execution role. The instance count(int) -- The number of machines to use for training. The instance type(str) -- The type of machine to use for training. The output path(str) -- Path to the s3 bucket where the model artifact will be saved. The session(sagemaker.session.Session) -- The SageMaker session object that we\u2019ve defined in the code above. The base job name(str) -- The name of the training job. The hyperparameters(dict) -- The MindsDB container requires to_predict value so it knows which column to predict. mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) Deploy the model The required configuration for deploying model: initial_instance_count (int) -- The initial number of instances to run in the Endpoint created from this Model. instance_type (str) -- The EC2 instance type to deploy this Model to. For example, \u2018ml.p2.xlarge\u2019, or \u2018local\u2019 for local mode. endpoint_name (str) -- The name of the endpoint on SageMaker. dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) ``` ### Make a prediction Load the test dataset and then call the Predictor predict method with test data . ``` python with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' )) Delete the endpoint Don't forget to delete the endpoint after using it. sess . delete_endpoint ( 'mindsdb-impl' ) Full code example import sagemaker as sage # Add AmazonSageMaker Execution role here role = \"arn:aws:iam:\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) #Hyperparameters to_predict is required for MindsDB container mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Train and host MindsDB Models using SageMaker SDK"},{"location":"integrations/SageMakerSDK/#add-dependencies","text":"Install SageMaker SDK : pip install sagemaker First, add IAM Role that have AmazonSageMakerFullAccess Policy. import sagemaker as sage role = \"arn:aws:iam::123213143532:role/service-role/AmazonSageMaker-ExecutionRole-20199\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] Next, provide s3 bucket where the models will be saved, get aws region from session and add URI to MindsDB image in AWS ECR: bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region )","title":"Add dependencies"},{"location":"integrations/SageMakerSDK/#start-training","text":"The required properties for invoking SageMaker training using Estimator are: The image name(str) -- The MindsDB container URI on ECR. The role(str) -- AWS arn with SageMaker execution role. The instance count(int) -- The number of machines to use for training. The instance type(str) -- The type of machine to use for training. The output path(str) -- Path to the s3 bucket where the model artifact will be saved. The session(sagemaker.session.Session) -- The SageMaker session object that we\u2019ve defined in the code above. The base job name(str) -- The name of the training job. The hyperparameters(dict) -- The MindsDB container requires to_predict value so it knows which column to predict. mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" })","title":"Start training"},{"location":"integrations/SageMakerSDK/#deploy-the-model","text":"The required configuration for deploying model: initial_instance_count (int) -- The initial number of instances to run in the Endpoint created from this Model. instance_type (str) -- The EC2 instance type to deploy this Model to. For example, \u2018ml.p2.xlarge\u2019, or \u2018local\u2019 for local mode. endpoint_name (str) -- The name of the endpoint on SageMaker. dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) ``` ### Make a prediction Load the test dataset and then call the Predictor predict method with test data . ``` python with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Deploy the model"},{"location":"integrations/SageMakerSDK/#delete-the-endpoint","text":"Don't forget to delete the endpoint after using it. sess . delete_endpoint ( 'mindsdb-impl' )","title":"Delete the endpoint"},{"location":"integrations/SageMakerSDK/#full-code-example","text":"import sagemaker as sage # Add AmazonSageMaker Execution role here role = \"arn:aws:iam:\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) #Hyperparameters to_predict is required for MindsDB container mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Full code example"},{"location":"integrations/UseMindsDBinSage/","text":"The following section explains how to train and host models using Amazon SageMaker console . Create Train Job Follow the steps below to successfully start a train job and use MindsDB to create the models: 1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. From the left panel choose Create Training Job and provide the following information Job name IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Algorithm source - Your own algorithm container in ECR Provide container ECR path Container - the ECR registry Image URI that we have pushed Input mode - File Resource configuration - leave the default instance type and count Hyperparameters - MindsDB requires to_predict column name, so it knows which column we want to predict, e.g. Key - to_predict Value - Class(the column in diabetes dataset) Input data configuration Channel name - training Data source - s3 S3 location - path to the s3 bucket where the dataset is located 7.Output data configuration - path to the s3 where the models will be saved Model creation Create model and add the required settings: 1. Model name - must be unique IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Container input options Provide model artifacts and inference image location Use a single model Location of the inference code - 846763053924.dkr.ecr.us-east-1.amazonaws.com/mindsdb_impl:latest Location of model artifacts - path to model.tar.gz inside s3 bucket. Endpoint configuration In the endpoint configuration, add the models to deploy, and the hardware requirements: Endpoint configuration name. Add model - select the previously created model. Choose Create endpoint configuration. Create endpoint The last step is to create endpoint and provide endpoint configuration that specify which models to deploy and the requirements: Endpoint name. Attach endpoint configuration - select the previously created endpoint configuration. Choose Create endpoint. After finishing the above steps, SageMaker will create a new instance and start the inference code.","title":"Train and host MindsDB models"},{"location":"integrations/UseMindsDBinSage/#create-train-job","text":"Follow the steps below to successfully start a train job and use MindsDB to create the models: 1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. From the left panel choose Create Training Job and provide the following information Job name IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Algorithm source - Your own algorithm container in ECR Provide container ECR path Container - the ECR registry Image URI that we have pushed Input mode - File Resource configuration - leave the default instance type and count Hyperparameters - MindsDB requires to_predict column name, so it knows which column we want to predict, e.g. Key - to_predict Value - Class(the column in diabetes dataset) Input data configuration Channel name - training Data source - s3 S3 location - path to the s3 bucket where the dataset is located 7.Output data configuration - path to the s3 where the models will be saved","title":"Create Train Job"},{"location":"integrations/UseMindsDBinSage/#model-creation","text":"Create model and add the required settings: 1. Model name - must be unique IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Container input options Provide model artifacts and inference image location Use a single model Location of the inference code - 846763053924.dkr.ecr.us-east-1.amazonaws.com/mindsdb_impl:latest Location of model artifacts - path to model.tar.gz inside s3 bucket.","title":"Model creation"},{"location":"integrations/UseMindsDBinSage/#endpoint-configuration","text":"In the endpoint configuration, add the models to deploy, and the hardware requirements: Endpoint configuration name. Add model - select the previously created model. Choose Create endpoint configuration.","title":"Endpoint configuration"},{"location":"integrations/UseMindsDBinSage/#create-endpoint","text":"The last step is to create endpoint and provide endpoint configuration that specify which models to deploy and the requirements: Endpoint name. Attach endpoint configuration - select the previously created endpoint configuration. Choose Create endpoint. After finishing the above steps, SageMaker will create a new instance and start the inference code.","title":"Create endpoint"},{"location":"lightwood/API/","text":"","title":"API"},{"location":"lightwood/info/","text":"Lightwood is a Pytorch based framework with two objectives: Make it so simple that you can build predictive models with a line of code. Make it so flexible that you can change and customize everything. Lightwood was inspired on Keras + Ludwig but runs on Pytorch and gives you full control of what you can do. Prerequisites Python >=3.6 64bit version Installing Lightwood You can install Lightwood using pip : pip3 install lightwood If this fails, please report the bug on github and try installing the current master branch: git clone git@github.com:mindsdb/lightwood.git ; cd lightwood ; pip install --no-cache-dir -e . Please note that, depending on your os and python setup, you might want to use pip instead of pip3 . You need python 3.6 or higher. Note on MacOS, you need to install libomp: brew install libomp Install using virtual environment We suggest you to install Lightwood on a virtual environment to avoid dependency issues. Make sure your Python version is >=3.6. To set up a virtual environment: Install on Windows Install the latest version of pip : python -m pip install --upgrade pip pip --version Activate your virtual environment and install lightwood: py -m pip install --user virtualenv . \\e nv \\S cripts \\a ctivate pip install lightwood You can also use python instead of py Install on Linux or macOS Before installing Lightwood in a virtual environment you need to first create and activate the venv : python -m venv env source env/bin/activate pip install lightwood Quick example Assume that you have a training file (sensor_data.csv) such as this one. sensor1 sensor2 sensor3 1 -1 -1 0 1 0 -1 -1 1 1 0 0 0 1 0 -1 1 -1 0 0 0 -1 -1 1 1 0 0 And you would like to learn to predict the values of sensor3 given the readings in sensor1 and sensor2 . Learn You can train a Predictor as follows: from lightwood import Predictor import pandas sensor3_predictor = Predictor ( output = [ 'sensor3' ]) sensor3_predictor . learn ( from_data = pandas . read_csv ( 'sensor_data.csv' )) Predict You can now be given new readings from sensor1 and sensor2 predict what sensor3 will be. prediction = sensor3_predictor . predict ( when = { 'sensor1' : 1 , 'sensor2' : - 1 }) print ( prediction ) Of course, that example was just the tip of the iceberg, please read below about the main concepts of lightwood, the API and then jump into examples. Lightwood Predictor API from lightwood import Predictor Lightwood has one main class; The Predictor , which is a modular construct that you can train and get predictions from. It is made out of 3 main building blocks ( features, encoders, mixers ) that you can configure, modify and expand as you wish. Building blocks Features : input_features : These are the columns in your dataset that you want to take as input for your predictor. output_features : These are the columns in your dataset that you want to learn how to predict. Encoders : These are tools to turn the data in your input or output features into vector/tensor representations and vice-versa. Mixers : How you mix the output of encoded features and also other mixers Constructor, __init__() my_predictor = Predictor ( output = [] | config = { ... } | load_from_path =< file_path > ) Predictor, can take any of the following arguments load_from_path : If you have a saved predictor that you want to load, just give the path to the file output : A list with the column names you want to predict. ( Note: If you pass this argument, lightwood will simply try to guess the best config possible ) config : A dictionary, containing the configuration on how to glue all the building blocks. config The config argument allows you to pass a dictionary that defines and gives you absolute control over how to build your predictive model. A config example goes as follows: from lightwood import COLUMN_DATA_TYPES , BUILTIN_MIXERS , BUILTIN_ENCODERS config = { ## REQUIRED: 'input_features' : [ # by default each feature has an encoder, so all you have to do is specify the data type { 'name' : 'sensor1' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, { 'name' : 'sensor2' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, # some encoders have attributes that can be specified on the configuration # in this particular lets assume we have a photo of the product, we would like to encode this image and optimize for speed { 'name' : 'product_photo' , 'type' : COLUMN_DATA_TYPES . IMAGE , 'encoder_class' : BUILTIN_ENCODERS . Image . Img2VecEncoder , # note that this is just a class, you can build your own if you wish 'encoder_attrs' : { 'aim' : 'speed' # you can check the encoder attributes here: # https://github.com/mindsdb/lightwood/blob/master/lightwood/encoders/image/img_2_vec.py } } ], 'output_features' : [ { 'name' : 'action_to_take' , 'type' : COLUMN_DATA_TYPES . CATEGORICAL } ], ## OPTIONAL 'mixer' : { 'class' : BUILTIN_MIXERS . NnMixer } } features Both input_features and output_features configs are simple dicts that have the following schema { 'name' : str , Optional ( 'type' ): any of COLUMN_DATA_TYPES , Optional ( 'encoder_class' ): object , Optional ( 'encoder_attrs' ): dict } name : is the name of the column as it is in the input data frame type : is the type of data contained. Where out of the box, supported COLUMN_DATA_TYPES are NUMERIC, CATEGORICAL, DATETIME, IMAGE, TEXT, TIME_SERIES : If you specify the type, lightwood will use the default encoder for that type, however, you can specify/define any encoder that you want to use. encoder_class : This is if you want to replace the default encoder with a different one, so you put the encoder class there encoder_attrs : These are the attributes that you want to setup on the encoder once the class its initialized mixer The default_mixer key, provides information as to what mixer to use. The schema for this variable is as follows: mixer_schema = Schema ({ 'class' : object , Optional ( 'attrs' ): dict }) class : It's the actual class, that defines the Mixer, you can use any of the BUILTIN_MIXERS or pass your own. attrs : This is a dictionary containing the attributes you want to replace on the mixer object once its initialized. We do this, so you have maximum flexibility as to what you can customize on your Mixers. learn() my_predictor . learn ( from_data = pandas_dataframe ) This method is used to make the predictor learn from some data, thus the learn method takes the following arguments. from_data : A pandas dataframe, that has some or all the columns in the config. The reason why we decide to only support pandas dataframes, its because, its easy to load any data to a pandas dataframe, and spark for python dataframe is a format we support. test_data : (Optional) This is if you want to specify what data to test with, if no test_data passed, lightwood will break the from_data into test and train automatically. callback_on_iter : (Optional) This is function callback that is called every 100 epochs during the learn process. predict() my_predictor . predict ( when = { .. } | when_data = pandas_dataframe ) This method is used to make predictions and it can take one of the following arguments when : this is a dictionary of conditions to predict under. when_data : Sometimes you want to predict more than one row at a time, so here it is: a pandas dataframe containing the conditional values you want to use to make a prediction. save() my_predictor . save ( path_to = string to path ) Use this method to save the predictor into a desired path calculate_accuracy() print ( my_predictor . calculate_accuracy ( from_data = data_source )) Returns the predictors overall accuracy.","title":"Info"},{"location":"lightwood/info/#prerequisites","text":"Python >=3.6 64bit version","title":"Prerequisites"},{"location":"lightwood/info/#installing-lightwood","text":"You can install Lightwood using pip : pip3 install lightwood If this fails, please report the bug on github and try installing the current master branch: git clone git@github.com:mindsdb/lightwood.git ; cd lightwood ; pip install --no-cache-dir -e . Please note that, depending on your os and python setup, you might want to use pip instead of pip3 . You need python 3.6 or higher. Note on MacOS, you need to install libomp: brew install libomp","title":"Installing Lightwood"},{"location":"lightwood/info/#install-using-virtual-environment","text":"We suggest you to install Lightwood on a virtual environment to avoid dependency issues. Make sure your Python version is >=3.6. To set up a virtual environment:","title":"Install using virtual environment"},{"location":"lightwood/info/#install-on-windows","text":"Install the latest version of pip : python -m pip install --upgrade pip pip --version Activate your virtual environment and install lightwood: py -m pip install --user virtualenv . \\e nv \\S cripts \\a ctivate pip install lightwood You can also use python instead of py","title":"Install on Windows"},{"location":"lightwood/info/#install-on-linux-or-macos","text":"Before installing Lightwood in a virtual environment you need to first create and activate the venv : python -m venv env source env/bin/activate pip install lightwood","title":"Install on Linux or macOS"},{"location":"lightwood/info/#quick-example","text":"Assume that you have a training file (sensor_data.csv) such as this one. sensor1 sensor2 sensor3 1 -1 -1 0 1 0 -1 -1 1 1 0 0 0 1 0 -1 1 -1 0 0 0 -1 -1 1 1 0 0 And you would like to learn to predict the values of sensor3 given the readings in sensor1 and sensor2 .","title":"Quick example"},{"location":"lightwood/info/#learn","text":"You can train a Predictor as follows: from lightwood import Predictor import pandas sensor3_predictor = Predictor ( output = [ 'sensor3' ]) sensor3_predictor . learn ( from_data = pandas . read_csv ( 'sensor_data.csv' ))","title":"Learn"},{"location":"lightwood/info/#predict","text":"You can now be given new readings from sensor1 and sensor2 predict what sensor3 will be. prediction = sensor3_predictor . predict ( when = { 'sensor1' : 1 , 'sensor2' : - 1 }) print ( prediction ) Of course, that example was just the tip of the iceberg, please read below about the main concepts of lightwood, the API and then jump into examples.","title":"Predict"},{"location":"lightwood/info/#lightwood-predictor-api","text":"from lightwood import Predictor Lightwood has one main class; The Predictor , which is a modular construct that you can train and get predictions from. It is made out of 3 main building blocks ( features, encoders, mixers ) that you can configure, modify and expand as you wish. Building blocks Features : input_features : These are the columns in your dataset that you want to take as input for your predictor. output_features : These are the columns in your dataset that you want to learn how to predict. Encoders : These are tools to turn the data in your input or output features into vector/tensor representations and vice-versa. Mixers : How you mix the output of encoded features and also other mixers","title":"Lightwood Predictor API"},{"location":"lightwood/info/#constructor-__init__","text":"my_predictor = Predictor ( output = [] | config = { ... } | load_from_path =< file_path > ) Predictor, can take any of the following arguments load_from_path : If you have a saved predictor that you want to load, just give the path to the file output : A list with the column names you want to predict. ( Note: If you pass this argument, lightwood will simply try to guess the best config possible ) config : A dictionary, containing the configuration on how to glue all the building blocks.","title":"Constructor, __init__()"},{"location":"lightwood/info/#config","text":"The config argument allows you to pass a dictionary that defines and gives you absolute control over how to build your predictive model. A config example goes as follows: from lightwood import COLUMN_DATA_TYPES , BUILTIN_MIXERS , BUILTIN_ENCODERS config = { ## REQUIRED: 'input_features' : [ # by default each feature has an encoder, so all you have to do is specify the data type { 'name' : 'sensor1' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, { 'name' : 'sensor2' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, # some encoders have attributes that can be specified on the configuration # in this particular lets assume we have a photo of the product, we would like to encode this image and optimize for speed { 'name' : 'product_photo' , 'type' : COLUMN_DATA_TYPES . IMAGE , 'encoder_class' : BUILTIN_ENCODERS . Image . Img2VecEncoder , # note that this is just a class, you can build your own if you wish 'encoder_attrs' : { 'aim' : 'speed' # you can check the encoder attributes here: # https://github.com/mindsdb/lightwood/blob/master/lightwood/encoders/image/img_2_vec.py } } ], 'output_features' : [ { 'name' : 'action_to_take' , 'type' : COLUMN_DATA_TYPES . CATEGORICAL } ], ## OPTIONAL 'mixer' : { 'class' : BUILTIN_MIXERS . NnMixer } }","title":"config"},{"location":"lightwood/info/#features","text":"Both input_features and output_features configs are simple dicts that have the following schema { 'name' : str , Optional ( 'type' ): any of COLUMN_DATA_TYPES , Optional ( 'encoder_class' ): object , Optional ( 'encoder_attrs' ): dict } name : is the name of the column as it is in the input data frame type : is the type of data contained. Where out of the box, supported COLUMN_DATA_TYPES are NUMERIC, CATEGORICAL, DATETIME, IMAGE, TEXT, TIME_SERIES : If you specify the type, lightwood will use the default encoder for that type, however, you can specify/define any encoder that you want to use. encoder_class : This is if you want to replace the default encoder with a different one, so you put the encoder class there encoder_attrs : These are the attributes that you want to setup on the encoder once the class its initialized","title":"features"},{"location":"lightwood/info/#mixer","text":"The default_mixer key, provides information as to what mixer to use. The schema for this variable is as follows: mixer_schema = Schema ({ 'class' : object , Optional ( 'attrs' ): dict }) class : It's the actual class, that defines the Mixer, you can use any of the BUILTIN_MIXERS or pass your own. attrs : This is a dictionary containing the attributes you want to replace on the mixer object once its initialized. We do this, so you have maximum flexibility as to what you can customize on your Mixers.","title":"mixer"},{"location":"lightwood/info/#learn_1","text":"my_predictor . learn ( from_data = pandas_dataframe ) This method is used to make the predictor learn from some data, thus the learn method takes the following arguments. from_data : A pandas dataframe, that has some or all the columns in the config. The reason why we decide to only support pandas dataframes, its because, its easy to load any data to a pandas dataframe, and spark for python dataframe is a format we support. test_data : (Optional) This is if you want to specify what data to test with, if no test_data passed, lightwood will break the from_data into test and train automatically. callback_on_iter : (Optional) This is function callback that is called every 100 epochs during the learn process.","title":"learn()"},{"location":"lightwood/info/#predict_1","text":"my_predictor . predict ( when = { .. } | when_data = pandas_dataframe ) This method is used to make predictions and it can take one of the following arguments when : this is a dictionary of conditions to predict under. when_data : Sometimes you want to predict more than one row at a time, so here it is: a pandas dataframe containing the conditional values you want to use to make a prediction.","title":"predict()"},{"location":"lightwood/info/#save","text":"my_predictor . save ( path_to = string to path ) Use this method to save the predictor into a desired path","title":"save()"},{"location":"lightwood/info/#calculate_accuracy","text":"print ( my_predictor . calculate_accuracy ( from_data = data_source )) Returns the predictors overall accuracy.","title":"calculate_accuracy()"},{"location":"model/clickhouse/","text":"Train a model from ClickHouse database Train new model To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsDB database and tables Note that after connecting MindsDB and ClickHouse , on start, the MindsDB server will automatically create the mindsDB database and add the predictors table. The INSERT query for training a new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1, feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Train new model example The following example shows you how to train a new model from the ClickHouse client. The table used for training the model is the Air Pollution in Seoul timeseries dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called pollution_measurement that predicts the passenger SO2 value. Since, this is a timeseries dataset, the timeseries_settings will order the data by the Measurement date column and will set the window for rows to \"look back\" when making a prediction. Model training status To check that the training finished successfully, you can SELECT from mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from a ClickHouse database. The next step is to get predictions by querying the model .","title":"Train a model from ClickHouse database"},{"location":"model/clickhouse/#train-a-model-from-clickhouse-database","text":"","title":"Train a model from ClickHouse database"},{"location":"model/clickhouse/#train-new-model","text":"To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsDB database and tables Note that after connecting MindsDB and ClickHouse , on start, the MindsDB server will automatically create the mindsDB database and add the predictors table. The INSERT query for training a new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1, feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"model/clickhouse/#train-new-model-example","text":"The following example shows you how to train a new model from the ClickHouse client. The table used for training the model is the Air Pollution in Seoul timeseries dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called pollution_measurement that predicts the passenger SO2 value. Since, this is a timeseries dataset, the timeseries_settings will order the data by the Measurement date column and will set the window for rows to \"look back\" when making a prediction.","title":"Train new model example"},{"location":"model/clickhouse/#model-training-status","text":"To check that the training finished successfully, you can SELECT from mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from a ClickHouse database. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/mariadb/","text":"How to train a model from MariaDB? How to train a new model To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb.predictors table Note that after connecting MindsDB and MariaDB , on start, the MindsDB server will automatically create the mindsdb database and add the predictors table. Prerequisite Don't forget to enable CONNECT Storage Engine as explained in connect your data section . The INSERT query for training new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Timeseries To train timeseries model, check out the timeseries example . Train new model example The following example shows you how to train a new model from the MariaDB client. The table used for training the model is the Used cars dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM data.used_cars_data' ); The INSERT query will train a new model called used_cars_model that predicts the cars price value. How to check model training status? To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from the MariaDB database. The next step is to get predictions by querying the model .","title":"How to train a model from MariaDB?"},{"location":"model/mariadb/#how-to-train-a-model-from-mariadb","text":"","title":"How to train a model from MariaDB?"},{"location":"model/mariadb/#how-to-train-a-new-model","text":"To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb.predictors table Note that after connecting MindsDB and MariaDB , on start, the MindsDB server will automatically create the mindsdb database and add the predictors table. Prerequisite Don't forget to enable CONNECT Storage Engine as explained in connect your data section . The INSERT query for training new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Timeseries To train timeseries model, check out the timeseries example .","title":"How to train a new model"},{"location":"model/mariadb/#train-new-model-example","text":"The following example shows you how to train a new model from the MariaDB client. The table used for training the model is the Used cars dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM data.used_cars_data' ); The INSERT query will train a new model called used_cars_model that predicts the cars price value.","title":"Train new model example"},{"location":"model/mariadb/#how-to-check-model-training-status","text":"To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from the MariaDB database. The next step is to get predictions by querying the model .","title":"How to check model training status?"},{"location":"model/mongodb/","text":"Train a model from the MongoDB API Train new model To train a new model, you will need to insert() a new document inside the mindsdb.predictors collection. The object sent to the insert() for training the new model should contain: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a list of features. connection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used. select_data_query (object) -- The object that contains info about getting the data to train the model. database(string) - The name of the database collection(string) - The name of the collection find(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...}) training_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide training_options . db . predictors . insert ( { 'name' : str , 'predict' : str | list of fields , 'connection' : str , # optional 'select_data_query' : { 'database' : str , 'collection' : str , 'find' : dict } , 'training_options' : dict # optional } ) For the timeseries model: db.predictors.insert({ 'name': str, 'predict': str | list of fields, 'connection': str, # optional 'select_data_query':{ 'database': str, 'collection': str, 'find': dict }, 'training_options': { \"timeseries_settings\": { \"order_by\": list of fields, \"group_by\": list of fields, #optional \"nr_predictions\": int, #optional \"use_previous_target\": Boolean, \"window\": int } } }) Train new model example The following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. db . predictors . insert ( { 'name' : 'churn' , 'predict' : 'Churn' , 'select_data_query' : { 'database' : 'test_data' , 'collection' : 'customer_churn' , 'find' : {} } } ) This INSERT query will train a new model called churn that predicts the customer Churn value. Model training status To check that the training finished successfully, you can find() the model status inside mindsdb.predictors collection e.g.: db . predictors . find () That's it You have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model . Delete model To delete the model run remove function on predictors collection and send the name of the model to delete as: db.predictors.remove({name: 'model_name'})","title":"Train a model from the MongoDB API"},{"location":"model/mongodb/#train-a-model-from-the-mongodb-api","text":"","title":"Train a model from the MongoDB API"},{"location":"model/mongodb/#train-new-model","text":"To train a new model, you will need to insert() a new document inside the mindsdb.predictors collection. The object sent to the insert() for training the new model should contain: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a list of features. connection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used. select_data_query (object) -- The object that contains info about getting the data to train the model. database(string) - The name of the database collection(string) - The name of the collection find(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...}) training_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide training_options . db . predictors . insert ( { 'name' : str , 'predict' : str | list of fields , 'connection' : str , # optional 'select_data_query' : { 'database' : str , 'collection' : str , 'find' : dict } , 'training_options' : dict # optional } ) For the timeseries model: db.predictors.insert({ 'name': str, 'predict': str | list of fields, 'connection': str, # optional 'select_data_query':{ 'database': str, 'collection': str, 'find': dict }, 'training_options': { \"timeseries_settings\": { \"order_by\": list of fields, \"group_by\": list of fields, #optional \"nr_predictions\": int, #optional \"use_previous_target\": Boolean, \"window\": int } } })","title":"Train new model"},{"location":"model/mongodb/#train-new-model-example","text":"The following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. db . predictors . insert ( { 'name' : 'churn' , 'predict' : 'Churn' , 'select_data_query' : { 'database' : 'test_data' , 'collection' : 'customer_churn' , 'find' : {} } } ) This INSERT query will train a new model called churn that predicts the customer Churn value.","title":"Train new model example"},{"location":"model/mongodb/#model-training-status","text":"To check that the training finished successfully, you can find() the model status inside mindsdb.predictors collection e.g.: db . predictors . find () That's it You have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/mongodb/#delete-model","text":"To delete the model run remove function on predictors collection and send the name of the model to delete as: db.predictors.remove({name: 'model_name'})","title":"Delete model"},{"location":"model/mssql/","text":"Train a model from Microsoft SQL Server Train new model To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb.predictors table Note that after connecting the MindsDB and Microsoft SQL server , on start, the MindsDB server will automatically create the mindsdb database and add the predictors table. Prerequisite Don't forget to install the prerequisites as explained in connect your data section . There are both options for training the model by using the openquery or exec statement. The INSERT query for training new model using exec statement: exec ( 'INSERT INTO mindsdb.predictors (name, predict, select_data_query) VALUES (''model_name'', ''target_variable'', ''SELECT * FROM table_name'')' ) AT mindsdb ; Or, openquery : INSERT openquery ( mindsdb , 'SELECT name, predict, select_data_query FROM mindsdb.predictors WHERE 1=0' ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Train new model example The following example shows you how to train a new model from a mssql client. The table used for training the model is the Medical insurance dataset. exec ( 'INSERT INTO mindsdb.predictors (name, predict, select_data_query) VALUES (\"insurance_model\", \"charges\", \"SELECT * FROM mindsdb_test.dbo.insurance\")' ) AT mindsdb ; This INSERT query will train a new model called insurance_model that predicts the charges value. Model training status To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: exec ( 'SELECT * FROM mindsdb.predictors' ) AT mindsdb ; Note: mindsdb is the name of the api['mysql]['database'] key from config.json. The default name is mindsdb . That's it You have successfully trained a new model from a Microsoft SQL Server. The next step is to get predictions by querying the model . Train time series model example The table used for training the model is the Air Pollution in Seoul timeseries dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called pollution_measurement that predicts the passenger SO2 value. Since, this is a timeseries dataset, the timeseries_settings will order the data by the Measurement date column and will set the window for rows to \"look back\" when making a prediction. Model training status To check that the training finished successfully, you can SELECT from mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ;","title":"Train a model from Microsoft SQL Server"},{"location":"model/mssql/#train-a-model-from-microsoft-sql-server","text":"","title":"Train a model from Microsoft SQL Server"},{"location":"model/mssql/#train-new-model","text":"To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb.predictors table Note that after connecting the MindsDB and Microsoft SQL server , on start, the MindsDB server will automatically create the mindsdb database and add the predictors table. Prerequisite Don't forget to install the prerequisites as explained in connect your data section . There are both options for training the model by using the openquery or exec statement. The INSERT query for training new model using exec statement: exec ( 'INSERT INTO mindsdb.predictors (name, predict, select_data_query) VALUES (''model_name'', ''target_variable'', ''SELECT * FROM table_name'')' ) AT mindsdb ; Or, openquery : INSERT openquery ( mindsdb , 'SELECT name, predict, select_data_query FROM mindsdb.predictors WHERE 1=0' ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface .","title":"Train new model"},{"location":"model/mssql/#train-new-model-example","text":"The following example shows you how to train a new model from a mssql client. The table used for training the model is the Medical insurance dataset. exec ( 'INSERT INTO mindsdb.predictors (name, predict, select_data_query) VALUES (\"insurance_model\", \"charges\", \"SELECT * FROM mindsdb_test.dbo.insurance\")' ) AT mindsdb ; This INSERT query will train a new model called insurance_model that predicts the charges value.","title":"Train new model example"},{"location":"model/mssql/#model-training-status","text":"To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: exec ( 'SELECT * FROM mindsdb.predictors' ) AT mindsdb ; Note: mindsdb is the name of the api['mysql]['database'] key from config.json. The default name is mindsdb . That's it You have successfully trained a new model from a Microsoft SQL Server. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/mssql/#train-time-series-model-example","text":"The table used for training the model is the Air Pollution in Seoul timeseries dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called pollution_measurement that predicts the passenger SO2 value. Since, this is a timeseries dataset, the timeseries_settings will order the data by the Measurement date column and will set the window for rows to \"look back\" when making a prediction.","title":"Train time series model example"},{"location":"model/mssql/#model-training-status_1","text":"To check that the training finished successfully, you can SELECT from mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ;","title":"Model training status"},{"location":"model/mysql/","text":"Train a model from MySQL database Train new model To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb.predictors table Note that after connecting the MindsDB and MySQL servers , on start, the MindsDB server will automatically create the mindsdb database and add the predictors table. Prerequisite Don't forget to enable FEDERATED Storage Engine as explained in connect your data section . The INSERT query for training new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Timeseries To train timeseries model, check out the timeseries example . Train new model example The following example shows you how to train a new model from a mysql client. The table used for training the model is the Us consumption dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called airq_predictor that predicts the SO2 value. Since this is timeseries data, the timeseries_settings will order the data by the t column and will set the window for rows to \"look back\" when making a prediction. Model training status To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from a MySQL database. The next step is to get predictions by querying the model .","title":"Train a model from MySQL database"},{"location":"model/mysql/#train-a-model-from-mysql-database","text":"","title":"Train a model from MySQL database"},{"location":"model/mysql/#train-new-model","text":"To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb.predictors table Note that after connecting the MindsDB and MySQL servers , on start, the MindsDB server will automatically create the mindsdb database and add the predictors table. Prerequisite Don't forget to enable FEDERATED Storage Engine as explained in connect your data section . The INSERT query for training new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Timeseries To train timeseries model, check out the timeseries example .","title":"Train new model"},{"location":"model/mysql/#train-new-model-example","text":"The following example shows you how to train a new model from a mysql client. The table used for training the model is the Us consumption dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called airq_predictor that predicts the SO2 value. Since this is timeseries data, the timeseries_settings will order the data by the t column and will set the window for rows to \"look back\" when making a prediction.","title":"Train new model example"},{"location":"model/mysql/#model-training-status","text":"To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from a MySQL database. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/postgresql/","text":"Train a model from PostgreSQL database Train new model To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb schema and tables Note that after connecting MindsDB and PostgreSQL , on start, the MindsDB server will automatically create the mindsDB schema and add the predictors table. Prerequisite Don't forget to install the MySQL foreign data wrapper as explained in connect your data section . The INSERT query for training a new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Timeseries To train timeseries model, check out the timeseries example . Train new model example The following example shows you how to train a new model from a psql client. The table used for training the model is the Airline Passenger Satisfaction dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'airline_survey_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This INSERT query will train a new model called airline_survey_model that predicts the passenger satisfaction value. Model training status To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from PostgreSQL database. The next step is to get predictions by querying the model .","title":"Train a model from PostgreSQL database"},{"location":"model/postgresql/#train-a-model-from-postgresql-database","text":"","title":"Train a model from PostgreSQL database"},{"location":"model/postgresql/#train-new-model","text":"To train a new model, you will need to INSERT a new record inside the mindsdb.predictors table. How to create the mindsb schema and tables Note that after connecting MindsDB and PostgreSQL , on start, the MindsDB server will automatically create the mindsDB schema and add the predictors table. Prerequisite Don't forget to install the MySQL foreign data wrapper as explained in connect your data section . The INSERT query for training a new model is quite simple, e.g.: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in the INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a comma separated string, e.g. 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . Timeseries To train timeseries model, check out the timeseries example .","title":"Train new model"},{"location":"model/postgresql/#train-new-model-example","text":"The following example shows you how to train a new model from a psql client. The table used for training the model is the Airline Passenger Satisfaction dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'airline_survey_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This INSERT query will train a new model called airline_survey_model that predicts the passenger satisfaction value.","title":"Train new model example"},{"location":"model/postgresql/#model-training-status","text":"To check that the training finished successfully, you can SELECT from the mindsdb.predictors table and get the training status, e.g.: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's it You have successfully trained a new model from PostgreSQL database. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/quality/","text":"Evaluate model quality using Studio Once the model is trained, you can use the model quality preview to get insights about the trained model. Visualize model quality Note that any model can be evaluated using MindsDB Studio. That means not only models trained with Studio but also models trained from SQL clients, MindsDB APIs or the SDK. To do this: From the left navigation menu, select the Predictors dashboard. Click on the PREVIEW button on the model you want to evaluate. Click on the minus sign to expand each section. Model Accuracy In this section, MindsDB will show you visualizations about: Data splitting (80% training data and 20% test data) Model accuracy (How accurate is the model when blindsided) Column importance This section tries to answer the question, What is important for this model? . MindsDB tries various combinations of missing columns to determine the importance of each one. The column importance rating ranges from 0, which means the column is useless to 10, meaning the column's predictive ability is great. Confusion matrix This section tries to answer the question, When can you trust this model? . Here, the confusion matrix shows the performance that the model gets when solving a classification problem. Each entry in the confusion matrix shows how many samples of each class were correctly classified and how many and where incorrectly classified. To get a detailed explanation, you can move the mouse cursor over the graphics. Let's query the model After evaluating the performance of the model, the next step is to get predictions by querying the model .","title":"Evaluate model quality using Studio"},{"location":"model/quality/#evaluate-model-quality-using-studio","text":"Once the model is trained, you can use the model quality preview to get insights about the trained model. Visualize model quality Note that any model can be evaluated using MindsDB Studio. That means not only models trained with Studio but also models trained from SQL clients, MindsDB APIs or the SDK. To do this: From the left navigation menu, select the Predictors dashboard. Click on the PREVIEW button on the model you want to evaluate. Click on the minus sign to expand each section.","title":"Evaluate model quality using Studio"},{"location":"model/quality/#model-accuracy","text":"In this section, MindsDB will show you visualizations about: Data splitting (80% training data and 20% test data) Model accuracy (How accurate is the model when blindsided)","title":"Model Accuracy"},{"location":"model/quality/#column-importance","text":"This section tries to answer the question, What is important for this model? . MindsDB tries various combinations of missing columns to determine the importance of each one. The column importance rating ranges from 0, which means the column is useless to 10, meaning the column's predictive ability is great.","title":"Column importance"},{"location":"model/quality/#confusion-matrix","text":"This section tries to answer the question, When can you trust this model? . Here, the confusion matrix shows the performance that the model gets when solving a classification problem. Each entry in the confusion matrix shows how many samples of each class were correctly classified and how many and where incorrectly classified. To get a detailed explanation, you can move the mouse cursor over the graphics. Let's query the model After evaluating the performance of the model, the next step is to get predictions by querying the model .","title":"Confusion matrix"},{"location":"model/timeseries/","text":"Train time series model example To train new timeseries model using SQL you will need to insert an object to the training_options coulmn. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table' , '{ \"timeseries_settings\": { \"order_by\": [\"order\"], \"group_by\": [\"group\"], \"nr_predictions\": 1, \"use_previous_target\": True, \"window\": 10 }}' ); You can specify the following keys inside the timeseries_settings : order_by (list of strings) -- The list of columns names on which the data should be ordered. group_by (list of strings) -- The list of columns based on which to group multiple unrelated entities present in your timeseries data. nr_predictions (int) -- use_previous_target (Boolean) -- Use the previous values of the target column[s] for making predictions. Defaults to True window (int) -- The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. Could be used to specify something like \"Always use the previous 10 rows\". Please, check the following example to get more info. The table used for training the model is the Air Pollution in Seoul timeseries dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called airq_model that predicts the passenger SO2 value. Since, this is a timeseries dataset, the timeseries_settings will order the data by the Measurement date column and will set the window for rows to look back when making a prediction.","title":"Train time series model example"},{"location":"model/timeseries/#train-time-series-model-example","text":"To train new timeseries model using SQL you will need to insert an object to the training_options coulmn. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table' , '{ \"timeseries_settings\": { \"order_by\": [\"order\"], \"group_by\": [\"group\"], \"nr_predictions\": 1, \"use_previous_target\": True, \"window\": 10 }}' ); You can specify the following keys inside the timeseries_settings : order_by (list of strings) -- The list of columns names on which the data should be ordered. group_by (list of strings) -- The list of columns based on which to group multiple unrelated entities present in your timeseries data. nr_predictions (int) -- use_previous_target (Boolean) -- Use the previous values of the target column[s] for making predictions. Defaults to True window (int) -- The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. Could be used to specify something like \"Always use the previous 10 rows\". Please, check the following example to get more info. The table used for training the model is the Air Pollution in Seoul timeseries dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called airq_model that predicts the passenger SO2 value. Since, this is a timeseries dataset, the timeseries_settings will order the data by the Measurement date column and will set the window for rows to look back when making a prediction.","title":"Train time series model example"},{"location":"model/train/","text":"Train new model using Studio Before training the new model, you must connect to a Datasource. To learn how to do that, check out: Connect to MySQL Connect to PostgreSQL Connect to MariaDB Connect to ClickHouse Connect to Microsoft SQL Server Connect to MongoDB Atlas Connect to remote URL Upload local dataset Connect to Snowflake Basic mode Training a new model from MindsDB Studio is quite easy: From the left navigation menu, open the Predictors dashboard. Click on the TRAIN NEW button. In the New Predictor modal window: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press TRAIN . Additional training options In the Advanced mode , you can find a few additional options when training the Machine Learning model, such as using a GPU for training, excluding columns from training or changing the sample margin of error. To do this, inside the New Predictor modal window: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the USE GPU checkbox. Check the columns that you want to exclude from model training. Add the sample margin of error value. Press TRAIN . Timeseries To build a timeseries model, you need to select the Yes, it is timeseries checkbox inside the Advanced Mode section: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the Yes, it is. checkbox. Select the Order by: value, the column based on which the data should be ordered. Select the Group by: value, the column based on which to group multiple entities in the timeseries data. Add the Look Back Window: value, the number of rows to look back into when making a prediction. Press TRAIN . That's it You have successfully trained a new Machine Learning model. The next step is to evaluate the model quality .","title":"Train new model using Studio"},{"location":"model/train/#train-new-model-using-studio","text":"Before training the new model, you must connect to a Datasource. To learn how to do that, check out: Connect to MySQL Connect to PostgreSQL Connect to MariaDB Connect to ClickHouse Connect to Microsoft SQL Server Connect to MongoDB Atlas Connect to remote URL Upload local dataset Connect to Snowflake","title":"Train new model using Studio"},{"location":"model/train/#basic-mode","text":"Training a new model from MindsDB Studio is quite easy: From the left navigation menu, open the Predictors dashboard. Click on the TRAIN NEW button. In the New Predictor modal window: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press TRAIN .","title":"Basic mode"},{"location":"model/train/#additional-training-options","text":"In the Advanced mode , you can find a few additional options when training the Machine Learning model, such as using a GPU for training, excluding columns from training or changing the sample margin of error. To do this, inside the New Predictor modal window: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the USE GPU checkbox. Check the columns that you want to exclude from model training. Add the sample margin of error value. Press TRAIN .","title":"Additional training options"},{"location":"model/train/#timeseries","text":"To build a timeseries model, you need to select the Yes, it is timeseries checkbox inside the Advanced Mode section: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the Yes, it is. checkbox. Select the Order by: value, the column based on which the data should be ordered. Select the Group by: value, the column based on which to group multiple entities in the timeseries data. Add the Look Back Window: value, the number of rows to look back into when making a prediction. Press TRAIN . That's it You have successfully trained a new Machine Learning model. The next step is to evaluate the model quality .","title":"Timeseries"},{"location":"model/query/clickhouse/","text":"Query the model from ClickHouse database This section assumes that you have trained a new model using the ClickHouse client or MindsDB Studio . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the ClickHouse database, you will be able to query it from other databases too. Using functions inside WHERE clause There is an issue where ClickHouse is not parsing the functions sent inside the WHERE clause. So, for e.g toDate() or toDateTime() inside WHERE will not work. For now you can avoid using the functions until we got feedback from ClickHouse. Track the progress of this issue here . Query example The following example shows you how to train a new model from the ClickHouse client. The table used for training the model is the Air Pollution in Seoul timeseries dataset. MindsDB will predict the SO2 (Sulfur dioxide) value in the air based on the values added in the WHERE statement. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a response from MindsDB similar toas: SO2 confidence info 0.009897379182791115 0.99 Check JSON below i nf o :{ \"predicted_value\" : 0.009897379182791113 , \"confidence\" : 0.99 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.0059810733322441575 , 0.01381368503333807 ], \"important_missing_information\" : [ \"Address\" ] }","title":"Query the model from ClickHouse database"},{"location":"model/query/clickhouse/#query-the-model-from-clickhouse-database","text":"This section assumes that you have trained a new model using the ClickHouse client or MindsDB Studio . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the ClickHouse database, you will be able to query it from other databases too. Using functions inside WHERE clause There is an issue where ClickHouse is not parsing the functions sent inside the WHERE clause. So, for e.g toDate() or toDateTime() inside WHERE will not work. For now you can avoid using the functions until we got feedback from ClickHouse. Track the progress of this issue here .","title":"Query the model from ClickHouse database"},{"location":"model/query/clickhouse/#query-example","text":"The following example shows you how to train a new model from the ClickHouse client. The table used for training the model is the Air Pollution in Seoul timeseries dataset. MindsDB will predict the SO2 (Sulfur dioxide) value in the air based on the values added in the WHERE statement. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a response from MindsDB similar toas: SO2 confidence info 0.009897379182791115 0.99 Check JSON below i nf o :{ \"predicted_value\" : 0.009897379182791113 , \"confidence\" : 0.99 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.0059810733322441575 , 0.01381368503333807 ], \"important_missing_information\" : [ \"Address\" ] }","title":"Query example"},{"location":"model/query/mariadb/","text":"Query the model from MariaDB database Prerequisite Don't forget to enable CONNECT Storage Engine as explained in connect your data section . This section assumes that you have trained a new model using MariaDB or MindsDB Studio . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the MariaDB database, you will be able to query it from other databases too. Query example The following example shows you how to train a new model from the MariaDB client. The table used for training the model is the Used cars dataset. MindsDB will predict the price based on the values added in the WHERE clause. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a response from MindsDB similar to: price confidence info 16117 0.98 Check JSON below i nf o : { \"predicted_value\" : 16117.627834024992 , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10737.135673357996 , 21498.119994691988 ], \"important_missing_information\" : [] }","title":"Query the model from MariaDB database"},{"location":"model/query/mariadb/#query-the-model-from-mariadb-database","text":"Prerequisite Don't forget to enable CONNECT Storage Engine as explained in connect your data section . This section assumes that you have trained a new model using MariaDB or MindsDB Studio . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the MariaDB database, you will be able to query it from other databases too.","title":"Query the model from MariaDB database"},{"location":"model/query/mariadb/#query-example","text":"The following example shows you how to train a new model from the MariaDB client. The table used for training the model is the Used cars dataset. MindsDB will predict the price based on the values added in the WHERE clause. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a response from MindsDB similar to: price confidence info 16117 0.98 Check JSON below i nf o : { \"predicted_value\" : 16117.627834024992 , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10737.135673357996 , 21498.119994691988 ], \"important_missing_information\" : [] }","title":"Query example"},{"location":"model/query/mongodb/","text":"Query the model from MongoDB API This section assumes that you have trained a new model using MongoDB client or MindsDB Studio . To get the predictions from the model, you will need to call find() method on the model collection and provide values for which you want to get prediction as an object: db . model_name . find ( { 'key' : 'value' , 'key' : 'value' } ) Note The object provided to find() method must be valid JSON format. Query example The following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer Churn value based on the object values sent to find() method. db . churn . find ( { 'PhoneService' : 'Yes' , 'InternetService' : 'DSL' , 'OnlineService' : 'No' , 'MonthlyCharges' : 53 . 85 , 'TotalCharges' : 108 . 15 , 'tenure' : 2 , 'PaperlessBilling' : 'Yes' } ) You should get a response from MindsDB similar to: predicted_value confidence info Yes 0.8 Check JSON below { \"Churn\" : \"Yes\" , \"Churn_confidence\" : 0.8 , \"Churn_explain\" : { \"class_distribution\" : { \"No\" : 0.44513007027299717 , \"Yes\" : 0.5548699297270028 }, \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Contract\" ] } }","title":"Query the model from MongoDB API"},{"location":"model/query/mongodb/#query-the-model-from-mongodb-api","text":"This section assumes that you have trained a new model using MongoDB client or MindsDB Studio . To get the predictions from the model, you will need to call find() method on the model collection and provide values for which you want to get prediction as an object: db . model_name . find ( { 'key' : 'value' , 'key' : 'value' } ) Note The object provided to find() method must be valid JSON format.","title":"Query the model from MongoDB API"},{"location":"model/query/mongodb/#query-example","text":"The following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer Churn value based on the object values sent to find() method. db . churn . find ( { 'PhoneService' : 'Yes' , 'InternetService' : 'DSL' , 'OnlineService' : 'No' , 'MonthlyCharges' : 53 . 85 , 'TotalCharges' : 108 . 15 , 'tenure' : 2 , 'PaperlessBilling' : 'Yes' } ) You should get a response from MindsDB similar to: predicted_value confidence info Yes 0.8 Check JSON below { \"Churn\" : \"Yes\" , \"Churn_confidence\" : 0.8 , \"Churn_explain\" : { \"class_distribution\" : { \"No\" : 0.44513007027299717 , \"Yes\" : 0.5548699297270028 }, \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Contract\" ] } }","title":"Query example"},{"location":"model/query/mssql/","text":"Query the model from Microsoft SQL Server This section assumes that you have trained a new model using SQL client or MindsDB Studio . Prerequisite Don't forget to install the prerequisites as explained in connect your data section . To query the model, you will need to SELECT from the model table: exec ( 'SELECT <target_variable> AS predicted, <target_variable_confidence> AS confidence, <target_variable_explain> AS info FROM mindsdb.<model_name> WHERE <feature value>' ) AT mindsdb ; Query the model from other databases Note that even if you have trained the model from the Microsoft SQL Server, you will be able to query it from other databases too. Query example The following example shows you how to query the model from a mssql client. The table used for training the model is the Medical insurance dataset. MindsDB will predict the charges based on the values added in when_data . exec ( 'SELECT charges AS predicted, charges_confidence AS confidence, charges_explain AS info FROM mindsdb.insurance_model WHERE age=30' ) AT mindsdb ; You should get a response from MindsDB similar to: predicted confidence info 7571.147 0.9 Check JSON below i nf o : { \"predicted_value\" : 7571.147932782108 , \"confidence\" : 0.9 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"smoker\" , \"bmi\" ]","title":"Query the model from Microsoft SQL Server"},{"location":"model/query/mssql/#query-the-model-from-microsoft-sql-server","text":"This section assumes that you have trained a new model using SQL client or MindsDB Studio . Prerequisite Don't forget to install the prerequisites as explained in connect your data section . To query the model, you will need to SELECT from the model table: exec ( 'SELECT <target_variable> AS predicted, <target_variable_confidence> AS confidence, <target_variable_explain> AS info FROM mindsdb.<model_name> WHERE <feature value>' ) AT mindsdb ; Query the model from other databases Note that even if you have trained the model from the Microsoft SQL Server, you will be able to query it from other databases too.","title":"Query the model from Microsoft SQL Server"},{"location":"model/query/mssql/#query-example","text":"The following example shows you how to query the model from a mssql client. The table used for training the model is the Medical insurance dataset. MindsDB will predict the charges based on the values added in when_data . exec ( 'SELECT charges AS predicted, charges_confidence AS confidence, charges_explain AS info FROM mindsdb.insurance_model WHERE age=30' ) AT mindsdb ; You should get a response from MindsDB similar to: predicted confidence info 7571.147 0.9 Check JSON below i nf o : { \"predicted_value\" : 7571.147932782108 , \"confidence\" : 0.9 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"smoker\" , \"bmi\" ]","title":"Query example"},{"location":"model/query/mysql/","text":"Query the model from MySQL database This section assumes that you have trained a new model using MySQL or MindsDB Studio . Prerequisite Don't forget to enable FEDERATED Storage Engine as explained in connect your data section . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE when_data =< JSON features values > Query the model from other databases Note that even if you have trained the model from the MySQL database, you will be able to query it from other databases too. Query example The following example shows you how to query the model from a MySQL client. The table used for training the model is the Us consumption dataset. MindsDB will predict the consumption based on the values added in when_data . SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a response from MindsDB similar to: consumption confidence info 1.0 0.93 Check JSON below i nf o : { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.93 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [] }","title":"Query the model from MySQL database"},{"location":"model/query/mysql/#query-the-model-from-mysql-database","text":"This section assumes that you have trained a new model using MySQL or MindsDB Studio . Prerequisite Don't forget to enable FEDERATED Storage Engine as explained in connect your data section . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE when_data =< JSON features values > Query the model from other databases Note that even if you have trained the model from the MySQL database, you will be able to query it from other databases too.","title":"Query the model from MySQL database"},{"location":"model/query/mysql/#query-example","text":"The following example shows you how to query the model from a MySQL client. The table used for training the model is the Us consumption dataset. MindsDB will predict the consumption based on the values added in when_data . SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a response from MindsDB similar to: consumption confidence info 1.0 0.93 Check JSON below i nf o : { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.93 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [] }","title":"Query example"},{"location":"model/query/postgresql/","text":"Query the model from PostgreSQL database This section assumes that you have trained a new model using psql or MindsDB Studio . Prerequisite Don't forget to install the MySQL foreign data wrapper as explained in connect your data section . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the PostgreSQL database, you will be able to query it from other databases too. Query example The following example shows you how to query the model from a psql client. The table used for training the model is the Airline Passenger sattisfaction dataset. MindsDB will predict the satisfaction based on the values added in the WHERE statement. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . airline_survey_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; You should get a response from MindsDB similar to: satisfaction confidence info satisfied 0.94 Check JSON below i nf o : { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"id\" , \"Inflight wifi service\" , \"Online boarding\" , \"Seat comfort\" , \"Baggage handling\" ] }","title":"Query the model from PostgreSQL database"},{"location":"model/query/postgresql/#query-the-model-from-postgresql-database","text":"This section assumes that you have trained a new model using psql or MindsDB Studio . Prerequisite Don't forget to install the MySQL foreign data wrapper as explained in connect your data section . To query the model, you will need to SELECT from the model table: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the PostgreSQL database, you will be able to query it from other databases too.","title":"Query the model from PostgreSQL database"},{"location":"model/query/postgresql/#query-example","text":"The following example shows you how to query the model from a psql client. The table used for training the model is the Airline Passenger sattisfaction dataset. MindsDB will predict the satisfaction based on the values added in the WHERE statement. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . airline_survey_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; You should get a response from MindsDB similar to: satisfaction confidence info satisfied 0.94 Check JSON below i nf o : { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"id\" , \"Inflight wifi service\" , \"Online boarding\" , \"Seat comfort\" , \"Baggage handling\" ] }","title":"Query example"},{"location":"model/query/studio/","text":"Query the model using MindsDB Studio To get predictive analytics from the trained model: From the left navigation menu, open the Query dashboard. Click on the NEW QUERY button. In the New Query modal window: Select the From predictor option(the name of the pre-trained model). Add the feature values for which you want to get predictions. Example: querying the UsedCars model In this example we want to solve the problem of estimating the right price for a used car that has Diesel as a fuel type and has done around 20 000 miles. To get the price: From the predictors dashboard, click on the Query button. Click on the NEW QUERY button. In the New Query modal window: Add the feature values for which you want to get predictions, e.g. Mileage 20 000. Fuel type Diesel.","title":"Query the model using MindsDB Studio"},{"location":"model/query/studio/#query-the-model-using-mindsdb-studio","text":"To get predictive analytics from the trained model: From the left navigation menu, open the Query dashboard. Click on the NEW QUERY button. In the New Query modal window: Select the From predictor option(the name of the pre-trained model). Add the feature values for which you want to get predictions.","title":"Query the model using MindsDB Studio"},{"location":"model/query/studio/#example-querying-the-usedcars-model","text":"In this example we want to solve the problem of estimating the right price for a used car that has Diesel as a fuel type and has done around 20 000 miles. To get the price: From the predictors dashboard, click on the Query button. Click on the NEW QUERY button. In the New Query modal window: Add the feature values for which you want to get predictions, e.g. Mileage 20 000. Fuel type Diesel.","title":"Example: querying the UsedCars model"},{"location":"mongo/connect/","text":"","title":"Connect"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/","text":"Forecast Metro Traffic using MindsDB Cloud and MongoDB Atlas In this tutorial, we will be learning to: Connect a MongoDB database to MindsDB. Train a model to predict metro traffic. Get a prediction from the model given certain input parameters. All of that without writing a single line of code and in less than 15 minutes . Yes, you read that right! We will be using the Metro traffic dataset that can be downloaded from here . You are also free to use your own dataset and follow along the tutorial. Pre-requisites This tutorial is primarily going to be about MindsDB so the reader is expected to have some level of familiarity with MongoDB Atlas . In short, MongoDB Atlas is a Database as a Service(DaaS), which we will be using to spin up a MongoDB database cluster and load our dataset. Download a copy of the Metro Traffic dataset from here . You are also expected to have an account on MindsDB Cloud. If not, head over to https://cloud.mindsdb.com/ and create an account. It hardly takes a few minutes. We also need MongoDB Compass to load the dataset into the collection. It can be downloaded from here . Finally, No! You are not required to have any background in programming or machine learning . As mentioned before you wont be writing a single line of code! About MindsDB MindsDB is a predictive platform that makes databases intelligent and machine learning easy to use. It allows data analysts to build and visualize forecasts in BI dashboards without going through the complexity of ML pipelines, all through SQL. It also helps data scientists to streamline MLOps by providing advanced instruments for in-database machine learning and optimize ML workflows through a declarative JSON-AI syntax. Although only SQL is mentioned, MongoDB is also supported. Dataset overview The dataset contains information about the: Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN . More details can be found here . The dataset is a .csv file that contains 9 columns: 1. holiday : Categorical US National holidays plus regional holiday, Minnesota State Fair 2. temp : Numeric Average temp in kelvin 3. rain_1h : Numeric Amount in mm of rain that occurred in the hour 4. snow_1h : Numeric Amount in mm of snow that occurred in the hour 5. clouds_all : Numeric Percentage of cloud cover 6. weather_main : Categorical Short textual description of the current weather 7. weather_description : Categorical Longer textual description of the current weather 8. date_time : DateTime Hour of the data collected in local CST time 9. traffic_volume : Numeric Hourly I-94 ATR 301 reported westbound traffic volume Phew! With all of that out of the way, we can finally get started! Setting up a Cluster on MongoDB Atlas Head over to https://cloud.mongodb.com/ and create a new project named mindsdb and within it a new database cluster named mindsDB . Typically, it takes a minute or two to provision a cluster. Once it is done, you should have something like this: Click on the \"Connect\" button. In the popup modal, you will be asked to add a connection IP address. Although not recommended, for the sake of this tutorial, choose \"Allow access from anywhere\" and then \"Add IP Address\". Next, you will be asked to create a new database user. After providing a username and password, click on the \"Create Database User\" button. In the next step, select \"Connect using MongoDB Compass\". Copy the connection string which should look like this: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/ We will now use this connection string to connect to our database from MongoDB Compass and load our dataset. Loading the dataset with MongoDB Compass Open MongoDB Compass. Paste the connection string and click on \"Connect\". On successful authentication, you will be welcomed by this screen. Click on \"Create Database\" and create a database named mindsDB and a collection named data . You will now see mindsDB listed. Click on it and you will see that it contains a collection named data . We will be loading data from the .csv file into this collection. Open the data collection by clicking on it. Click on the \"Import data\" button and load your .csv file. You will now be able to preview your dataset and also assign the data types as shown below. Then, \"import\" the dataset and wait for a few seconds for the import to finish. Connecting MindsDB to MongoDB Database Head over to https://cloud.mindsdb.com/ and click on \"Add Database\". Enter the required details as shown below. The connection string must be similar to: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/mindsDB Click on \"Connect\" and that's it! We have successfully linked our Database to MindsDB. Next, head over to the Datasets tab and click on \"From database\". Enter the details as shown below. In the Find field, we can specify a Mongo query using which MindsDB will include only the results of this query in the data source. By specifying {} , we are telling MindsDB to include every single document in the data collection in the data source. 6. Click on \"Create\" and now we will see that our data source named \"Metro Traffic Dataset\" has been added. One can check for the quality and also preview the data source. We are now ready to train an ML model to predict the traffic_volume using MindsDB. Training the ML Model Head over to the Predictors Tab and click on \"Train New\". In the popup modal, give a name to the predictor and select the column that needs to be predicted, which in our case is traffic_volume . After entering the details, click on \"Generate\". That's how simple training an ML model is with MindsDB. Now all you have to do is wait for a few minutes for the model to get trained after which you will be able to run queries and get predictions on the traffic_volume . Running Queries to get predictions Once the status changes to COMPLETE, it means that our model is now ready and we can start getting predictions. We can see that the model has an accuracy of 98.6%, which is impressive! To start getting predictions, click on the \"Query\" button and then \"New Query\". Let's say we wanted to know the traffic_volume for some day and all we know is the following: { temp: 300 , # temperature of 300 Kelvin clouds_all: 10 , # 10% cloud cover weather_main: \"Clouds\" , weather_description: \"few clouds\" , holiday: \"None\" } Enter the above details and \"Run Query\". We can see that the model predicted with 99% confidence that on such a day, the traffic volume would be 832. You can play with the inputs and run a few more queries and observe the results. What Next? This tutorial can be extended to perform lots of awesome things. For example, it would be interesting to see the dependence between the weather and the traffic volume. Some interesting questions that can be asked are: 1. Given a certain traffic_volume what is the probability the sky is clear ? What is the probability that it is raining? \ud83c\udf27\ufe0f 2. Given a certain traffic_volume , how certain can we be that the day is a holiday? \ud83c\udfd6\ufe0f 3. Can we predict more parameters instead of only the traffic_volume ? Apart from these, you can also install MindsDB on your machine and connect with your local databases to get predictions. You can also use a BI tool to visualize these predictions. Apart from SQL and MongoDB, you can explore other data source integrations that MindsDB supports like Oracle and Kafka. If you have made it this far, thank you for your time and hope you found this useful. If you like the article, like it and share it with others. Happy Querying!","title":"Forecast Metro Traffic using MindsDB Cloud and MongoDB Atlas"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#forecast-metro-traffic-using-mindsdb-cloud-and-mongodb-atlas","text":"In this tutorial, we will be learning to: Connect a MongoDB database to MindsDB. Train a model to predict metro traffic. Get a prediction from the model given certain input parameters. All of that without writing a single line of code and in less than 15 minutes . Yes, you read that right! We will be using the Metro traffic dataset that can be downloaded from here . You are also free to use your own dataset and follow along the tutorial.","title":"Forecast Metro Traffic using MindsDB Cloud and MongoDB Atlas"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#pre-requisites","text":"This tutorial is primarily going to be about MindsDB so the reader is expected to have some level of familiarity with MongoDB Atlas . In short, MongoDB Atlas is a Database as a Service(DaaS), which we will be using to spin up a MongoDB database cluster and load our dataset. Download a copy of the Metro Traffic dataset from here . You are also expected to have an account on MindsDB Cloud. If not, head over to https://cloud.mindsdb.com/ and create an account. It hardly takes a few minutes. We also need MongoDB Compass to load the dataset into the collection. It can be downloaded from here . Finally, No! You are not required to have any background in programming or machine learning . As mentioned before you wont be writing a single line of code!","title":"Pre-requisites"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#about-mindsdb","text":"MindsDB is a predictive platform that makes databases intelligent and machine learning easy to use. It allows data analysts to build and visualize forecasts in BI dashboards without going through the complexity of ML pipelines, all through SQL. It also helps data scientists to streamline MLOps by providing advanced instruments for in-database machine learning and optimize ML workflows through a declarative JSON-AI syntax. Although only SQL is mentioned, MongoDB is also supported.","title":"About MindsDB"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#dataset-overview","text":"The dataset contains information about the: Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN . More details can be found here . The dataset is a .csv file that contains 9 columns: 1. holiday : Categorical US National holidays plus regional holiday, Minnesota State Fair 2. temp : Numeric Average temp in kelvin 3. rain_1h : Numeric Amount in mm of rain that occurred in the hour 4. snow_1h : Numeric Amount in mm of snow that occurred in the hour 5. clouds_all : Numeric Percentage of cloud cover 6. weather_main : Categorical Short textual description of the current weather 7. weather_description : Categorical Longer textual description of the current weather 8. date_time : DateTime Hour of the data collected in local CST time 9. traffic_volume : Numeric Hourly I-94 ATR 301 reported westbound traffic volume Phew! With all of that out of the way, we can finally get started!","title":"Dataset overview"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#setting-up-a-cluster-on-mongodb-atlas","text":"Head over to https://cloud.mongodb.com/ and create a new project named mindsdb and within it a new database cluster named mindsDB . Typically, it takes a minute or two to provision a cluster. Once it is done, you should have something like this: Click on the \"Connect\" button. In the popup modal, you will be asked to add a connection IP address. Although not recommended, for the sake of this tutorial, choose \"Allow access from anywhere\" and then \"Add IP Address\". Next, you will be asked to create a new database user. After providing a username and password, click on the \"Create Database User\" button. In the next step, select \"Connect using MongoDB Compass\". Copy the connection string which should look like this: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/ We will now use this connection string to connect to our database from MongoDB Compass and load our dataset.","title":"Setting up a Cluster on MongoDB Atlas"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#loading-the-dataset-with-mongodb-compass","text":"Open MongoDB Compass. Paste the connection string and click on \"Connect\". On successful authentication, you will be welcomed by this screen. Click on \"Create Database\" and create a database named mindsDB and a collection named data . You will now see mindsDB listed. Click on it and you will see that it contains a collection named data . We will be loading data from the .csv file into this collection. Open the data collection by clicking on it. Click on the \"Import data\" button and load your .csv file. You will now be able to preview your dataset and also assign the data types as shown below. Then, \"import\" the dataset and wait for a few seconds for the import to finish.","title":"Loading the dataset with MongoDB Compass"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#connecting-mindsdb-to-mongodb-database","text":"Head over to https://cloud.mindsdb.com/ and click on \"Add Database\". Enter the required details as shown below. The connection string must be similar to: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/mindsDB Click on \"Connect\" and that's it! We have successfully linked our Database to MindsDB. Next, head over to the Datasets tab and click on \"From database\". Enter the details as shown below. In the Find field, we can specify a Mongo query using which MindsDB will include only the results of this query in the data source. By specifying {} , we are telling MindsDB to include every single document in the data collection in the data source. 6. Click on \"Create\" and now we will see that our data source named \"Metro Traffic Dataset\" has been added. One can check for the quality and also preview the data source. We are now ready to train an ML model to predict the traffic_volume using MindsDB.","title":"Connecting MindsDB to MongoDB Database"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#training-the-ml-model","text":"Head over to the Predictors Tab and click on \"Train New\". In the popup modal, give a name to the predictor and select the column that needs to be predicted, which in our case is traffic_volume . After entering the details, click on \"Generate\". That's how simple training an ML model is with MindsDB. Now all you have to do is wait for a few minutes for the model to get trained after which you will be able to run queries and get predictions on the traffic_volume .","title":"Training the ML Model"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#running-queries-to-get-predictions","text":"Once the status changes to COMPLETE, it means that our model is now ready and we can start getting predictions. We can see that the model has an accuracy of 98.6%, which is impressive! To start getting predictions, click on the \"Query\" button and then \"New Query\". Let's say we wanted to know the traffic_volume for some day and all we know is the following: { temp: 300 , # temperature of 300 Kelvin clouds_all: 10 , # 10% cloud cover weather_main: \"Clouds\" , weather_description: \"few clouds\" , holiday: \"None\" } Enter the above details and \"Run Query\". We can see that the model predicted with 99% confidence that on such a day, the traffic volume would be 832. You can play with the inputs and run a few more queries and observe the results.","title":"Running Queries to get predictions"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#what-next","text":"This tutorial can be extended to perform lots of awesome things. For example, it would be interesting to see the dependence between the weather and the traffic volume. Some interesting questions that can be asked are: 1. Given a certain traffic_volume what is the probability the sky is clear ? What is the probability that it is raining? \ud83c\udf27\ufe0f 2. Given a certain traffic_volume , how certain can we be that the day is a holiday? \ud83c\udfd6\ufe0f 3. Can we predict more parameters instead of only the traffic_volume ? Apart from these, you can also install MindsDB on your machine and connect with your local databases to get predictions. You can also use a BI tool to visualize these predictions. Apart from SQL and MongoDB, you can explore other data source integrations that MindsDB supports like Oracle and Kafka. If you have made it this far, thank you for your time and hope you found this useful. If you like the article, like it and share it with others. Happy Querying!","title":"What Next?"},{"location":"mongo/tutorials/test/","text":"Tutorials","title":"Tutorials"},{"location":"mongo/tutorials/test/#tutorials","text":"","title":"Tutorials"},{"location":"server/SDKs/","text":"Work In Progress documentation Note that the documentation for MindsDB SDK's is under development. If you found missing feature or something isn't working please reach out to us on the Python SDK or JS SDK repositories. MindsDB Server Note that to use MindsDB SDK, you will need to have MindsDB Server started so you can connect to it. The MindsDB SDK's are providing all of the MindsDB's native functionalities through MindsDB HTTP Interface. Currently, MindsDB provides SDK's for Python and JavaScript. Installing Python SDK The Python SDK can be installed from PyPI: pip install mindsdb_sdk Or you can install it from source: git clone git@github.com:mindsdb/mindsdb_python_sdk.git cd mindsdb_python_sdk python setup.py develop pip install -r requirements.txt Connect to your data DataSources make it very simple to connect MindsDB to your data. Datasource can be: File(csv, tsv, json, xslx, xls) Pandas dataframe MindsDB datasource that is an enriched version of a pandas dataframe. MindsDB datasource could be MariaDB, Snowflake, S3, Sqlite3, Redshift, PostgreSQL, MsSQL, MongoDB, GCS, Clickhouse, AWS Athena. For more info please check the full list of datasource implementation here . Create new datasource from local file Before you train new model you need to create a datasource, so MindsDB can ingest and prepare the data. The following example use Medical Cost dataset . Let's load the local file as an pandas dataframe and create new datasource: from mindsdb_sdk import SDK # import SDK import pandas as pd # import pandas mindsdb_sdk = SDK ( 'http://localhost:47334' ) # Connect to MindsDB Server URL datasources = mindsdb_sdk . datasources df = pd . read_csv ( 'datasets/insurance.csv' ) # read the dataset datasources [ 'health_insurance' ] = { 'df' : df } # create new datasource To check that the datasource was successfully created, you can call the list_info() method from datasources: datasources . list_info () This will return the info for each datasource as the following example: { ' na me' : 'heal t h_i nsuran ce' , 'source_ t ype' : ' f ile' , 'source' : '/mi n dsdb/var/da tas ources/heal t h_i nsuran ce/' , 'missed_ f iles' : No ne , 'crea te d_a t ' : ' 2021-02-19 T 14 : 20 : 57.860292 ' , 'upda te d_a t ' : ' 2021-02-19 T 14 : 20 : 57.908497 ' , 'row_cou nt ' : 27 , 'colum ns ' : [{ ' na me' : 'age' , ' t ype' : No ne , ' f ile_ t ype' : No ne , 'dic t ' : No ne } ... o t her colum ns } Train new model The learn method is used to make the predictor learn from the data. The required arguments to learn from data are model_name , target_variable , datasource . The following example train new model called insurance_model that predicts the charges from health_insurance datasource. model = mindsdb_sdk . predictors model . learn ( 'insurance_model' , 'charges' , 'health_insurance' ,) Query the model To get the predictons from the model use the predict method. This method is used to make predictions and it can take when_data argument. This can be file, dataframe or url or if you want to make the single predicton as in the following example use a dictionary. model . predict ( when_data = { 'age' : 30 , 'bmi' : 22 }) Usage example The following example trains new model from home_rentals dataset and predicts the rental price. from mindsdb_sdk import SDK # connect mdb = SDK ( 'http://localhost:47334' ) # upload datasource mdb . datasources [ 'home_rentals_data' ] = { 'file' : 'home_rentals.csv' } # create a new predictor and learn to predict predictor = mdb . predictors . learn ( name = 'home_rentals' , datasource = 'home_rentals_data' , to_predict = 'rental_price' ) # predict result = predictor . predict ({ 'initial_price' : '2000' , 'number_of_bathrooms' : '1' , 'sqft' : '700' }) Installing JavaScript SDK The JavaScript SDK can be installed with npm or yarn: npm install mindsdb-js-sdk or yarn add mindsdb-js-sdk Also, you can install it from source: git clone git@github.com:mindsdb/mindsdb_js_sdk.git cd mindsdb_js_sdk npm install Usage example The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. import MindsDB from 'mindsdb-js-sdk' ; //connection MindsDB . connect ( url ); const connected = await MindsDB . ping (); if ( ! connected ) return ; // lists of predictors and datasources const predictorsList = MindsDB . dataSources (); const predictors = MindsDB . predictors (); // get datasource const rentalsDatasource = await MindsDB . DataSource ({ name : 'home_rentals' }). load (); // get predictor const rentalsPredictor = await MindsDB . Predictor ({ name : 'home_rentals' }). load (); // query const result = rentalsPredictor . queryPredict ({ 'initial)|_price' : 2000 , 'sqft' : 500 }); console . log ( result ); MindsDB . disconnect ();","title":"SDKs"},{"location":"server/SDKs/#installing-python-sdk","text":"The Python SDK can be installed from PyPI: pip install mindsdb_sdk Or you can install it from source: git clone git@github.com:mindsdb/mindsdb_python_sdk.git cd mindsdb_python_sdk python setup.py develop pip install -r requirements.txt","title":"Installing Python SDK"},{"location":"server/SDKs/#connect-to-your-data","text":"DataSources make it very simple to connect MindsDB to your data. Datasource can be: File(csv, tsv, json, xslx, xls) Pandas dataframe MindsDB datasource that is an enriched version of a pandas dataframe. MindsDB datasource could be MariaDB, Snowflake, S3, Sqlite3, Redshift, PostgreSQL, MsSQL, MongoDB, GCS, Clickhouse, AWS Athena. For more info please check the full list of datasource implementation here .","title":"Connect to your data"},{"location":"server/SDKs/#create-new-datasource-from-local-file","text":"Before you train new model you need to create a datasource, so MindsDB can ingest and prepare the data. The following example use Medical Cost dataset . Let's load the local file as an pandas dataframe and create new datasource: from mindsdb_sdk import SDK # import SDK import pandas as pd # import pandas mindsdb_sdk = SDK ( 'http://localhost:47334' ) # Connect to MindsDB Server URL datasources = mindsdb_sdk . datasources df = pd . read_csv ( 'datasets/insurance.csv' ) # read the dataset datasources [ 'health_insurance' ] = { 'df' : df } # create new datasource To check that the datasource was successfully created, you can call the list_info() method from datasources: datasources . list_info () This will return the info for each datasource as the following example: { ' na me' : 'heal t h_i nsuran ce' , 'source_ t ype' : ' f ile' , 'source' : '/mi n dsdb/var/da tas ources/heal t h_i nsuran ce/' , 'missed_ f iles' : No ne , 'crea te d_a t ' : ' 2021-02-19 T 14 : 20 : 57.860292 ' , 'upda te d_a t ' : ' 2021-02-19 T 14 : 20 : 57.908497 ' , 'row_cou nt ' : 27 , 'colum ns ' : [{ ' na me' : 'age' , ' t ype' : No ne , ' f ile_ t ype' : No ne , 'dic t ' : No ne } ... o t her colum ns }","title":"Create new datasource from local file"},{"location":"server/SDKs/#train-new-model","text":"The learn method is used to make the predictor learn from the data. The required arguments to learn from data are model_name , target_variable , datasource . The following example train new model called insurance_model that predicts the charges from health_insurance datasource. model = mindsdb_sdk . predictors model . learn ( 'insurance_model' , 'charges' , 'health_insurance' ,)","title":"Train new model"},{"location":"server/SDKs/#query-the-model","text":"To get the predictons from the model use the predict method. This method is used to make predictions and it can take when_data argument. This can be file, dataframe or url or if you want to make the single predicton as in the following example use a dictionary. model . predict ( when_data = { 'age' : 30 , 'bmi' : 22 })","title":"Query the model"},{"location":"server/SDKs/#usage-example","text":"The following example trains new model from home_rentals dataset and predicts the rental price. from mindsdb_sdk import SDK # connect mdb = SDK ( 'http://localhost:47334' ) # upload datasource mdb . datasources [ 'home_rentals_data' ] = { 'file' : 'home_rentals.csv' } # create a new predictor and learn to predict predictor = mdb . predictors . learn ( name = 'home_rentals' , datasource = 'home_rentals_data' , to_predict = 'rental_price' ) # predict result = predictor . predict ({ 'initial_price' : '2000' , 'number_of_bathrooms' : '1' , 'sqft' : '700' })","title":"Usage example"},{"location":"server/SDKs/#installing-javascript-sdk","text":"The JavaScript SDK can be installed with npm or yarn: npm install mindsdb-js-sdk or yarn add mindsdb-js-sdk Also, you can install it from source: git clone git@github.com:mindsdb/mindsdb_js_sdk.git cd mindsdb_js_sdk npm install","title":"Installing JavaScript SDK"},{"location":"server/SDKs/#usage-example_1","text":"The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. import MindsDB from 'mindsdb-js-sdk' ; //connection MindsDB . connect ( url ); const connected = await MindsDB . ping (); if ( ! connected ) return ; // lists of predictors and datasources const predictorsList = MindsDB . dataSources (); const predictors = MindsDB . predictors (); // get datasource const rentalsDatasource = await MindsDB . DataSource ({ name : 'home_rentals' }). load (); // get predictor const rentalsPredictor = await MindsDB . Predictor ({ name : 'home_rentals' }). load (); // query const result = rentalsPredictor . queryPredict ({ 'initial)|_price' : 2000 , 'sqft' : 500 }); console . log ( result ); MindsDB . disconnect ();","title":"Usage example"},{"location":"sql/api/drop/","text":"DROP statement Work in progress Note this feature is in beta version. If you have additional questions or issues reach out to us on Slack . The DROP statement is used to delete an existing model table. DROP TABLE statement The DROP PREDICTOR statement is used to delete the model table: DROP PREDICTOR table_name ; DROP TABLE example The following SQL statement drops the model table called home_rentals_model : DROP PREDICTOR home_rentals_model ;","title":"DROP"},{"location":"sql/api/drop/#drop-statement","text":"Work in progress Note this feature is in beta version. If you have additional questions or issues reach out to us on Slack . The DROP statement is used to delete an existing model table.","title":"DROP statement"},{"location":"sql/api/drop/#drop-table-statement","text":"The DROP PREDICTOR statement is used to delete the model table: DROP PREDICTOR table_name ;","title":"DROP TABLE statement"},{"location":"sql/api/drop/#drop-table-example","text":"The following SQL statement drops the model table called home_rentals_model : DROP PREDICTOR home_rentals_model ;","title":"DROP TABLE example"},{"location":"sql/api/integration/","text":"CREATE DATASOURCE Statement The CREATE DATASOURCE statement is used to create new integration between MindsDB and your Database. The basic syntax for creating datasource is: CREATE DATASOURCE { name } FROM { db_type } WITH { json with parameters } CREATE PREDICTOR predictor_name - where predictor_name is the name of the model. FROM integration_name (select column_name, column_name2 FROM table_name) - where integration_name is the name of the datasource , where (select column_name, column_name2 FROM table_name) is the SELECT statement for selecting the data. If you want to change the default name of the datasource you can use the alias as ds_name . PREDICT column_name - where column_name is the column name of the target variable. If you want to change the name of the target variable you can use the as column_alias .","title":"CREATE DATASOURCE Statement"},{"location":"sql/api/integration/#create-datasource-statement","text":"The CREATE DATASOURCE statement is used to create new integration between MindsDB and your Database. The basic syntax for creating datasource is: CREATE DATASOURCE { name } FROM { db_type } WITH { json with parameters } CREATE PREDICTOR predictor_name - where predictor_name is the name of the model. FROM integration_name (select column_name, column_name2 FROM table_name) - where integration_name is the name of the datasource , where (select column_name, column_name2 FROM table_name) is the SELECT statement for selecting the data. If you want to change the default name of the datasource you can use the alias as ds_name . PREDICT column_name - where column_name is the column name of the target variable. If you want to change the name of the target variable you can use the as column_alias .","title":"CREATE DATASOURCE Statement"},{"location":"sql/api/join/","text":"JOIN clause The JOIN clause is used to combine rows from the database table and the model table on a related column. The basic syntax for joining from the data table and model is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); JOIN example The following SQL statement joins the home_rentals data with the home_rentals_model predicted price: SELECT * FROM db_integration . house_rentals_data AS t JOIN mindsdb . home_rentals_model AS tb WHERE t . neighborhood in ( 'downtown' , 'south_side' ); number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price select_data_query external_datasource when_data rental_price_original rental_price_confidence rental_price_explain rental_price_anomaly rental_price_min rental_price_max 0 1 484 great 10 2271 south_side 2271 nan nan nan nan 0.99 {\"predicted_value\": 2243, \"confidence\": 0.99, \"confidence_lower_bound\": 2200, \"confidence_upper_bound\": 2286, \"anomaly\": null, \"truth\": 2271} nan 2200 2286 1 1 674 good 1 2167 downtown 2167 nan nan nan nan 0.99 {\"predicted_value\": 2197, \"confidence\": 0.99, \"confidence_lower_bound\": 2154, \"confidence_upper_bound\": 2240, \"anomaly\": null, \"truth\": 2167} nan 2154 2240 0 1 529 great 3 2431 south_side 2431 nan nan nan nan 0.99 {\"predicted_value\": 2432, \"confidence\": 0.99, \"confidence_lower_bound\": 2389, \"confidence_upper_bound\": 2475, \"anomaly\": null, \"truth\": 2431} nan 2389 2475 3 2 1219 great 3 5510 south_side 5510 nan nan nan nan 0.99 {\"predicted_value\": 5550, \"confidence\": 0.99, \"confidence_lower_bound\": 5507, \"confidence_upper_bound\": 5593, \"anomaly\": null, \"truth\": 5510} nan 5507 5593 1 1 398 great 11 2272 south_side 2272 nan nan nan nan 0.99 {\"predicted_value\": 2252, \"confidence\": 0.99, \"confidence_lower_bound\": 2209, \"confidence_upper_bound\": 2295, \"anomaly\": null, \"truth\": 2272} nan 2209 2295","title":"JOIN"},{"location":"sql/api/join/#join-clause","text":"The JOIN clause is used to combine rows from the database table and the model table on a related column. The basic syntax for joining from the data table and model is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...);","title":"JOIN clause"},{"location":"sql/api/join/#join-example","text":"The following SQL statement joins the home_rentals data with the home_rentals_model predicted price: SELECT * FROM db_integration . house_rentals_data AS t JOIN mindsdb . home_rentals_model AS tb WHERE t . neighborhood in ( 'downtown' , 'south_side' ); number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price select_data_query external_datasource when_data rental_price_original rental_price_confidence rental_price_explain rental_price_anomaly rental_price_min rental_price_max 0 1 484 great 10 2271 south_side 2271 nan nan nan nan 0.99 {\"predicted_value\": 2243, \"confidence\": 0.99, \"confidence_lower_bound\": 2200, \"confidence_upper_bound\": 2286, \"anomaly\": null, \"truth\": 2271} nan 2200 2286 1 1 674 good 1 2167 downtown 2167 nan nan nan nan 0.99 {\"predicted_value\": 2197, \"confidence\": 0.99, \"confidence_lower_bound\": 2154, \"confidence_upper_bound\": 2240, \"anomaly\": null, \"truth\": 2167} nan 2154 2240 0 1 529 great 3 2431 south_side 2431 nan nan nan nan 0.99 {\"predicted_value\": 2432, \"confidence\": 0.99, \"confidence_lower_bound\": 2389, \"confidence_upper_bound\": 2475, \"anomaly\": null, \"truth\": 2431} nan 2389 2475 3 2 1219 great 3 5510 south_side 5510 nan nan nan nan 0.99 {\"predicted_value\": 5550, \"confidence\": 0.99, \"confidence_lower_bound\": 5507, \"confidence_upper_bound\": 5593, \"anomaly\": null, \"truth\": 5510} nan 5507 5593 1 1 398 great 11 2272 south_side 2272 nan nan nan nan 0.99 {\"predicted_value\": 2252, \"confidence\": 0.99, \"confidence_lower_bound\": 2209, \"confidence_upper_bound\": 2295, \"anomaly\": null, \"truth\": 2272} nan 2209 2295","title":"JOIN example"},{"location":"sql/api/predictor/","text":"CREATE PREDICTOR Statement The CREATE PREDICTOR statement is used to train new model. The basic syntax for training the model is: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; CREATE PREDICTOR predictor_name - where predictor_name is the name of the model. FROM integration_name (select column_name, column_name2 FROM table_name) - where integration_name is the name of the datasource , where (select column_name, column_name2 FROM table_name) is the SELECT statement for selecting the data. If you want to change the default name of the datasource you can use the alias as ds_name . PREDICT column_name - where column_name is the column name of the target variable. If you want to change the name of the target variable you can use the as column_alias . Example Data The below database table contains prices of properties from a metropolitan area in the US. This table will be used in all of the docs examples. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 0 1 529 great 3 2431 south_side 2431 3 2 1219 great 3 5510 south_side 5510 1 1 398 great 11 2272 south_side 2272 Create Predictor example This example shows how you can train the Machine Learning Model called home_rentals_model to predict the rentals price from the above data. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; SELECT Predictor status After you run the CREATE Predictor statement, you can check the status of the training model, by selecting from mindsdb.predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ; SELECT Predictor example To check the training status for the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; USING keyword The USING keyword accepts arguments as a JSON format where additional arguments can be provided to the CREATE PREDICTOR statement as: stop_train_in_x_seconds - Stop model training after X seconds. use_gpu - Switch between training on CPU or GPU (True|False). sample_margin_of_error - The amount of random sampling error in results (0 - 1) ignore_columns - Columns to be removed from the model training. is_timeseries - Training from time series data (True|False). CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias USING { \"ignore_columns\" : \"column_name3\" } ; USING example The following example trains the new home_rentals_model model which predicts the rental_price and removes the number of bathrooms. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price USING { \"ignore_columns\" : \"number_of_bathrooms\" } ; Time Series keywords To train a timeseries model, MindsDB provides additional keywords. WINDOW - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". HORIZON - keyword specifies the number of future predictions. CREATE PREDICTOR predictor_name FROM db_integration ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias GROUP BY column_name WINDOW 10 HORIZON 7 ; USING { \"is_timeseries\" : \"Yes\" } ; ORDER BY keyword The ORDER BY keyword is used to order the data by descending (DESC) or ascending (ASC) order. The default order will always be ASC CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ORDER BY column_name column_name2 ASC OR DESC ; ORDER BY ASC example The following example trains the new home_rentals_model model which predicts the rental_price and orders the data in ascending order by the number of days on the market. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ORDER BY days_on_market ASC ; ORDER BY DESC example The following example trains the new home_rentals_model model which predicts the rental_price and orders the data in descending order by the number of days on the market. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ORDER BY days_on_market DESC ; GROUP BY statement The GROUP BY statement is used to group the rows that contain the same values into one row. CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias GROUP BY column_name ; GROUP BY example The following example trains the new home_rentals_model model which predicts the rental_price and groups the data per location (good,great). CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price GROUP BY location ;","title":"Predictor"},{"location":"sql/api/predictor/#create-predictor-statement","text":"The CREATE PREDICTOR statement is used to train new model. The basic syntax for training the model is: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; CREATE PREDICTOR predictor_name - where predictor_name is the name of the model. FROM integration_name (select column_name, column_name2 FROM table_name) - where integration_name is the name of the datasource , where (select column_name, column_name2 FROM table_name) is the SELECT statement for selecting the data. If you want to change the default name of the datasource you can use the alias as ds_name . PREDICT column_name - where column_name is the column name of the target variable. If you want to change the name of the target variable you can use the as column_alias .","title":"CREATE PREDICTOR Statement"},{"location":"sql/api/predictor/#example-data","text":"The below database table contains prices of properties from a metropolitan area in the US. This table will be used in all of the docs examples. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 0 1 529 great 3 2431 south_side 2431 3 2 1219 great 3 5510 south_side 5510 1 1 398 great 11 2272 south_side 2272","title":"Example Data"},{"location":"sql/api/predictor/#create-predictor-example","text":"This example shows how you can train the Machine Learning Model called home_rentals_model to predict the rentals price from the above data. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ;","title":"Create Predictor example"},{"location":"sql/api/predictor/#select-predictor-status","text":"After you run the CREATE Predictor statement, you can check the status of the training model, by selecting from mindsdb.predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ;","title":"SELECT Predictor status"},{"location":"sql/api/predictor/#select-predictor-example","text":"To check the training status for the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ;","title":"SELECT Predictor example"},{"location":"sql/api/predictor/#using-keyword","text":"The USING keyword accepts arguments as a JSON format where additional arguments can be provided to the CREATE PREDICTOR statement as: stop_train_in_x_seconds - Stop model training after X seconds. use_gpu - Switch between training on CPU or GPU (True|False). sample_margin_of_error - The amount of random sampling error in results (0 - 1) ignore_columns - Columns to be removed from the model training. is_timeseries - Training from time series data (True|False). CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias USING { \"ignore_columns\" : \"column_name3\" } ;","title":"USING keyword"},{"location":"sql/api/predictor/#using-example","text":"The following example trains the new home_rentals_model model which predicts the rental_price and removes the number of bathrooms. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price USING { \"ignore_columns\" : \"number_of_bathrooms\" } ;","title":"USING example"},{"location":"sql/api/predictor/#time-series-keywords","text":"To train a timeseries model, MindsDB provides additional keywords. WINDOW - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". HORIZON - keyword specifies the number of future predictions. CREATE PREDICTOR predictor_name FROM db_integration ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias GROUP BY column_name WINDOW 10 HORIZON 7 ; USING { \"is_timeseries\" : \"Yes\" } ;","title":"Time Series keywords"},{"location":"sql/api/predictor/#order-by-keyword","text":"The ORDER BY keyword is used to order the data by descending (DESC) or ascending (ASC) order. The default order will always be ASC CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ORDER BY column_name column_name2 ASC OR DESC ;","title":"ORDER BY keyword"},{"location":"sql/api/predictor/#order-by-asc-example","text":"The following example trains the new home_rentals_model model which predicts the rental_price and orders the data in ascending order by the number of days on the market. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ORDER BY days_on_market ASC ;","title":"ORDER BY ASC example"},{"location":"sql/api/predictor/#order-by-desc-example","text":"The following example trains the new home_rentals_model model which predicts the rental_price and orders the data in descending order by the number of days on the market. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ORDER BY days_on_market DESC ;","title":"ORDER BY DESC example"},{"location":"sql/api/predictor/#group-by-statement","text":"The GROUP BY statement is used to group the rows that contain the same values into one row. CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias GROUP BY column_name ;","title":"GROUP BY statement"},{"location":"sql/api/predictor/#group-by-example","text":"The following example trains the new home_rentals_model model which predicts the rental_price and groups the data per location (good,great). CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price GROUP BY location ;","title":"GROUP BY example"},{"location":"sql/api/publish/","text":"PUBLISH statement","title":"PUBLISH statement"},{"location":"sql/api/publish/#publish-statement","text":"","title":"PUBLISH statement"},{"location":"sql/api/retrain/","text":"RETRAIN PREDICTOR Statement The RETRAIN statement is used to retrain old predictors. The basic syntax for retraining the predictors is: RETRAIN predictor_name ; The predictor is updated to leverage any new data in optimizing its predictive capabilities, without necessarily taking as long to train as starting from scratch. RETRAIN Predictor example This example shows how you can retrain the predictor called home_rentals_model . RETRAIN home_rentals_model ; SELECT Predictor status To check if the status of the predictor is outdated you can SELECT from predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ; SELECT Predictor example To check the status of the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; If the status is OUTDATED you can retrain the predictor.","title":"RETRAIN"},{"location":"sql/api/retrain/#retrain-predictor-statement","text":"The RETRAIN statement is used to retrain old predictors. The basic syntax for retraining the predictors is: RETRAIN predictor_name ; The predictor is updated to leverage any new data in optimizing its predictive capabilities, without necessarily taking as long to train as starting from scratch.","title":"RETRAIN PREDICTOR Statement"},{"location":"sql/api/retrain/#retrain-predictor-example","text":"This example shows how you can retrain the predictor called home_rentals_model . RETRAIN home_rentals_model ;","title":"RETRAIN Predictor example"},{"location":"sql/api/retrain/#select-predictor-status","text":"To check if the status of the predictor is outdated you can SELECT from predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ;","title":"SELECT Predictor status"},{"location":"sql/api/retrain/#select-predictor-example","text":"To check the status of the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; If the status is OUTDATED you can retrain the predictor.","title":"SELECT Predictor example"},{"location":"sql/api/select/","text":"SELECT statement The SELECT statement is used to get a predictions from the model table. The data is not persistent and is returned on the fly as a result-set. The basic syntax for selecting from the model is: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; Model table columns The below list contains the column names of the model table. Note that target_variable_ will be the name of the target variable column. target_variable_original - The original value of the target variable. target_variable_min - Lower bound of the predicted value. target_variable_max - Upper bound of the predicted value. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . when_data - The data to make the predictions from(WHERE clause params). select_data_query - SQL select query to create the datasource. external_datasource - Name of the pre-existing datasource that the model was built from. rental_price number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price_original rental_price_min rental_price_max rental_price_confidence rental_price_explain when_data select_data_query external_datasource 2450 4 2 800 good 12 2222 downtown nan 2407 2493 0.99 {\"predicted_value\": 2450, \"confidence\": 0.99, \"confidence_lower_bound\": 2407, \"confidence_upper_bound\": 2493, \"anomaly\": null, \"truth\": null} {\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"} nan nan SELECT example The following SQL statement selects all information from the home_rentals_model for the property that has \"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\". SELECT * FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ; The following SQL statement selects only the target variable rental_price as price and the home_rentals_model confidence as accuracy : SELECT rental_price as price , rental_price_confidence as confidence FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ;","title":"SELECT"},{"location":"sql/api/select/#select-statement","text":"The SELECT statement is used to get a predictions from the model table. The data is not persistent and is returned on the fly as a result-set. The basic syntax for selecting from the model is: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ;","title":"SELECT statement"},{"location":"sql/api/select/#model-table-columns","text":"The below list contains the column names of the model table. Note that target_variable_ will be the name of the target variable column. target_variable_original - The original value of the target variable. target_variable_min - Lower bound of the predicted value. target_variable_max - Upper bound of the predicted value. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . when_data - The data to make the predictions from(WHERE clause params). select_data_query - SQL select query to create the datasource. external_datasource - Name of the pre-existing datasource that the model was built from. rental_price number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price_original rental_price_min rental_price_max rental_price_confidence rental_price_explain when_data select_data_query external_datasource 2450 4 2 800 good 12 2222 downtown nan 2407 2493 0.99 {\"predicted_value\": 2450, \"confidence\": 0.99, \"confidence_lower_bound\": 2407, \"confidence_upper_bound\": 2493, \"anomaly\": null, \"truth\": null} {\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"} nan nan","title":"Model table columns"},{"location":"sql/api/select/#select-example","text":"The following SQL statement selects all information from the home_rentals_model for the property that has \"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\". SELECT * FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ; The following SQL statement selects only the target variable rental_price as price and the home_rentals_model confidence as accuracy : SELECT rental_price as price , rental_price_confidence as confidence FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ;","title":"SELECT example"},{"location":"sql/api/stream/","text":"Work in progress This documentation is in progress. If you want to get access to the beta version, reach out to us on Slack .","title":"Stream"},{"location":"sql/api/use/","text":"USE statement The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here . Preview the data To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"USE"},{"location":"sql/api/use/#use-statement","text":"The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here .","title":"USE statement"},{"location":"sql/api/use/#preview-the-data","text":"To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"Preview the data"},{"location":"sql/api/view/","text":"CREATE VIEW statement Work in progress Note this feature is in beta version. If you have additional questions or issues reach out to us on Slack . In MindsDB, the AI Table is a virtual table based on the result-set of the SQL Statement that JOINS the table data with the models prediction. The AI Table can be created using the CREATE AI table ai_table_name statement. CREATE VIEW ai_table_name as ( SELECT a . colum_name , a . colum_name2 , a . colum_name3 , p . model_column as model_column FROM integration_name . table_name as a JOIN predictor_name as p ); Example view The below table can be JOINED with the model trained from it as an AI Table. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 SQL Query for creating the home_rentals_model that predicts rental_price: CREATE PREDICTOR home_rentals_model FROM integration_name ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; Join the predicted rental_price from the model with the sqft , number_of_bathrooms , location from the table: CREATE VIEW home_rentals as ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price as price FROM mysql_db . home_rentals as a JOIN home_rentals_model as p );","title":"View"},{"location":"sql/api/view/#create-view-statement","text":"Work in progress Note this feature is in beta version. If you have additional questions or issues reach out to us on Slack . In MindsDB, the AI Table is a virtual table based on the result-set of the SQL Statement that JOINS the table data with the models prediction. The AI Table can be created using the CREATE AI table ai_table_name statement. CREATE VIEW ai_table_name as ( SELECT a . colum_name , a . colum_name2 , a . colum_name3 , p . model_column as model_column FROM integration_name . table_name as a JOIN predictor_name as p );","title":"CREATE VIEW statement"},{"location":"sql/api/view/#example-view","text":"The below table can be JOINED with the model trained from it as an AI Table. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 SQL Query for creating the home_rentals_model that predicts rental_price: CREATE PREDICTOR home_rentals_model FROM integration_name ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; Join the predicted rental_price from the model with the sqft , number_of_bathrooms , location from the table: CREATE VIEW home_rentals as ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price as price FROM mysql_db . home_rentals as a JOIN home_rentals_model as p );","title":"Example view"},{"location":"sql/connect/cloud/","text":"MindsDB Cloud as a SQL Database MindsDB Cloud provides a powerful MySQL API that allows cloud users to connect to it. The first step to connect is to use the MindsDB Cloud user. If you haven't signup to the MindsDB Cloud follow the steps explained here . After that you can use one of the below db clients: MySQL Command-Line Client DBeaver MySQL client Open mysql client and run: mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p The required parameters are: -h: Host name of mindsdbs mysql api (by default takes cloud.mindsdb.com if not specified) --port: TCP/IP port number for connection(3306) -u: MindsDB Cloud username -p: MindsDB Cloud password Dbeaver If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (cloud-mysql.mindsdb.com). Add the Database name (leave empty). Add Port (3306). Add the database user (your MindsDB Cloud username). Add Password for the user (your MindsDB Cloud password). Click on Finish . MindsDB Database At startup mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. external_datasource - Name of the pre-existing datasource created from GUI. training options - Additional training parameters. The full list can be found at Predictor Interface docs . Whitelist MindsDB Cloud IP address If you need to whitelist MindsDB Cloud IP address to have access to your database, reach out to MindsDB team so we can share the Cloud static IP with you.","title":"MindsDB Cloud"},{"location":"sql/connect/cloud/#mindsdb-cloud-as-a-sql-database","text":"MindsDB Cloud provides a powerful MySQL API that allows cloud users to connect to it. The first step to connect is to use the MindsDB Cloud user. If you haven't signup to the MindsDB Cloud follow the steps explained here . After that you can use one of the below db clients: MySQL Command-Line Client DBeaver","title":"MindsDB Cloud as a SQL Database"},{"location":"sql/connect/cloud/#mysql-client","text":"Open mysql client and run: mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p The required parameters are: -h: Host name of mindsdbs mysql api (by default takes cloud.mindsdb.com if not specified) --port: TCP/IP port number for connection(3306) -u: MindsDB Cloud username -p: MindsDB Cloud password","title":"MySQL client"},{"location":"sql/connect/cloud/#dbeaver","text":"If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (cloud-mysql.mindsdb.com). Add the Database name (leave empty). Add Port (3306). Add the database user (your MindsDB Cloud username). Add Password for the user (your MindsDB Cloud password). Click on Finish .","title":"Dbeaver"},{"location":"sql/connect/cloud/#mindsdb-database","text":"At startup mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. external_datasource - Name of the pre-existing datasource created from GUI. training options - Additional training parameters. The full list can be found at Predictor Interface docs . Whitelist MindsDB Cloud IP address If you need to whitelist MindsDB Cloud IP address to have access to your database, reach out to MindsDB team so we can share the Cloud static IP with you.","title":"MindsDB Database"},{"location":"sql/connect/local/","text":"MindsDB as a SQL Database MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command-Line Client or DBeaver . By default, MindsDB Server will start the HTTP and MySQL APIs. If you want to run only the MySQL API you can provide that as a parameter on the server start: python3 -m mindsdb --api=http,mysql This will start MySQL API on a 127.0.0.1:47335 with mindsdb as default user and create a mindsdb database. To change the default parameters you need to extend the MindsDBs config.json or create another config and send it as a parameter to the serve start command as: python3 -m mindsdb --api=http,mysql --config=config.json In case you are using Docker, visit the Docker extend config docs . To read more about available config.json options check the configuration docs . Connect Connecting to the localhost Make sure you always use 127.0.0.1 locally instead of localhost as a hostname. Connecting to MySQL API is the same as connecting to a MySQL database. You can use one of the below clients to connect: MySQL Command-Line Client DBeaver MySQL client Open mysql client and run: mysql -h 127.0.0.1 --port 47335 -u mindsdb -p The required parameters are: -h: Host name of mindsdbs mysql api (127.0.0.1). --port: TCP/IP port number for connection(47335). -u: MySQL user name to use when connecting(default mindsdb). -p: Password to use when connecting(default no password). Dbeaver If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (127.0.0.1). Add the Database name (leave empty). Add Port (47335). Add the database user (default mindsdb). Add Password for the user (default empty). Click on Finish . MindsDB Database On startup the mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. external_datasource - Name of the pre-existing datasource created from GUI. training options - Additional training parameters. The full list can be found at Predictor Interface docs .","title":"Local Deployment"},{"location":"sql/connect/local/#mindsdb-as-a-sql-database","text":"MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command-Line Client or DBeaver . By default, MindsDB Server will start the HTTP and MySQL APIs. If you want to run only the MySQL API you can provide that as a parameter on the server start: python3 -m mindsdb --api=http,mysql This will start MySQL API on a 127.0.0.1:47335 with mindsdb as default user and create a mindsdb database. To change the default parameters you need to extend the MindsDBs config.json or create another config and send it as a parameter to the serve start command as: python3 -m mindsdb --api=http,mysql --config=config.json In case you are using Docker, visit the Docker extend config docs . To read more about available config.json options check the configuration docs .","title":"MindsDB as a SQL Database"},{"location":"sql/connect/local/#connect","text":"Connecting to the localhost Make sure you always use 127.0.0.1 locally instead of localhost as a hostname. Connecting to MySQL API is the same as connecting to a MySQL database. You can use one of the below clients to connect: MySQL Command-Line Client DBeaver","title":"Connect"},{"location":"sql/connect/local/#mysql-client","text":"Open mysql client and run: mysql -h 127.0.0.1 --port 47335 -u mindsdb -p The required parameters are: -h: Host name of mindsdbs mysql api (127.0.0.1). --port: TCP/IP port number for connection(47335). -u: MySQL user name to use when connecting(default mindsdb). -p: Password to use when connecting(default no password).","title":"MySQL client"},{"location":"sql/connect/local/#dbeaver","text":"If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (127.0.0.1). Add the Database name (leave empty). Add Port (47335). Add the database user (default mindsdb). Add Password for the user (default empty). Click on Finish .","title":"Dbeaver"},{"location":"sql/connect/local/#mindsdb-database","text":"On startup the mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. external_datasource - Name of the pre-existing datasource created from GUI. training options - Additional training parameters. The full list can be found at Predictor Interface docs .","title":"MindsDB Database"},{"location":"sql/tutorials/ai-tables/","text":"AI Tables Intro There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence. Running MindsDB with Snowflake can significantly increase the performance and reduce the computational requirements of training your machine learning models. If you are already a Snowflake user, you can take advantage of key features quickly to enable fast, efficient machine learning capabilities directly in your database by simply connecting Snowflake to the MindsDB service. In this article, we will show you how to connect Snowflake to MindsDB, how you can enable real-time machine learning with Snowflake and MindsDB, and how to visualize the MindsDB predictions with Apache Superset. Machine Learning (ML) Lifecycle The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database. Deep Dive into the AI Tables Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"AI Tables intro"},{"location":"sql/tutorials/ai-tables/#ai-tables-intro","text":"There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence. Running MindsDB with Snowflake can significantly increase the performance and reduce the computational requirements of training your machine learning models. If you are already a Snowflake user, you can take advantage of key features quickly to enable fast, efficient machine learning capabilities directly in your database by simply connecting Snowflake to the MindsDB service. In this article, we will show you how to connect Snowflake to MindsDB, how you can enable real-time machine learning with Snowflake and MindsDB, and how to visualize the MindsDB predictions with Apache Superset.","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#machine-learning-ml-lifecycle","text":"The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database.","title":"Machine Learning (ML) Lifecycle"},{"location":"sql/tutorials/ai-tables/#deep-dive-into-the-ai-tables","text":"Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"Deep Dive into the AI Tables"},{"location":"sql/tutorials/bitcoin-forecasting/","text":"Forecast Bitcoin price using MindsDB Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax caled JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have. Pre-requisites First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API. Connect your database You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price. Create the model Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%! Create the prediction Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy! Conclusions As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#forecast-bitcoin-price-using-mindsdb","text":"Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax caled JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#pre-requisites","text":"First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-your-database","text":"You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-to-mindsdbs-mysql-api","text":"In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/bitcoin-forecasting/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price.","title":"Data"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-model","text":"Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%!","title":"Create the model"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-prediction","text":"Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy!","title":"Create the prediction"},{"location":"sql/tutorials/bitcoin-forecasting/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Conclusions"},{"location":"sql/tutorials/bodyfat/","text":"Determining Body Fat Percentage Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria. Pre-requisites A working copy of MindsDB. Check out the Docker or PyPi installation guides to install locally, or get up and running in seconds using MindsDB Cloud . Access to a MySQL Database. The dataset being used for this tutorial. Get it from Kaggle . mysql-client, DBeaver, MySQL Workbench, etc. to connect to the database and also to use MindsDB's MySQL API. Setting up the Database In this section you'll initialize your MySQL database and populate it with the Body Fat Prediction dataset. Data Overview For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: - Density: Individual's body density as determined by underwater weighing (float) - BodyFat: The individual's determined body fat percentage (float). This is what we want to predict - Age: Age of the individual (int) - Weight: Weight of the individual in pounds (float) - Height: Height of the individual in inches (float) - Neck: Circumference of the individual's neck in cm (float) - Chest: Circumference of the individual's chest in cm (float) - Abdomen: Circumference of the individual's abdomen in cm (float) - Hip: Circumference of the individual's hips in cm (float) - Thigh: Circumference of the individual's thigh in cm (float) - Knee: Circumference of the individual's knee in cm (float) - Ankle: Circumference of the individual's ankle in cm (float) - Biceps: Circumference of the individual's biceps in cm (float) - Forearm: Circumference of the individual's forearm in cm (float) - Wrist: Circumference of the individual's wrist in cm (float) First, connect to your MySQL instance. Next, create the database: CREATE DATABASE bodyfat ; Now switch to the newly created database: USE bodyfat ; Now create the table into which the dataset will be imported with the following schema: CREATE TABLE bodyfat ( Density FLOAT , BodyFat FLOAT , Age INT , Weight FLOAT , Height FLOAT , Neck FLOAT , Chest FLOAT , Abdomen FLOAT , Hip FLOAT , Thigh FLOAT , Knee FLOAT , Ankle FLOAT , Biceps FLOAT , Forearm FLOAT , Wrist FLOAT ); Now import the Body Fat Prediction dataset into the newly created table with: LOAD DATA LOCAL INFILE '/path/to/file/bodyfat.csv' INTO TABLE bodyfat FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' IGNORE 1 ROWS ; If successful, you should see output similar to: Query OK, 252 rows affected (0.028 sec) Records: 252 Deleted: 0 Skipped: 0 Warnings: 0 To test that the dataset was correctly imported, you can execute: SELECT Age , BodyFat FROM bodyfat LIMIT 10 ; Which should return output similar to: +------+---------+ | Age | BodyFat | +------+---------+ | 23 | 12.3 | | 22 | 6.1 | | 22 | 25.3 | | 26 | 10.4 | | 24 | 28.7 | | 24 | 20.9 | | 26 | 19.2 | | 25 | 12.4 | | 25 | 4.1 | | 23 | 11.7 | +------+---------+ 10 rows in set (0.001 sec) At this point, you have completed setting up the MySQL Database and loading the Body Fat Prediction dataset! Connect MindsDB to the Database In this section, you'll connect MindsDB to the database you've just set up. Start by heading to the MindsDB GUI. If you use an open-source version, launch MindsDB Studio, but for this tutorial, we'll be using MindsDB Cloud to connect to our database. Click on Databases in the upper left, then on Add Database in the lower right. In the popup screen, fill in the details for your MySQL database, and specify a name for the integration (here, we chose bodyfat_integration). You can test if the database is connectable by clicking Click Here to Test Connection , and if all is well, click on the Connect button: You should now see your new database integration appear in the MindsDB GUI: At this point, you have successfully connected MindsDB to the database! Connect to the MindsDB MySQL API Now, you'll connect to the MindsDB MySQL API to enable the use of SQL commands to train ML models and make predictions. For this tutorial, we'll be connecting to MindsDB Cloud using a commandline mysql client: mysql -h cloud.mindsdb.com --port 3306 -u username@email.com -p Execute the above command substituting the email address you used to sign up for MindsDB Cloud, followed by entering your password, and you should see output similar to: Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: 5.7.1-MindsDB-1.0 (MindsDB) Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [(none)]> You have now connected to the MindsDB MySQL API successfully! Using SQL Commands to Train ML Models We will now train a new machine learning model for the dataset we've created. In MindsDB terms it is called a Predictor. We will show how to create it automatically, but there is also a way to fine tune it, if you know what you are doing (check the MindsDB docs ). Go to your mysql-client and run the following command: USE mindsdb ; Now, we have to create a predictor based on the following syntax: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; For our case, we'll enter the following command: CREATE PREDICTOR bodyfat_predictor FROM bodyfat_integration ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; You should see output similar to the following: Query OK, 0 rows affected (3.077 sec) At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . predictors WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: +-------------------+----------+--------------------+---------+-------------------+---------------------+------------------+ | name | status | accuracy | predict | select_data_query | external_datasource | training_options | +-------------------+----------+--------------------+---------+-------------------+---------------------+------------------+ | bodyfat_predictor | complete | 0.9909730079130395 | BodyFat | | | | +-------------------+----------+--------------------+---------+-------------------+---------------------+------------------+ 1 row in set (0.101 sec) As you can see, the predictor training has completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset! Using SQL Commands to Make Predictions Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain AS Info FROM mindsdb . bodyfat_predictor WHERE when_data = '{\"Density\": 1.08, \"Age\": 25, \"Weight\": 170, \"Height\": 70, \"Neck\": 38.1, \"Chest\": 103.5, \"Abdomen\": 85.4, \"Hip\": 102.2, \"Thigh\": 63.0, \"Knee\": 39.4, \"Ankle\": 22.8, \"Biceps\": 33.3, \"Forearm\": 28.7, \"Wrist\": 18.3}' ; This should return output similar to: +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | BodyFat | BodyFat_confidence | Info | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 8.968581383955318 | 0.99 | {\"predicted_value\": 8.968581383955318, \"confidence\": 0.99, \"confidence_lower_bound\": 5.758912817402102, \"confidence_upper_bound\": 12.178249950508533, \"anomaly\": null, \"truth\": null} | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.464 sec) As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like. Making Batch Predictions using the JOIN Command The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN command is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | Age | Density | Weight | Height | Neck | Chest | Abdomen | Hip | BodyFat | predicted_BodyFat | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | 23 | 1.0708 | 154.25 | 67.75 | 36.2 | 93.1 | 85.2 | 94.5 | 12.3 | 12.475132275112655 | | 22 | 1.0853 | 173.25 | 72.25 | 38.5 | 93.6 | 83.0 | 98.7 | 6.1 | 6.07133439184195 | | 22 | 1.0414 | 154.0 | 66.25 | 34.0 | 95.8 | 87.9 | 99.2 | 25.3 | 25.156538398443754 | | 26 | 1.0751 | 184.75 | 72.25 | 37.4 | 101.8 | 86.4 | 101.2 | 10.4 | 10.696461885516461 | | 24 | 1.034 | 184.25 | 71.25 | 34.4 | 97.3 | 100.0 | 101.9 | 28.7 | 28.498772660802427 | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ 5 rows in set (1.091 sec) As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Bodyfat Prediction"},{"location":"sql/tutorials/bodyfat/#determining-body-fat-percentage","text":"Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria.","title":"Determining Body Fat Percentage"},{"location":"sql/tutorials/bodyfat/#pre-requisites","text":"A working copy of MindsDB. Check out the Docker or PyPi installation guides to install locally, or get up and running in seconds using MindsDB Cloud . Access to a MySQL Database. The dataset being used for this tutorial. Get it from Kaggle . mysql-client, DBeaver, MySQL Workbench, etc. to connect to the database and also to use MindsDB's MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/bodyfat/#setting-up-the-database","text":"In this section you'll initialize your MySQL database and populate it with the Body Fat Prediction dataset.","title":"Setting up the Database"},{"location":"sql/tutorials/bodyfat/#data-overview","text":"For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: - Density: Individual's body density as determined by underwater weighing (float) - BodyFat: The individual's determined body fat percentage (float). This is what we want to predict - Age: Age of the individual (int) - Weight: Weight of the individual in pounds (float) - Height: Height of the individual in inches (float) - Neck: Circumference of the individual's neck in cm (float) - Chest: Circumference of the individual's chest in cm (float) - Abdomen: Circumference of the individual's abdomen in cm (float) - Hip: Circumference of the individual's hips in cm (float) - Thigh: Circumference of the individual's thigh in cm (float) - Knee: Circumference of the individual's knee in cm (float) - Ankle: Circumference of the individual's ankle in cm (float) - Biceps: Circumference of the individual's biceps in cm (float) - Forearm: Circumference of the individual's forearm in cm (float) - Wrist: Circumference of the individual's wrist in cm (float) First, connect to your MySQL instance. Next, create the database: CREATE DATABASE bodyfat ; Now switch to the newly created database: USE bodyfat ; Now create the table into which the dataset will be imported with the following schema: CREATE TABLE bodyfat ( Density FLOAT , BodyFat FLOAT , Age INT , Weight FLOAT , Height FLOAT , Neck FLOAT , Chest FLOAT , Abdomen FLOAT , Hip FLOAT , Thigh FLOAT , Knee FLOAT , Ankle FLOAT , Biceps FLOAT , Forearm FLOAT , Wrist FLOAT ); Now import the Body Fat Prediction dataset into the newly created table with: LOAD DATA LOCAL INFILE '/path/to/file/bodyfat.csv' INTO TABLE bodyfat FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' IGNORE 1 ROWS ; If successful, you should see output similar to: Query OK, 252 rows affected (0.028 sec) Records: 252 Deleted: 0 Skipped: 0 Warnings: 0 To test that the dataset was correctly imported, you can execute: SELECT Age , BodyFat FROM bodyfat LIMIT 10 ; Which should return output similar to: +------+---------+ | Age | BodyFat | +------+---------+ | 23 | 12.3 | | 22 | 6.1 | | 22 | 25.3 | | 26 | 10.4 | | 24 | 28.7 | | 24 | 20.9 | | 26 | 19.2 | | 25 | 12.4 | | 25 | 4.1 | | 23 | 11.7 | +------+---------+ 10 rows in set (0.001 sec) At this point, you have completed setting up the MySQL Database and loading the Body Fat Prediction dataset!","title":"Data Overview"},{"location":"sql/tutorials/bodyfat/#connect-mindsdb-to-the-database","text":"In this section, you'll connect MindsDB to the database you've just set up. Start by heading to the MindsDB GUI. If you use an open-source version, launch MindsDB Studio, but for this tutorial, we'll be using MindsDB Cloud to connect to our database. Click on Databases in the upper left, then on Add Database in the lower right. In the popup screen, fill in the details for your MySQL database, and specify a name for the integration (here, we chose bodyfat_integration). You can test if the database is connectable by clicking Click Here to Test Connection , and if all is well, click on the Connect button: You should now see your new database integration appear in the MindsDB GUI: At this point, you have successfully connected MindsDB to the database!","title":"Connect MindsDB to the Database"},{"location":"sql/tutorials/bodyfat/#connect-to-the-mindsdb-mysql-api","text":"Now, you'll connect to the MindsDB MySQL API to enable the use of SQL commands to train ML models and make predictions. For this tutorial, we'll be connecting to MindsDB Cloud using a commandline mysql client: mysql -h cloud.mindsdb.com --port 3306 -u username@email.com -p Execute the above command substituting the email address you used to sign up for MindsDB Cloud, followed by entering your password, and you should see output similar to: Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: 5.7.1-MindsDB-1.0 (MindsDB) Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [(none)]> You have now connected to the MindsDB MySQL API successfully!","title":"Connect to the MindsDB MySQL API"},{"location":"sql/tutorials/bodyfat/#using-sql-commands-to-train-ml-models","text":"We will now train a new machine learning model for the dataset we've created. In MindsDB terms it is called a Predictor. We will show how to create it automatically, but there is also a way to fine tune it, if you know what you are doing (check the MindsDB docs ). Go to your mysql-client and run the following command: USE mindsdb ; Now, we have to create a predictor based on the following syntax: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; For our case, we'll enter the following command: CREATE PREDICTOR bodyfat_predictor FROM bodyfat_integration ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; You should see output similar to the following: Query OK, 0 rows affected (3.077 sec) At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . predictors WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: +-------------------+----------+--------------------+---------+-------------------+---------------------+------------------+ | name | status | accuracy | predict | select_data_query | external_datasource | training_options | +-------------------+----------+--------------------+---------+-------------------+---------------------+------------------+ | bodyfat_predictor | complete | 0.9909730079130395 | BodyFat | | | | +-------------------+----------+--------------------+---------+-------------------+---------------------+------------------+ 1 row in set (0.101 sec) As you can see, the predictor training has completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset!","title":"Using SQL Commands to Train ML Models"},{"location":"sql/tutorials/bodyfat/#using-sql-commands-to-make-predictions","text":"Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain AS Info FROM mindsdb . bodyfat_predictor WHERE when_data = '{\"Density\": 1.08, \"Age\": 25, \"Weight\": 170, \"Height\": 70, \"Neck\": 38.1, \"Chest\": 103.5, \"Abdomen\": 85.4, \"Hip\": 102.2, \"Thigh\": 63.0, \"Knee\": 39.4, \"Ankle\": 22.8, \"Biceps\": 33.3, \"Forearm\": 28.7, \"Wrist\": 18.3}' ; This should return output similar to: +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | BodyFat | BodyFat_confidence | Info | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 8.968581383955318 | 0.99 | {\"predicted_value\": 8.968581383955318, \"confidence\": 0.99, \"confidence_lower_bound\": 5.758912817402102, \"confidence_upper_bound\": 12.178249950508533, \"anomaly\": null, \"truth\": null} | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.464 sec) As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like.","title":"Using SQL Commands to Make Predictions"},{"location":"sql/tutorials/bodyfat/#making-batch-predictions-using-the-join-command","text":"The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN command is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | Age | Density | Weight | Height | Neck | Chest | Abdomen | Hip | BodyFat | predicted_BodyFat | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | 23 | 1.0708 | 154.25 | 67.75 | 36.2 | 93.1 | 85.2 | 94.5 | 12.3 | 12.475132275112655 | | 22 | 1.0853 | 173.25 | 72.25 | 38.5 | 93.6 | 83.0 | 98.7 | 6.1 | 6.07133439184195 | | 22 | 1.0414 | 154.0 | 66.25 | 34.0 | 95.8 | 87.9 | 99.2 | 25.3 | 25.156538398443754 | | 26 | 1.0751 | 184.75 | 72.25 | 37.4 | 101.8 | 86.4 | 101.2 | 10.4 | 10.696461885516461 | | 24 | 1.034 | 184.25 | 71.25 | 34.4 | 97.3 | 100.0 | 101.9 | 28.7 | 28.498772660802427 | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ 5 rows in set (1.091 sec) As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Making Batch Predictions using the JOIN Command"},{"location":"sql/tutorials/crop-prediction/","text":"Crop Recomendation Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB and MariaDB . Pre-requisites Before you start make sure you have: Access to MindsDB. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Access to a database. In this example we will use MariaDB. You can install it locally or through Docker . Downloaded the dataset. You can get it from Kaggle . Access to mysql-client. You can probably get it from your system\u2019s package manager. For Debian/Ubuntu check here . Optional: Access to ngrok. You can check the installation details at the ngrok website . Setup the database In this section, you will create a MariaDB database and a table into which you will then load the dataset. First, connect to your MariaDB instance. You can use the CLI based mysql or any manager like DBeaver . If you have MariaDB running locally, you can use the following line to connect. Remember to change the username if you have a different one set up in MariaDB. mysql -u root -p -h 127 .0.0.1 After connecting you can create a database for the project. You can skip this step if you already have a database you want to use. CREATE DATABASE agriculture ; You can check that the database was created with the following query. SHOW DATABASES ; The output will be similar to the one below. +--------------------+ | Database | +--------------------+ | agriculture | | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.01 sec) Next, you need to create a table for the dataset. To do so, first switch to the database you want to use. USE agriculture ; Now you can create the table with the following schema. CREATE TABLE crops ( N INT , P INT , K INT , temperature INT , humidity DECIMAL ( 10 , 2 ), ph DECIMAL ( 10 , 2 ), rainfall DECIMAL ( 10 , 2 ), label VARCHAR ( 50 ) ); You can check if the table was created with the SHOW TABLES; query. You should see a similar output. +-----------------------+ | Tables_in_agriculture | +-----------------------+ | crops | +-----------------------+ 1 row in set (0.00 sec) When the table is created you can load the dataset into it. To load the CSV file into the table use the following query. Remember to change the path to the dataset to match the file location on your system. LOAD DATA INFILE '/Crop_recommendation.csv' INTO TABLE crops FIELDS TERMINATED BY ',' IGNORE 1 LINE ; To verify that the data has been loaded, you can make a simple SELECT query. SELECT * FROM crops LIMIT 5 ; You should see a similar output. +------+------+------+-------------+----------+------+----------+-------+ | N | P | K | temperature | humidity | ph | rainfall | label | +------+------+------+-------------+----------+------+----------+-------+ | 90 | 42 | 43 | 21 | 82.00 | 6.50 | 202.94 | rice | 85 | 58 | 41 | 22 | 80.32 | 7.04 | 226.66 | rice | 60 | 55 | 44 | 23 | 82.32 | 7.84 | 263.96 | rice | 74 | 35 | 40 | 26 | 80.16 | 6.98 | 242.86 | rice | 78 | 42 | 42 | 20 | 81.60 | 7.63 | 262.72 | rice +------+------+------+-------------+----------+------+----------+-------+ 5 rows in set (0.00 sec) You have now finished setting up the MariaDB database! Connect MindsDB to your database In this section, you will connect your database to MindsDB. The recommended way of connecting a database to MindsDB is through its GUI. In the open source version you need to launch MindsDB Studio, but in this tutorial we will use the GUI at MindsDB Cloud. Since our MariaDB instance is local we will use ngrok to make it available to MindsDB Cloud. If you are using a MariaDB instance that already has a public address or you have installed MindsDB locally you can skip this step. First you need to set up an ngrok tunnel with the following command. If you have used a different port for your MariaDB installation, remember to change it here. ngrok tcp 3306 You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://x.tcp.ngrok.io:12345 -> localhost:5432 Now you can copy the forwarded address from the above output. You are interested in the x.tcp.ngrok.io:12345 part. With the address copied, head over to MindsDB GUI. In the main screen, select ADD DATABASE . Then add your integration details. Click Connect , you should now see your MariaDB database connection in the main screen. You are now done with connecting MindsDB to your database! \ud83d\ude80 Create a predictor In this section you will connect to MindsDB with the MySQL API and create a predictor with a single SQL command. Predictor is in fact a complete machine learning model, with datasource columns serving as features, and MindsDB takes care of the rest of ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. First you need to connect to MindsDB through the MySQL API. To do so, use the following command. Remember to change the username for the connection mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p After that switch to the mindsdb database. USE mindsdb ; Use the following query to create a predictor that will predict the label ( crop type ) for the specific field parameters. CREATE PREDICTOR crop_predictor FROM crops_integration ( SELECT * FROM crops ) PREDICT label as crop_type ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (11.66 sec) Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+---------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | external_datasource | training_options | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+---------------------+------------------+ | crop_predictor | complete | 0.9954545454545454 | label | up_to_date | 2.55.2 | | | | | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+---------------------+------------------+ 1 row in set (0.29 sec) You are now done with creating the predictor! \u2728 Make predictions In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE when_data = '{\"N\": 77, \"P\": 52, \"K\": 17, \"temperature\": 24, \"humidity\": 20.74, \"ph\": 5.71, \"rainfall\": 75.82}' \\ G label: maize 1 row in set (0.32 sec) As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label as predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +------+------+------+-------------+----------+------+----------+---------------------+ | N | P | K | temperature | humidity | ph | rainfall | predicted_crop_type | +------+------+------+-------------+----------+------+----------+---------------------+ | 90 | 42 | 43 | 21 | 82.0 | 6.5 | 202.94 | rice | | 85 | 58 | 41 | 22 | 80.32 | 7.04 | 226.66 | rice | | 60 | 55 | 44 | 23 | 82.32 | 7.84 | 263.96 | rice | | 74 | 35 | 40 | 26 | 80.16 | 6.98 | 242.86 | rice | | 78 | 42 | 42 | 20 | 81.6 | 7.63 | 262.72 | rice | +------+------+------+-------------+----------+------+----------+---------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Crop Recomendation"},{"location":"sql/tutorials/crop-prediction/#crop-recomendation","text":"Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB and MariaDB .","title":"Crop Recomendation"},{"location":"sql/tutorials/crop-prediction/#pre-requisites","text":"Before you start make sure you have: Access to MindsDB. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Access to a database. In this example we will use MariaDB. You can install it locally or through Docker . Downloaded the dataset. You can get it from Kaggle . Access to mysql-client. You can probably get it from your system\u2019s package manager. For Debian/Ubuntu check here . Optional: Access to ngrok. You can check the installation details at the ngrok website .","title":"Pre-requisites"},{"location":"sql/tutorials/crop-prediction/#setup-the-database","text":"In this section, you will create a MariaDB database and a table into which you will then load the dataset. First, connect to your MariaDB instance. You can use the CLI based mysql or any manager like DBeaver . If you have MariaDB running locally, you can use the following line to connect. Remember to change the username if you have a different one set up in MariaDB. mysql -u root -p -h 127 .0.0.1 After connecting you can create a database for the project. You can skip this step if you already have a database you want to use. CREATE DATABASE agriculture ; You can check that the database was created with the following query. SHOW DATABASES ; The output will be similar to the one below. +--------------------+ | Database | +--------------------+ | agriculture | | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.01 sec) Next, you need to create a table for the dataset. To do so, first switch to the database you want to use. USE agriculture ; Now you can create the table with the following schema. CREATE TABLE crops ( N INT , P INT , K INT , temperature INT , humidity DECIMAL ( 10 , 2 ), ph DECIMAL ( 10 , 2 ), rainfall DECIMAL ( 10 , 2 ), label VARCHAR ( 50 ) ); You can check if the table was created with the SHOW TABLES; query. You should see a similar output. +-----------------------+ | Tables_in_agriculture | +-----------------------+ | crops | +-----------------------+ 1 row in set (0.00 sec) When the table is created you can load the dataset into it. To load the CSV file into the table use the following query. Remember to change the path to the dataset to match the file location on your system. LOAD DATA INFILE '/Crop_recommendation.csv' INTO TABLE crops FIELDS TERMINATED BY ',' IGNORE 1 LINE ; To verify that the data has been loaded, you can make a simple SELECT query. SELECT * FROM crops LIMIT 5 ; You should see a similar output. +------+------+------+-------------+----------+------+----------+-------+ | N | P | K | temperature | humidity | ph | rainfall | label | +------+------+------+-------------+----------+------+----------+-------+ | 90 | 42 | 43 | 21 | 82.00 | 6.50 | 202.94 | rice | 85 | 58 | 41 | 22 | 80.32 | 7.04 | 226.66 | rice | 60 | 55 | 44 | 23 | 82.32 | 7.84 | 263.96 | rice | 74 | 35 | 40 | 26 | 80.16 | 6.98 | 242.86 | rice | 78 | 42 | 42 | 20 | 81.60 | 7.63 | 262.72 | rice +------+------+------+-------------+----------+------+----------+-------+ 5 rows in set (0.00 sec) You have now finished setting up the MariaDB database!","title":"Setup the database"},{"location":"sql/tutorials/crop-prediction/#connect-mindsdb-to-your-database","text":"In this section, you will connect your database to MindsDB. The recommended way of connecting a database to MindsDB is through its GUI. In the open source version you need to launch MindsDB Studio, but in this tutorial we will use the GUI at MindsDB Cloud. Since our MariaDB instance is local we will use ngrok to make it available to MindsDB Cloud. If you are using a MariaDB instance that already has a public address or you have installed MindsDB locally you can skip this step. First you need to set up an ngrok tunnel with the following command. If you have used a different port for your MariaDB installation, remember to change it here. ngrok tcp 3306 You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://x.tcp.ngrok.io:12345 -> localhost:5432 Now you can copy the forwarded address from the above output. You are interested in the x.tcp.ngrok.io:12345 part. With the address copied, head over to MindsDB GUI. In the main screen, select ADD DATABASE . Then add your integration details. Click Connect , you should now see your MariaDB database connection in the main screen. You are now done with connecting MindsDB to your database! \ud83d\ude80","title":"Connect MindsDB to your database"},{"location":"sql/tutorials/crop-prediction/#create-a-predictor","text":"In this section you will connect to MindsDB with the MySQL API and create a predictor with a single SQL command. Predictor is in fact a complete machine learning model, with datasource columns serving as features, and MindsDB takes care of the rest of ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. First you need to connect to MindsDB through the MySQL API. To do so, use the following command. Remember to change the username for the connection mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p After that switch to the mindsdb database. USE mindsdb ; Use the following query to create a predictor that will predict the label ( crop type ) for the specific field parameters. CREATE PREDICTOR crop_predictor FROM crops_integration ( SELECT * FROM crops ) PREDICT label as crop_type ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (11.66 sec) Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+---------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | external_datasource | training_options | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+---------------------+------------------+ | crop_predictor | complete | 0.9954545454545454 | label | up_to_date | 2.55.2 | | | | | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+---------------------+------------------+ 1 row in set (0.29 sec) You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/crop-prediction/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE when_data = '{\"N\": 77, \"P\": 52, \"K\": 17, \"temperature\": 24, \"humidity\": 20.74, \"ph\": 5.71, \"rainfall\": 75.82}' \\ G label: maize 1 row in set (0.32 sec) As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label as predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +------+------+------+-------------+----------+------+----------+---------------------+ | N | P | K | temperature | humidity | ph | rainfall | predicted_crop_type | +------+------+------+-------------+----------+------+----------+---------------------+ | 90 | 42 | 43 | 21 | 82.0 | 6.5 | 202.94 | rice | | 85 | 58 | 41 | 22 | 80.32 | 7.04 | 226.66 | rice | | 60 | 55 | 44 | 23 | 82.32 | 7.84 | 263.96 | rice | | 74 | 35 | 40 | 26 | 80.16 | 6.98 | 242.86 | rice | | 78 | 42 | 42 | 20 | 81.6 | 7.63 | 262.72 | rice | +------+------+------+-------------+----------+------+----------+---------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/customer-churn/","text":"MindsDB as a Machine Learning framework can help marketing, sales, and customer retention teams determine the best incentive and the right time to make an offer to minimize customer turnover. In this tutorial you will learn how to use SQL queries to train a machine learning model and make predictions in three simple steps: Connect a database with customer's data to MindsDB. Use an CREATE PREDICTOR statement to train the machine learning model automatically. Query predictions with a simple SELECT statement from MindsDB AI Table (this special table returns data from an ML model upon being queried). Using SQL to perform machine learning at the data layer will bring you many benefits like removing unnecessary ETL-ing, seamless integration with your data, and enabling predictive analytics in your BI tool. Let's see how this works with a real world example to predict the probability of churn for a new customer of a telecom company. Note: You can follow up this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Pre-requisites First, make sure you have successfully installed MindsDB. Check out the installation guide for Docker or PyPi install. Second, you will need to have mysql-client or DBeaver, MySQL WOrkbench etc installed locally to connect to MySQL API. Database Connection First, we need to connect MindsDB to the database where the Customer Churn data is stored. In the left navigation click on Database. Next, click on the ADD DATABASE. Here, we need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration Database - the database name Host - database host name Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict customer churn. Connect to MindsDB\u2019s MySQL API I will use a mysql command line client in the next part of the tutorial but you can follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MySQL API: mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p In the above command, we specify the hostname and user name explicitly, as well as a password for connecting. If you got the above screen that means you have successfully connected. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data Overview In this tutorial, we will use the customer churn data-set. Each row represents a customer and we will train a machine learning model to help us predict if the customer is going to stop using the company products. Below is a short description of each feature inside the data. CustomerId - Customer ID Gender - Male or Female customer SeniorCitizen - Whether the customer is a senior citizen or not (1, 0) Partner - Whether the customer has a partner or not (Yes, No) Dependents - Whether the customer has dependents or not (Yes, No) Tenure - Number of months the customer has stayed with the company PhoneService - Whether the customer has a phone service or not (Yes, No) MultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService - Customer\u2019s internet service provider (DSL, Fiber optic, No) OnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service) TechSupport - Whether the customer has tech support or not (Yes, No, No internet service) StreamingTv - Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service) Contract - The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling - Whether the customer has paperless billing or not (Yes, No) PaymentMethod - The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges - The monthly charge amount TotalCharges - The total amount charged to the customer Churn - Whether the customer churned or not (Yes or No). This is what we want to predict. Using SQL Statements to train/query models Now, we will train a new machine learning model from the datasource we have created using MindsDB Studio. Switch back to mysql-client and run: use mindsdb; show tables; You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; The required values that we need to provide are: predictor_name (string) - The name of the model integration_name (string) - The name of connection to your database. column_name (string) - The feature you want to predict. To train the model that will predict customer churn run: CREATE PREDICTOR churn_model FROM demo ( SELECT * FROM CustomerChurnData ) PREDICT Churn as customer_churn USING { \"ignore_columns\" : [ \"gender\" ] } ; What we did here was to create a predictor called customer_churn to predict the Churn and also ignore the gender column as an irrelevant column for the model. Also note that the ID columns in this case customerId will be automatically detected by MindsDB and ignored. The model training has started. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The complete status means that the model training has successfully finished. The next steps would be to query the model and predict the customer churn. Let\u2019s be creative and imagine a customer. Customer will use only DSL service, no phone service and multiple lines, she was with the company for 1 month and has a partner. Add all of this information to the WHERE clause. SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\"}' ; With the confidence of around 82% MindsDB predicted that this customer will churn. One important thing to check here is the important_missing_information value, where MindsDB is pointing to the important missing information for giving a more accurate prediction, in this case, Contract, MonthlyCharges, TotalCharges and OnlineBackup. Let\u2019s include those values in the WHERE clause, and run a new query: SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\", \"OnlineSecurity\": \"No\", \"OnlineBackup\": \"Yes\", \"DeviceProtection\": \"No\", \"TechSupport\": \"No\", \"StreamingTV\": \"No\", \"StreamingMovies\": \"No\", \"Contract\": \"Month-to-month\", \"PaperlessBilling\": \"Yes\", \"PaymentMethod\": \"Electronic check\", \"MonthlyCharges\": 29.85, \"TotalCharges\": 29.85}' ;","title":"Customer Churn"},{"location":"sql/tutorials/customer-churn/#pre-requisites","text":"First, make sure you have successfully installed MindsDB. Check out the installation guide for Docker or PyPi install. Second, you will need to have mysql-client or DBeaver, MySQL WOrkbench etc installed locally to connect to MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/customer-churn/#database-connection","text":"First, we need to connect MindsDB to the database where the Customer Churn data is stored. In the left navigation click on Database. Next, click on the ADD DATABASE. Here, we need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration Database - the database name Host - database host name Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict customer churn.","title":"Database Connection"},{"location":"sql/tutorials/customer-churn/#connect-to-mindsdbs-mysql-api","text":"I will use a mysql command line client in the next part of the tutorial but you can follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MySQL API: mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p In the above command, we specify the hostname and user name explicitly, as well as a password for connecting. If you got the above screen that means you have successfully connected. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/customer-churn/#data-overview","text":"In this tutorial, we will use the customer churn data-set. Each row represents a customer and we will train a machine learning model to help us predict if the customer is going to stop using the company products. Below is a short description of each feature inside the data. CustomerId - Customer ID Gender - Male or Female customer SeniorCitizen - Whether the customer is a senior citizen or not (1, 0) Partner - Whether the customer has a partner or not (Yes, No) Dependents - Whether the customer has dependents or not (Yes, No) Tenure - Number of months the customer has stayed with the company PhoneService - Whether the customer has a phone service or not (Yes, No) MultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService - Customer\u2019s internet service provider (DSL, Fiber optic, No) OnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service) TechSupport - Whether the customer has tech support or not (Yes, No, No internet service) StreamingTv - Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service) Contract - The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling - Whether the customer has paperless billing or not (Yes, No) PaymentMethod - The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges - The monthly charge amount TotalCharges - The total amount charged to the customer Churn - Whether the customer churned or not (Yes or No). This is what we want to predict.","title":"Data Overview"},{"location":"sql/tutorials/customer-churn/#using-sql-statements-to-trainquery-models","text":"Now, we will train a new machine learning model from the datasource we have created using MindsDB Studio. Switch back to mysql-client and run: use mindsdb; show tables; You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; The required values that we need to provide are: predictor_name (string) - The name of the model integration_name (string) - The name of connection to your database. column_name (string) - The feature you want to predict. To train the model that will predict customer churn run: CREATE PREDICTOR churn_model FROM demo ( SELECT * FROM CustomerChurnData ) PREDICT Churn as customer_churn USING { \"ignore_columns\" : [ \"gender\" ] } ; What we did here was to create a predictor called customer_churn to predict the Churn and also ignore the gender column as an irrelevant column for the model. Also note that the ID columns in this case customerId will be automatically detected by MindsDB and ignored. The model training has started. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The complete status means that the model training has successfully finished. The next steps would be to query the model and predict the customer churn. Let\u2019s be creative and imagine a customer. Customer will use only DSL service, no phone service and multiple lines, she was with the company for 1 month and has a partner. Add all of this information to the WHERE clause. SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\"}' ; With the confidence of around 82% MindsDB predicted that this customer will churn. One important thing to check here is the important_missing_information value, where MindsDB is pointing to the important missing information for giving a more accurate prediction, in this case, Contract, MonthlyCharges, TotalCharges and OnlineBackup. Let\u2019s include those values in the WHERE clause, and run a new query: SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\", \"OnlineSecurity\": \"No\", \"OnlineBackup\": \"Yes\", \"DeviceProtection\": \"No\", \"TechSupport\": \"No\", \"StreamingTV\": \"No\", \"StreamingMovies\": \"No\", \"Contract\": \"Month-to-month\", \"PaperlessBilling\": \"Yes\", \"PaymentMethod\": \"Electronic check\", \"MonthlyCharges\": 29.85, \"TotalCharges\": 29.85}' ;","title":"Using SQL Statements to train/query models"},{"location":"sql/tutorials/heart-disease/","text":"Cardiovascular disease remains the leading cause of morbidity and mortality according to the National Center for Health Statistics in the United States, and consequently, early diagnosis is of paramount importance. Machine learning technology, a subfield of artificial intelligence, is enabling scientists, clinicians and patients to detect it in the earlier stages and therefore save lives. Until now, building, deploying and maintaining applied machine learning solutions was a complicated and expensive task, because it required skilled personnel and expensive tools. But not only that. A traditional machine learning project requires building integrations with data and applications, that is not only a technical but also an organizational challenge. So what if machine learning can become a part of the standard tools that are already in use? This is exactly the problem that MindsDB is solving. It makes machine learning easy to use by automating and integrating it into the mainstream instruments for data and analytics, namely databases and business intelligence software. It adds an AI \u201cbrain\u201d to databases so that they can learn automatically from existing data, allowing you to generate and visualize predictions using standard data query language like SQL. Lastly, MindsDB is open-source, and anyone can use it for free. In this article, we will show step by step how to use MindsDB inside databases to predict the risk of heart disease for patients. You can follow this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Let\u2019s get started! \u200b Pre-requisites \u200b If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. \u200b Connect to your data \u200b First, we need to connect MindsDB to the database where the Heart Disease data is stored. Open MindsDB GUI and in the left navigation bar click on Database. Next, click on the ADD DATABASE button. Here, we need to provide all of the required parameters for connecting to the database. \u200b * Supported Database - select the database that you want to connect to * Integrations Name - add a name to the integration * Database - the database name * Host - database host name * Port - database port * Username - database user * Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict the risk of heart disease for a certain patient. \u200b How to use MindsDB \u200b MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works. The first step is to connect to MindsDB\u2019s MySQL API. Go to your MySQL client and execute: \u200b mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p \u200b In the above command, we specify the hostname and user name, as well as a password for connecting. If you use a local instance of MindsDB you have to specify its parameters. Please refer to the documentation . If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. \u200b Data Overview \u200b For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. \u200b * age - In Years * sex - 1 = Male; 0 = Female * cp - chest pain type (4 values) * trestbps - Resting blood pressure (in mm Hg on admission to the hospital) * chol - Serum cholesterol in mg/dl * fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) * restecg - Resting electrocardiographic results * thalach - Maximum heart rate achieved * exang - Exercise induced angina (1 = yes; 0 = no) * oldpeak - ST depression induced by exercise relative to rest * slope - the slope of the peak exercise ST segment * ca - Number of major vessels (0-3) colored by fluoroscopy * thal - 1 = normal; 2 = fixed defect; 3 = reversible defect * target - 1 or 0 (This is what we will predict) \u200b Using SQL Statements to automatically train ML models \u200b Now, we will train a new machine learning model from the datasource we have created. Go to your mysql-client and run: \u200b use mindsdb; show tables; \u200b You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: \u200b CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; \u200b The required values that we need to provide are: \u200b * predictor_name (string) - The name of the model * integration_name (string) - The name of the connection to your database. * column_name (string) - The feature you want to predict. \u200b To train the model that will predict the risk of heart disease as target run: \u200b CREATE PREDICTOR patients_target FROM db_integration ( SELECT * FROM HeartDiseaseData ) PREDICT target USING { \"ignore_columns\" : [ \"sex\" ] } ; \u200b \u200b What we did here was to create a predictor called patients_target to predict the presence of heart disease as target and also ignore the sex column as an irrelevant column for the model. The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: \u200b SELECT * FROM mindsdb . predictors WHERE name = 'patients_target' ; \u200b The complete status means that the model training has successfully finished. Using SQL Statements to make predictions \u200b \u200bThe next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. \u200b SELECT target as prediction , target_confidence as confidence , target_explain as info FROM mindsdb . patients_target WHERE when_data = '{\"age\": 30, \"chol\": 177, \"slope\": 2, \"thal\": 2}' ; \u200b With a confidence of around 99%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM db_integration . HeartDiseaseData AS t JOIN mindsdb . patients_target AS tb WHERE t . thal in ( '2' ); \u200b Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots. Conclusion In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Heart Disease"},{"location":"sql/tutorials/heart-disease/#pre-requisites","text":"\u200b If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. \u200b","title":"Pre-requisites"},{"location":"sql/tutorials/heart-disease/#connect-to-your-data","text":"\u200b First, we need to connect MindsDB to the database where the Heart Disease data is stored. Open MindsDB GUI and in the left navigation bar click on Database. Next, click on the ADD DATABASE button. Here, we need to provide all of the required parameters for connecting to the database. \u200b * Supported Database - select the database that you want to connect to * Integrations Name - add a name to the integration * Database - the database name * Host - database host name * Port - database port * Username - database user * Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict the risk of heart disease for a certain patient. \u200b","title":"Connect to your data"},{"location":"sql/tutorials/heart-disease/#how-to-use-mindsdb","text":"\u200b MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works. The first step is to connect to MindsDB\u2019s MySQL API. Go to your MySQL client and execute: \u200b mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p \u200b In the above command, we specify the hostname and user name, as well as a password for connecting. If you use a local instance of MindsDB you have to specify its parameters. Please refer to the documentation . If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. \u200b","title":"How to use MindsDB"},{"location":"sql/tutorials/heart-disease/#data-overview","text":"\u200b For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. \u200b * age - In Years * sex - 1 = Male; 0 = Female * cp - chest pain type (4 values) * trestbps - Resting blood pressure (in mm Hg on admission to the hospital) * chol - Serum cholesterol in mg/dl * fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) * restecg - Resting electrocardiographic results * thalach - Maximum heart rate achieved * exang - Exercise induced angina (1 = yes; 0 = no) * oldpeak - ST depression induced by exercise relative to rest * slope - the slope of the peak exercise ST segment * ca - Number of major vessels (0-3) colored by fluoroscopy * thal - 1 = normal; 2 = fixed defect; 3 = reversible defect * target - 1 or 0 (This is what we will predict) \u200b","title":"Data Overview"},{"location":"sql/tutorials/heart-disease/#using-sql-statements-to-automatically-train-ml-models","text":"\u200b Now, we will train a new machine learning model from the datasource we have created. Go to your mysql-client and run: \u200b use mindsdb; show tables; \u200b You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: \u200b CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; \u200b The required values that we need to provide are: \u200b * predictor_name (string) - The name of the model * integration_name (string) - The name of the connection to your database. * column_name (string) - The feature you want to predict. \u200b To train the model that will predict the risk of heart disease as target run: \u200b CREATE PREDICTOR patients_target FROM db_integration ( SELECT * FROM HeartDiseaseData ) PREDICT target USING { \"ignore_columns\" : [ \"sex\" ] } ; \u200b \u200b What we did here was to create a predictor called patients_target to predict the presence of heart disease as target and also ignore the sex column as an irrelevant column for the model. The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: \u200b SELECT * FROM mindsdb . predictors WHERE name = 'patients_target' ; \u200b The complete status means that the model training has successfully finished.","title":"Using SQL Statements to automatically train ML models"},{"location":"sql/tutorials/heart-disease/#using-sql-statements-to-make-predictions","text":"\u200b \u200bThe next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. \u200b SELECT target as prediction , target_confidence as confidence , target_explain as info FROM mindsdb . patients_target WHERE when_data = '{\"age\": 30, \"chol\": 177, \"slope\": 2, \"thal\": 2}' ; \u200b With a confidence of around 99%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM db_integration . HeartDiseaseData AS t JOIN mindsdb . patients_target AS tb WHERE t . thal in ( '2' ); \u200b Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots.","title":"Using SQL Statements to make predictions"},{"location":"sql/tutorials/heart-disease/#conclusion","text":"In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Conclusion"},{"location":"sql/tutorials/insurance-cost-prediction/","text":"Predict Insurance Cost using MindsDB Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma Can you accurately predict insurance costs? In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data. Pre-requisites First, you need MindsDB installed. If you want to use it locally, you can use Docker or PIP . Alternatively, to use MindsDB without installing it locally, you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API. Connect your database First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float) Create the model Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized. Make prediction Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). Finally, we have trained an insurance model using SQL and MindsDB. Conclusions As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Insurance Cost"},{"location":"sql/tutorials/insurance-cost-prediction/#predict-insurance-cost-using-mindsdb","text":"Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma","title":"Predict Insurance Cost using MindsDB"},{"location":"sql/tutorials/insurance-cost-prediction/#can-you-accurately-predict-insurance-costs","text":"In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data.","title":"Can you accurately predict insurance costs?"},{"location":"sql/tutorials/insurance-cost-prediction/#pre-requisites","text":"First, you need MindsDB installed. If you want to use it locally, you can use Docker or PIP . Alternatively, to use MindsDB without installing it locally, you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-your-database","text":"First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-to-mindsdbs-mysql-api","text":"Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/insurance-cost-prediction/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float)","title":"Data"},{"location":"sql/tutorials/insurance-cost-prediction/#create-the-model","text":"Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized.","title":"Create the model"},{"location":"sql/tutorials/insurance-cost-prediction/#make-prediction","text":"Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). Finally, we have trained an insurance model using SQL and MindsDB.","title":"Make prediction"},{"location":"sql/tutorials/insurance-cost-prediction/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Conclusions"},{"location":"sql/tutorials/mindsdb-superset-snowflake/","text":"Using MindsDB Machine Learning to Solve a Real-World time series Problem Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results. Set Up MindsDB First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc. Connect MindsDB to the Data for model training MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to. Step 1: Getting the Training Data We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3. Step 2: Training the Predictive Model Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM MINDSDB . PREDICTORS ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE PREDICTOR rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE PREDICTOR statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete. Step 3: Getting the Forecasts We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions. Visualizing the Results using Apache Superset Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB. Visualizing Data The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55. Conclusions: Powerful forecasting with MindsDB, your database, and Superset The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"AI-powered forecasts in Apache Superset and Snowflake"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#using-mindsdb-machine-learning-to-solve-a-real-world-time-series-problem","text":"Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results.","title":"Using MindsDB Machine Learning to Solve a Real-World time series Problem"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#set-up-mindsdb","text":"First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc.","title":"Set Up MindsDB"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#connect-mindsdb-to-the-data-for-model-training","text":"MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to.","title":"Connect MindsDB to the Data for model training"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-1-getting-the-training-data","text":"We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3.","title":"Step 1: Getting the Training Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-2-training-the-predictive-model","text":"Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM MINDSDB . PREDICTORS ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE PREDICTOR rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE PREDICTOR statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete.","title":"Step 2: Training the Predictive Model"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-3-getting-the-forecasts","text":"We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions.","title":"Step 3: Getting the Forecasts"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-the-results-using-apache-superset","text":"Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB.","title":"Visualizing the Results using Apache Superset"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-data","text":"The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55.","title":"Visualizing Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#conclusions-powerful-forecasting-with-mindsdb-your-database-and-superset","text":"The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"Conclusions: Powerful forecasting with MindsDB, your database, and Superset"},{"location":"sql/tutorials/process-quality/","text":"Manufacturing process quality Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB and PostgreSQL . Pre-requisites Before you start make sure you have: Access to MindsDB. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Access to a PostgreSQL database. You can install locally or use a managed service like Heroku Postgres . Downloaded the dataset. You can get it from Kaggle . Access to mysql-client. You can probably get it from your systems package manager. For Debian/Ubuntu check here . Optional: Access to ngrok. You can check the installation details at the ngrok website . Setup the database In this section you will create a PostgreSQL database and a table into which you will then load the dataset. First connect to your PostgreSQL instance. You can use the CLI based psql or any manager like PgAdmin . If you have PostgreSQL running locally, you can use the following line to connect. Remember to change the username if you have a different one set up in PostgreSQL. psql -h localhost -U postgres After connecting you can create a database for the project. You can skip this step if you already have the database you want to use. CREATE DATABASE manufacturing ; Next, you need to create a table for the dataset. To do so, first switch to the database you want to use. \\ c manufacturing Now you can create the table with the following schema. CREATE TABLE process_quality ( \"date\" date , \"% Iron Feed\" decimal , \"% Silica Feed\" decimal , \"Starch Flow\" decimal , \"Amina Flow\" decimal , \"Ore Pulp Flow\" decimal , \"Ore Pulp pH\" decimal , \"Ore Pulp Density\" decimal , \"Flotation Column 01 Air Flow\" decimal , \"Flotation Column 02 Air Flow\" decimal , \"Flotation Column 03 Air Flow\" decimal , \"Flotation Column 04 Air Flow\" decimal , \"Flotation Column 05 Air Flow\" decimal , \"Flotation Column 06 Air Flow\" decimal , \"Flotation Column 07 Air Flow\" decimal , \"Flotation Column 01 Level\" decimal , \"Flotation Column 02 Level\" decimal , \"Flotation Column 03 Level\" decimal , \"Flotation Column 04 Level\" decimal , \"Flotation Column 05 Level\" decimal , \"Flotation Column 06 Level\" decimal , \"Flotation Column 07 Level\" decimal , \"% Iron Concentrate\" decimal , \"% Silica Concentrate\" decimal ); You can check if the table was created with the \\dt command. You should see a similar output. List of relations Schema | Name | Type | Owner --------+-----------------+-------+---------- public | process_quality | table | postgres (1 row) When the table is created you can load the dataset into it. Unfortunately the data comes with commas as a decimal separator. This is not supported with PostgreSQL. A simple way of handling that is to convert all commas in the CSV file to dots and then changing the delimiter to a dot when loading the dataset. You can do this with sed . sed -i 's/,/./g' MiningProcess_Flotation_Plant_Database.csv To load the CSV file into the table use the following query. Remember to change the path to the dataset to match the file location on your system. COPY process_quality FROM '/path/to/dataset/MiningProcess_Flotation_Plant_Database.csv' DELIMITER '.' CSV HEADER ; To verify that the data has been loaded, you can make a simple SELECT query. SELECT \"% Iron Feed\" , \"% Silica Feed\" , \"Starch Flow\" FROM process_quality LIMIT 10 ; You should see a similar output. % Iron Feed | % Silica Feed | Starch Flow -------------+---------------+------------- 55.2 | 16.98 | 3019.53 55.2 | 16.98 | 3024.41 55.2 | 16.98 | 3043.46 55.2 | 16.98 | 3047.36 55.2 | 16.98 | 3033.69 55.2 | 16.98 | 3079.1 55.2 | 16.98 | 3127.79 55.2 | 16.98 | 3152.93 55.2 | 16.98 | 3147.27 55.2 | 16.98 | 3142.58 (10 rows) The last step is to change the table column names so that they don't contain spaces and special characters. This will simplify things later. Use the following queries to change the column names. ALTER TABLE process_quality RENAME COLUMN \"% Iron Feed\" TO iron_feed ; ALTER TABLE process_quality RENAME COLUMN \"% Silica Feed\" TO silica_feed ; ALTER TABLE process_quality RENAME COLUMN \"Starch Flow\" TO starch_flow ; ALTER TABLE process_quality RENAME COLUMN \"Amina Flow\" TO amina_flow ; ALTER TABLE process_quality RENAME COLUMN \"Ore Pulp Flow\" TO ore_pulp_flow ; ALTER TABLE process_quality RENAME COLUMN \"Ore Pulp pH\" TO ore_pulp_ph ; ALTER TABLE process_quality RENAME COLUMN \"Ore Pulp Density\" TO ore_pulp_density ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 01 Air Flow\" TO floatation_column_01_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 02 Air Flow\" TO floatation_column_02_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 03 Air Flow\" TO floatation_column_03_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 04 Air Flow\" TO floatation_column_04_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 05 Air Flow\" TO floatation_column_05_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 06 Air Flow\" TO floatation_column_06_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 07 Air Flow\" TO floatation_column_07_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 01 Level\" TO floatation_column_01_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 02 Level\" TO floatation_column_02_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 03 Level\" TO floatation_column_03_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 04 Level\" TO floatation_column_04_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 05 Level\" TO floatation_column_05_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 06 Level\" TO floatation_column_06_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 07 Level\" TO floatation_column_07_level ; ALTER TABLE process_quality RENAME COLUMN \"% Iron Concentrate\" TO iron_concentrate ; ALTER TABLE process_quality RENAME COLUMN \"% Silica Concentrate\" TO silica_concentrate ; You have now finished setting up the PostgreSQL database! \ud83d\udc18 Connect MindsDB to your database In this section, you will connect your database to MindsDB. The recommended way of connecting a database to MindsDB is through MindsDB GUI. If you are using an open-source version, launch MindsDB Studio. In this tutorial we will use MindsDB Cloud that has GUI already. Since our PostgreSQL instance is local we will use ngrok to make it available to MindsDB Cloud. If you are using a PostgreSQL instance that already has a public address or you have installed MindsDB locally you can skip this step. First you need to set up an ngrok tunnel with the following command. If you have used a different port for your PostgreSQL installation, remember to change it here. ngrok tcp 5423 You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://x.tcp.ngrok.io:12345 -> localhost:5432 Now you can copy the forwarded address from the above output. You are interested in the x.tcp.ngrok.io:12345 part. With the address copied, head over to MindsDB GUI. In the main screen, select ADD DATABASE . Then add your integration details. Click Connect , you should now see your PostgreSQL database connection in the main screen. You are now done with connecting MindsDB to your database! \ud83d\ude80 Create a predictor In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE PREDICTOR process_quality_predictor FROM process_quality_integration ( SELECT * FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate as quality USING { \"ignore_columns\" : [ \"date\" ] } ; After creating the Predictor you should see a similar output: Query OK, 0 rows affected (2 min 27.52 sec) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'process_quality_predictor' ; After the Predictor has finished training, you will see a similar output. +-----------------------------+----------+----------+--------------------+-------------------+---------------------+------------------+ | name | status | accuracy | predict | select_data_query | external_datasource | training_options | +-----------------------------+----------+----------+--------------------+-------------------+---------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | | +-----------------------------+----------+----------+--------------------+-------------------+---------------------+------------------+ 1 row in set (0.28 sec) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728 Make predictions In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain as Info FROM mindsdb . process_quality_predictor WHERE when_data = '{\"iron_feed\": 48.81, \"silica_feed\": 25.31, \"starch_flow\": 2504.94, \"amina_flow\": 309.448, \"ore_pulp_flow\": 377.6511682692, \"ore_pulp_ph\": 10.0607, \"ore_pulp_density\": 1.68676}' ; The output should look similar to this. +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1.68 | 0.99 | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.81 sec) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence as confidence , predictions . silica_concentrate as predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58.84 | 11.46 | 3277.34 | 564.209 | 403.242 | 9.88472 | 1.76297 | 0.99 | 2.129567174379606 | | 58.84 | 11.46 | 3333.59 | 565.308 | 401.016 | 9.88543 | 1.76331 | 0.99 | 2.129548423407259 | | 58.84 | 11.46 | 3400.39 | 565.674 | 399.551 | 9.88613 | 1.76366 | 0.99 | 2.130100408285386 | | 58.84 | 11.46 | 3410.55 | 563.843 | 397.559 | 9.88684 | 1.764 | 0.99 | 2.1298757513510136 | | 58.84 | 11.46 | 3408.98 | 559.57 | 401.719 | 9.88755 | 1.76434 | 0.99 | 2.130438907683961 | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this, check out MindsDB documentation .","title":"Process Quality"},{"location":"sql/tutorials/process-quality/#manufacturing-process-quality","text":"Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB and PostgreSQL .","title":"Manufacturing process quality"},{"location":"sql/tutorials/process-quality/#pre-requisites","text":"Before you start make sure you have: Access to MindsDB. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Access to a PostgreSQL database. You can install locally or use a managed service like Heroku Postgres . Downloaded the dataset. You can get it from Kaggle . Access to mysql-client. You can probably get it from your systems package manager. For Debian/Ubuntu check here . Optional: Access to ngrok. You can check the installation details at the ngrok website .","title":"Pre-requisites"},{"location":"sql/tutorials/process-quality/#setup-the-database","text":"In this section you will create a PostgreSQL database and a table into which you will then load the dataset. First connect to your PostgreSQL instance. You can use the CLI based psql or any manager like PgAdmin . If you have PostgreSQL running locally, you can use the following line to connect. Remember to change the username if you have a different one set up in PostgreSQL. psql -h localhost -U postgres After connecting you can create a database for the project. You can skip this step if you already have the database you want to use. CREATE DATABASE manufacturing ; Next, you need to create a table for the dataset. To do so, first switch to the database you want to use. \\ c manufacturing Now you can create the table with the following schema. CREATE TABLE process_quality ( \"date\" date , \"% Iron Feed\" decimal , \"% Silica Feed\" decimal , \"Starch Flow\" decimal , \"Amina Flow\" decimal , \"Ore Pulp Flow\" decimal , \"Ore Pulp pH\" decimal , \"Ore Pulp Density\" decimal , \"Flotation Column 01 Air Flow\" decimal , \"Flotation Column 02 Air Flow\" decimal , \"Flotation Column 03 Air Flow\" decimal , \"Flotation Column 04 Air Flow\" decimal , \"Flotation Column 05 Air Flow\" decimal , \"Flotation Column 06 Air Flow\" decimal , \"Flotation Column 07 Air Flow\" decimal , \"Flotation Column 01 Level\" decimal , \"Flotation Column 02 Level\" decimal , \"Flotation Column 03 Level\" decimal , \"Flotation Column 04 Level\" decimal , \"Flotation Column 05 Level\" decimal , \"Flotation Column 06 Level\" decimal , \"Flotation Column 07 Level\" decimal , \"% Iron Concentrate\" decimal , \"% Silica Concentrate\" decimal ); You can check if the table was created with the \\dt command. You should see a similar output. List of relations Schema | Name | Type | Owner --------+-----------------+-------+---------- public | process_quality | table | postgres (1 row) When the table is created you can load the dataset into it. Unfortunately the data comes with commas as a decimal separator. This is not supported with PostgreSQL. A simple way of handling that is to convert all commas in the CSV file to dots and then changing the delimiter to a dot when loading the dataset. You can do this with sed . sed -i 's/,/./g' MiningProcess_Flotation_Plant_Database.csv To load the CSV file into the table use the following query. Remember to change the path to the dataset to match the file location on your system. COPY process_quality FROM '/path/to/dataset/MiningProcess_Flotation_Plant_Database.csv' DELIMITER '.' CSV HEADER ; To verify that the data has been loaded, you can make a simple SELECT query. SELECT \"% Iron Feed\" , \"% Silica Feed\" , \"Starch Flow\" FROM process_quality LIMIT 10 ; You should see a similar output. % Iron Feed | % Silica Feed | Starch Flow -------------+---------------+------------- 55.2 | 16.98 | 3019.53 55.2 | 16.98 | 3024.41 55.2 | 16.98 | 3043.46 55.2 | 16.98 | 3047.36 55.2 | 16.98 | 3033.69 55.2 | 16.98 | 3079.1 55.2 | 16.98 | 3127.79 55.2 | 16.98 | 3152.93 55.2 | 16.98 | 3147.27 55.2 | 16.98 | 3142.58 (10 rows) The last step is to change the table column names so that they don't contain spaces and special characters. This will simplify things later. Use the following queries to change the column names. ALTER TABLE process_quality RENAME COLUMN \"% Iron Feed\" TO iron_feed ; ALTER TABLE process_quality RENAME COLUMN \"% Silica Feed\" TO silica_feed ; ALTER TABLE process_quality RENAME COLUMN \"Starch Flow\" TO starch_flow ; ALTER TABLE process_quality RENAME COLUMN \"Amina Flow\" TO amina_flow ; ALTER TABLE process_quality RENAME COLUMN \"Ore Pulp Flow\" TO ore_pulp_flow ; ALTER TABLE process_quality RENAME COLUMN \"Ore Pulp pH\" TO ore_pulp_ph ; ALTER TABLE process_quality RENAME COLUMN \"Ore Pulp Density\" TO ore_pulp_density ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 01 Air Flow\" TO floatation_column_01_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 02 Air Flow\" TO floatation_column_02_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 03 Air Flow\" TO floatation_column_03_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 04 Air Flow\" TO floatation_column_04_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 05 Air Flow\" TO floatation_column_05_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 06 Air Flow\" TO floatation_column_06_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 07 Air Flow\" TO floatation_column_07_airflow ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 01 Level\" TO floatation_column_01_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 02 Level\" TO floatation_column_02_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 03 Level\" TO floatation_column_03_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 04 Level\" TO floatation_column_04_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 05 Level\" TO floatation_column_05_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 06 Level\" TO floatation_column_06_level ; ALTER TABLE process_quality RENAME COLUMN \"Flotation Column 07 Level\" TO floatation_column_07_level ; ALTER TABLE process_quality RENAME COLUMN \"% Iron Concentrate\" TO iron_concentrate ; ALTER TABLE process_quality RENAME COLUMN \"% Silica Concentrate\" TO silica_concentrate ; You have now finished setting up the PostgreSQL database! \ud83d\udc18","title":"Setup the database"},{"location":"sql/tutorials/process-quality/#connect-mindsdb-to-your-database","text":"In this section, you will connect your database to MindsDB. The recommended way of connecting a database to MindsDB is through MindsDB GUI. If you are using an open-source version, launch MindsDB Studio. In this tutorial we will use MindsDB Cloud that has GUI already. Since our PostgreSQL instance is local we will use ngrok to make it available to MindsDB Cloud. If you are using a PostgreSQL instance that already has a public address or you have installed MindsDB locally you can skip this step. First you need to set up an ngrok tunnel with the following command. If you have used a different port for your PostgreSQL installation, remember to change it here. ngrok tcp 5423 You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://x.tcp.ngrok.io:12345 -> localhost:5432 Now you can copy the forwarded address from the above output. You are interested in the x.tcp.ngrok.io:12345 part. With the address copied, head over to MindsDB GUI. In the main screen, select ADD DATABASE . Then add your integration details. Click Connect , you should now see your PostgreSQL database connection in the main screen. You are now done with connecting MindsDB to your database! \ud83d\ude80","title":"Connect MindsDB to your database"},{"location":"sql/tutorials/process-quality/#create-a-predictor","text":"In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE PREDICTOR process_quality_predictor FROM process_quality_integration ( SELECT * FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate as quality USING { \"ignore_columns\" : [ \"date\" ] } ; After creating the Predictor you should see a similar output: Query OK, 0 rows affected (2 min 27.52 sec) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'process_quality_predictor' ; After the Predictor has finished training, you will see a similar output. +-----------------------------+----------+----------+--------------------+-------------------+---------------------+------------------+ | name | status | accuracy | predict | select_data_query | external_datasource | training_options | +-----------------------------+----------+----------+--------------------+-------------------+---------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | | +-----------------------------+----------+----------+--------------------+-------------------+---------------------+------------------+ 1 row in set (0.28 sec) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/process-quality/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain as Info FROM mindsdb . process_quality_predictor WHERE when_data = '{\"iron_feed\": 48.81, \"silica_feed\": 25.31, \"starch_flow\": 2504.94, \"amina_flow\": 309.448, \"ore_pulp_flow\": 377.6511682692, \"ore_pulp_ph\": 10.0607, \"ore_pulp_density\": 1.68676}' ; The output should look similar to this. +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1.68 | 0.99 | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.81 sec) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence as confidence , predictions . silica_concentrate as predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58.84 | 11.46 | 3277.34 | 564.209 | 403.242 | 9.88472 | 1.76297 | 0.99 | 2.129567174379606 | | 58.84 | 11.46 | 3333.59 | 565.308 | 401.016 | 9.88543 | 1.76331 | 0.99 | 2.129548423407259 | | 58.84 | 11.46 | 3400.39 | 565.674 | 399.551 | 9.88613 | 1.76366 | 0.99 | 2.130100408285386 | | 58.84 | 11.46 | 3410.55 | 563.843 | 397.559 | 9.88684 | 1.764 | 0.99 | 2.1298757513510136 | | 58.84 | 11.46 | 3408.98 | 559.57 | 401.719 | 9.88755 | 1.76434 | 0.99 | 2.130438907683961 | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this, check out MindsDB documentation .","title":"Make predictions"},{"location":"tutorials/AdvancedExamples/","text":"Multiple column predictions What is a multiple column prediction ? In some cases, you might want to predict more than one column of your data. In order for mindsdb to predict multiple columns, you simply need to change the to_predict argument from a string (denoting the name of the column) to an array containing the names of the columns you want to predict. In the following example, we've altered the real estate model to predict the location and neighborhood both, instead of the rental_price . Code example import mindsdb_native mdb = mindsdb_native . Predictor ( name = 'multilabel_real_estate_model' ) mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , to_predict = [ 'location' , 'neighborhood' ] # Array with the names of the columns we want to predict ) Multimedia inputs (images, audio and video) Currently, we only support images as inputs into models. We are working on support audio, you can check this issue to track the progress. Video input support is not yet planned. For any sort of media files, simply provide the full path to the file in the column. For example: [ { 'img' : '/mnt/data/img_1.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_2.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_3.jpg' 'is_bicycle' : False } ] Please provide the full path to a file on your local machine, not a url or the binary data from an image loaded up in a dataframe. Currently, the timeline on supporting multimedia output is still undecided, if you need that feature or want to implement, feel free to contact us. That being said, image outputs might actually work, we just haven't tested anything yet. Unbalanced dataset Given a dataset that is \"imbalanced\", the model being trained might not give the results you expect. For example, let's say you have the following dataset: data . csv x , is_power_of_9 1 , False 2 , False 3 , False 4 , False 5 , False 6 , False 7 , False 8 , False 9 , True 10 , False 11 , False . . . 81 , True 82 , False 83 , False 84 , False . . . 100 , False On which we want to predicted the aptly-named column is_power_of_9 . We have 2 occurrences of the output True and 98 occurrences of the False output. So a model could always predict False and it would have an accuracy of 98%, which is pretty decent, so unless there's a way for the model to learn those 2 instance of True without losing on accuracy, it might just decided 98% is the best possible model. However, let's say we really care about those True predictions being correct, or at least we want the model to consider them equally important, in that case we would call the learn function using the equal_accuracy_for_all_output_categories argument set to true. This essentially means that a model with 50% accuracy, with 2 correct predictions for True (100%) and 48 for False (49%) is considered better than a model with 98% accuracy that only predictions False when mindsdb trains the model. We could call this as: predictior.learn(from_data='data.csv', equal_accuracy_for_all_output_categories=True)","title":"Examples of Advanced Usecases"},{"location":"tutorials/AdvancedExamples/#multiple-column-predictions","text":"","title":"Multiple column predictions"},{"location":"tutorials/AdvancedExamples/#what-is-a-multiple-column-prediction","text":"In some cases, you might want to predict more than one column of your data. In order for mindsdb to predict multiple columns, you simply need to change the to_predict argument from a string (denoting the name of the column) to an array containing the names of the columns you want to predict. In the following example, we've altered the real estate model to predict the location and neighborhood both, instead of the rental_price .","title":"What is a multiple column prediction ?"},{"location":"tutorials/AdvancedExamples/#code-example","text":"import mindsdb_native mdb = mindsdb_native . Predictor ( name = 'multilabel_real_estate_model' ) mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , to_predict = [ 'location' , 'neighborhood' ] # Array with the names of the columns we want to predict )","title":"Code example"},{"location":"tutorials/AdvancedExamples/#multimedia-inputs-images-audio-and-video","text":"Currently, we only support images as inputs into models. We are working on support audio, you can check this issue to track the progress. Video input support is not yet planned. For any sort of media files, simply provide the full path to the file in the column. For example: [ { 'img' : '/mnt/data/img_1.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_2.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_3.jpg' 'is_bicycle' : False } ] Please provide the full path to a file on your local machine, not a url or the binary data from an image loaded up in a dataframe. Currently, the timeline on supporting multimedia output is still undecided, if you need that feature or want to implement, feel free to contact us. That being said, image outputs might actually work, we just haven't tested anything yet.","title":"Multimedia inputs (images, audio and video)"},{"location":"tutorials/AdvancedExamples/#unbalanced-dataset","text":"Given a dataset that is \"imbalanced\", the model being trained might not give the results you expect. For example, let's say you have the following dataset: data . csv x , is_power_of_9 1 , False 2 , False 3 , False 4 , False 5 , False 6 , False 7 , False 8 , False 9 , True 10 , False 11 , False . . . 81 , True 82 , False 83 , False 84 , False . . . 100 , False On which we want to predicted the aptly-named column is_power_of_9 . We have 2 occurrences of the output True and 98 occurrences of the False output. So a model could always predict False and it would have an accuracy of 98%, which is pretty decent, so unless there's a way for the model to learn those 2 instance of True without losing on accuracy, it might just decided 98% is the best possible model. However, let's say we really care about those True predictions being correct, or at least we want the model to consider them equally important, in that case we would call the learn function using the equal_accuracy_for_all_output_categories argument set to true. This essentially means that a model with 50% accuracy, with 2 correct predictions for True (100%) and 48 for False (49%) is considered better than a model with 98% accuracy that only predictions False when mindsdb trains the model. We could call this as: predictior.learn(from_data='data.csv', equal_accuracy_for_all_output_categories=True)","title":"Unbalanced dataset"},{"location":"tutorials/Timeseries/","text":"Handling Timeseries Data Timeseries interface A timeseries is a problem where rows are related to each other in a sequential way, such that the prediction of the value in the present row should take into account a number of previous rows. To build a timeseries model you need to pass timeseries_settings dictionary to learn timeseries_settings = { order_by: List<String> | Mandatory window: Int | Mandatory group_by: List<String> | Optional (default: []) nr_predictions: Int | Optional (default: 1) use_previous_target: Bool | Optional (default: True) historical_columns: List<String> | Optional (default: []) } Let's go through these settings one by one: order_by - The columns based on which the data should be ordered group_by - The columns based on which to group multiple unrelated entities present in your timeseries data. For example, let's say your data consists of sequential readings from 3x sensors. Treating the problem as a timeseries makes sense for individual sensors, so you would specify: group_by=['sensor_id'] nr_predictions - The number of points in the future that predictions should be made for, defaults to 1 . Once trained, the model will be able to predict up to this many points into the future. use_previous_target - Use the previous values of the target column[s] for making predictions. Defaults to True . window - The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. historical_columns - The temporal dynamics of these columns will be used as additional context to train the time series encoder of the predictor. Note that non-historical columns will also be used to forecast, though without considering their change through time [Status: Experimental]. Code example import mindsdb_native mdb = mindsdb_native . Predictor ( name = 'assembly_machines_model' ) mdb . learn ( from_data = 'assembly_machines_historical_data.tsv' , to_predict = 'failure' , timeseries_settings = { 'order_by' : [ 'timestamp' ], # Order the observations by timestamp 'group_by' : [ 'machine_id' ], # The ordering should be done on a per-machine basis, rather than for every single row 'nr_predictions' : 3 , # Predict failures for the timestamp given and for 2 more timesteps in the future 'use_previous_target' : True , # Use the previous values in the target column (`failure`), since when the last failure happened could be a relevant data-point for our prediction. 'window' : 20 , # Consider the previous 20 rows for every single row our model is trying to predict 'historical_columns' : [ 'sensor_activity' ] # Mark `sensor_activity` column as historical, to use its temporal dynamics as additional context } ) results = mdb . predict ( when_data = 'new_assembly_machines_data.tsv' ) Historical data When making timeseries predictions, it's important to provide mindsdb with the context for those predictions, i.e. with the previous rows that came before the one you are trying to predict for. Say your columns are: date, nr_customers, store . You order by date , group by store and need to predict nr_customers . You set window=3 . You train your model and then want to make a prediction using a csv with the following content: date, nr_customers, store 2020-10-06, unknown , A1 This prediction will be less than ideal, since mindsdb doesn't know how many customers came to the store on 2020-10-05 or 2020-10-04 , which is probably the main insight the trained model is using to make predictions. So instead you need to pass a file with the following content: date, nr_customers, store 2020-10-04, 55 , A1 2020-10-05, 123 , A1 2020-10-06, None , A1 Note that mindsdb will generate a prediction for every row here (even if the target value nr_customers already exists), but you only care about the prediction for the last row, the previous 2 are there to provide historical context. Also note that, if you window was, say, equal to 5 , you would have had to provide 4 more rows instead of 2 more. Also note that, if you were to give the file: date, nr_customers, store 2020-10-04, 55 , B11 2020-10-05, 123 , A2 2020-10-06, None , A1 This wouldn't count as historical context, since you are grouping by the store column, so only rows where store is A1 will be relevant historical context for predicting a row where the store == A1 Experimental features Some features are still very experimental and have many blindspots, so if you're interested in using them please contact us so we can help and get your feedback on how to improve. Database integration When you train MindsDB from a database, it can auto-generate a query to select historical context, based on the query you used to source your training data. This can be enabled by passing advanced_args={'use_database_history': True} to the predict call (or to the SELECT call if operating from within a database). Anomaly detection We support anomaly detection for numerical time series. Our approach tags a data point as anomalous if it falls outside of the predicted range. Normally, MindsDB will try to generate bounds that are tight enough to be informative about your predictions. However, as this feature is unsupervised (because it does not need labeled anomalies in the data), we offer two parameters to tune it for any specific use case: anomaly_error_rate (Float, 0 < x < 1 ): If smaller, it leads to wider bounds, thus less sensitivity to anomalies. If bigger, it leads to narrower bounds and increased sensitivity. By default, MindsDB will try to find an appropriate value. anomaly_cooldown (Int): period during which no anomalies will be flagged, triggered after an initial anomaly is detected. Copes with potential predictor accuracy issues while it adjusts to the anomalous event. These parameters can be passed in the advanced_args dictionary when calling predict , and do not require predictor re-training. Database example (from SQL) -- Pending, feel free to contribute some or ask us directly about this feature Database example (from code) -- Pending, feel free to contribute some or ask us directly about this feature","title":"Handling Timeseries Data"},{"location":"tutorials/Timeseries/#handling-timeseries-data","text":"","title":"Handling Timeseries Data"},{"location":"tutorials/Timeseries/#timeseries-interface","text":"A timeseries is a problem where rows are related to each other in a sequential way, such that the prediction of the value in the present row should take into account a number of previous rows. To build a timeseries model you need to pass timeseries_settings dictionary to learn timeseries_settings = { order_by: List<String> | Mandatory window: Int | Mandatory group_by: List<String> | Optional (default: []) nr_predictions: Int | Optional (default: 1) use_previous_target: Bool | Optional (default: True) historical_columns: List<String> | Optional (default: []) } Let's go through these settings one by one: order_by - The columns based on which the data should be ordered group_by - The columns based on which to group multiple unrelated entities present in your timeseries data. For example, let's say your data consists of sequential readings from 3x sensors. Treating the problem as a timeseries makes sense for individual sensors, so you would specify: group_by=['sensor_id'] nr_predictions - The number of points in the future that predictions should be made for, defaults to 1 . Once trained, the model will be able to predict up to this many points into the future. use_previous_target - Use the previous values of the target column[s] for making predictions. Defaults to True . window - The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. historical_columns - The temporal dynamics of these columns will be used as additional context to train the time series encoder of the predictor. Note that non-historical columns will also be used to forecast, though without considering their change through time [Status: Experimental].","title":"Timeseries interface"},{"location":"tutorials/Timeseries/#code-example","text":"import mindsdb_native mdb = mindsdb_native . Predictor ( name = 'assembly_machines_model' ) mdb . learn ( from_data = 'assembly_machines_historical_data.tsv' , to_predict = 'failure' , timeseries_settings = { 'order_by' : [ 'timestamp' ], # Order the observations by timestamp 'group_by' : [ 'machine_id' ], # The ordering should be done on a per-machine basis, rather than for every single row 'nr_predictions' : 3 , # Predict failures for the timestamp given and for 2 more timesteps in the future 'use_previous_target' : True , # Use the previous values in the target column (`failure`), since when the last failure happened could be a relevant data-point for our prediction. 'window' : 20 , # Consider the previous 20 rows for every single row our model is trying to predict 'historical_columns' : [ 'sensor_activity' ] # Mark `sensor_activity` column as historical, to use its temporal dynamics as additional context } ) results = mdb . predict ( when_data = 'new_assembly_machines_data.tsv' )","title":"Code example"},{"location":"tutorials/Timeseries/#historical-data","text":"When making timeseries predictions, it's important to provide mindsdb with the context for those predictions, i.e. with the previous rows that came before the one you are trying to predict for. Say your columns are: date, nr_customers, store . You order by date , group by store and need to predict nr_customers . You set window=3 . You train your model and then want to make a prediction using a csv with the following content: date, nr_customers, store 2020-10-06, unknown , A1 This prediction will be less than ideal, since mindsdb doesn't know how many customers came to the store on 2020-10-05 or 2020-10-04 , which is probably the main insight the trained model is using to make predictions. So instead you need to pass a file with the following content: date, nr_customers, store 2020-10-04, 55 , A1 2020-10-05, 123 , A1 2020-10-06, None , A1 Note that mindsdb will generate a prediction for every row here (even if the target value nr_customers already exists), but you only care about the prediction for the last row, the previous 2 are there to provide historical context. Also note that, if you window was, say, equal to 5 , you would have had to provide 4 more rows instead of 2 more. Also note that, if you were to give the file: date, nr_customers, store 2020-10-04, 55 , B11 2020-10-05, 123 , A2 2020-10-06, None , A1 This wouldn't count as historical context, since you are grouping by the store column, so only rows where store is A1 will be relevant historical context for predicting a row where the store == A1","title":"Historical data"},{"location":"tutorials/Timeseries/#experimental-features","text":"Some features are still very experimental and have many blindspots, so if you're interested in using them please contact us so we can help and get your feedback on how to improve.","title":"Experimental features"},{"location":"tutorials/Timeseries/#database-integration","text":"When you train MindsDB from a database, it can auto-generate a query to select historical context, based on the query you used to source your training data. This can be enabled by passing advanced_args={'use_database_history': True} to the predict call (or to the SELECT call if operating from within a database).","title":"Database integration"},{"location":"tutorials/Timeseries/#anomaly-detection","text":"We support anomaly detection for numerical time series. Our approach tags a data point as anomalous if it falls outside of the predicted range. Normally, MindsDB will try to generate bounds that are tight enough to be informative about your predictions. However, as this feature is unsupervised (because it does not need labeled anomalies in the data), we offer two parameters to tune it for any specific use case: anomaly_error_rate (Float, 0 < x < 1 ): If smaller, it leads to wider bounds, thus less sensitivity to anomalies. If bigger, it leads to narrower bounds and increased sensitivity. By default, MindsDB will try to find an appropriate value. anomaly_cooldown (Int): period during which no anomalies will be flagged, triggered after an initial anomaly is detected. Copes with potential predictor accuracy issues while it adjusts to the anomalous event. These parameters can be passed in the advanced_args dictionary when calling predict , and do not require predictor re-training.","title":"Anomaly detection"},{"location":"tutorials/Timeseries/#database-example-from-sql","text":"-- Pending, feel free to contribute some or ask us directly about this feature","title":"Database example (from SQL)"},{"location":"tutorials/Timeseries/#database-example-from-code","text":"-- Pending, feel free to contribute some or ask us directly about this feature","title":"Database example (from code)"},{"location":"tutorials/clickhouse/","text":"Machine Learning Models as Tables We will start this article by raising one of the most asked questions regarding Machine Learning, What is the difference between Machine Learning and Artificial Intelligence? When we think about machine learning we can think about it as a subset of Artificial Intelligence. In simple words, the idea behind machine learning is to enable machines to learn by themselves, by using small to large datasets and finding common patterns inside the data. That said, data is the core of any machine learning algorithm and having access to the data is one of the crucial steps for machine learning success. This will bring us to the second question, Where data lives these days? A great deal in databases With the increase of data in volume, variety, velocity in today's databases. Is there a better place to bring machine learning to, than being able to do machine learning right in the databases?We believe that database users meet the most important aspect of applied machine learning, which is to understand what predictive questions are important and what data is relevant to answer those questions. Additionally, adding the statistical analysis for creating the most appropriate model to that, will yield the best combination which is auto machine learning straight from the database. Bringing AutoML to those that know data best can significantly augment the capacity to solve important problems. That\u2019s why we decided to build a seamless integration with Clickhouse, in a way such that any ClickHouse user can create, train and test machine learning models with the same knowledge they have of Structured Query Language (SQL). How can we achieve this? We make use of ClickHouse\u2019s neat capabilities of accessing external tables as if they were internal tables. As such, the integration of these models is painless and transparent allowing us to: Exposing machine learning models like tables that can be queried. You simply SELECT what you want to predict and you pass in the WHERE statement the conditions for the prediction. Automatically, build, test and train machine learning models with a simple INSERT statement, where you specify what you want to learn and from what query. Why MindsDB? MindsDB is a fast-growing Open Source AutoML framework with built-in explainability - a unique visualization capability that helps better understand and trust the accuracy of predictions. With MindsDB, developers can build, train and test Machine Learning models, without needing the help of a Data Scientist/Machine Learning Engineer. It is different from typical AutoML frameworks in that MindsDB has a strong focus on trustworthiness by means of explainability, allowing users to get valuable insights into why and how the model is reaching its predictions. Why ClickHouse? Speed and efficiency are key to ClickHouse. ClickHouse can process queries up to 100 times faster than traditional databases and is the perfect solution for Digital advertising, E-commerce, Web and App analytics, Monitoring, Telecommunications analytics. In the rest of this article, we will try to describe in detail the above points with integration between MindsDB, as an Auto-Machine Learning framework and ClickHouse, as an OLAP Database Management System. How to install MindsDB Installing MindsDB is as easy as any other Python package. All you need for installation are a Python version greater than 3.6.x and around 1 GB available disk space. Other than that you just use pip or pip3 to install it as: pip install mindsdb For more detailed installation instructions please check out installation docs. If you got an error or have any questions, please post them to our support forum community.mindsdb.com How to install ClickHouse If you already have ClickHouse installed and your analytics data saved then you\u2019re ready to start playing with MindsDB, so just skip to the Connect MindsDB to ClickHouse section.If not, ClickHouse can run on any Linux or Mac OS X with x86_64 CPU architecture. Depending on your machine check out available installation options. Once the installation is done, you can start the server as a daemon: sudo service clickhouse-server start Starting the server will not display any output, so you can execute: sudo service clickhouse-server status to check that the ClickHouse is running successfully. Next, use clickhouse-client to connect to it:clickhouse-clientIf you get Code: 516. DB::Exception: Received from localhost:9000. DB::Exception: default: Authentication failed: error you will need to provide the default password that you added during the installation process: clickhouse-client --password ****** That\u2019s it. You have successfully connected to your local ClickHouse server. Import dataset to ClickHouse As with any other database management system, ClickHouse also groups tables into databases. To list the available databases you can run a show databases query that will display the default databases: SHOW DATABASES; For storing the data, we will create a new database called data: CREATE DATABASE IF NOT EXISTS data; The dataset that we will use in this tutorial provides time-series air pollution measurement information from data.seoul. Let\u2019s create a table and store the data in ClickHouse. Note that you can follow up to this tutorial with different data, just edit the example queries in line with your data. CREATE TABLE pollution_measurement ( ` Measurement date ` DateTime , ` Station code ` String , Address String , Latitude Float32 , Longitude Float32 , SO2 Decimal32 ( 5 ), NO2 Decimal32 ( 5 ), O3 Decimal32 ( 5 ), CO Decimal32 ( 5 ), PM10 Decimal32 ( 1 ), ` PM2 . 5 ` Decimal32 ( 1 ) ) ENGINE = MergeTree () ORDER BY ( ` Station code ` , ` Measurement date ` ); Note that we need to use backticks to escape the special characters in the column name. The parameters added to the Decimal32(p) are the precision of the decimal digits for e.g Decimal32(5) can contain numbers from -99999.99999 to 99999.99999. The Engine = MergeTree, specify the type of the table in ClickHouse. To learn more about all of the available table engines head over to the table-engines documentation. Lastly, what we need to do is to import the data inside the pollution_measurement table: clickhouse-client --date_time_input_format=best_effort --query=\"INSERT INTO data.pollution_measurement FORMAT CSV\" < Measurement_summary.csv The --date_time_input_format=best_effort enables the datetime parser to parse the basic and all ISO 8601 date and time formats. The pollution_measurement data should be added to the data.pollution_measurement table. To make sure it is successfully added execute SELECT query: SELECT * FROM data.pollution_measurement LIMIT 5;\u200d We are halfway there! We have successfully installed MindsDB and ClickHouse and have the data saved in the database. Now, we will use MindsDB to connect to ClickHouse and train and query Machine Learning models from the air pollution measurement data. If you don\u2019t want to install ClickHouse locally, ClickHouse Docker image is a good solution. Connect MindsDB to ClickHouse Let\u2019s start MindsDB: python3 -m mindsdb --api=mysql --config=config.json The --api parameter specifies the type of API to use (mysql). The --config specifies the location of the configuration file. The minimum required configuration for connecting to ClickHouse is: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_clickhouse\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"type\" : \"clickhouse\" , \"user\" : \"default\" } }, \"storage_dir\" : \"/storage\" } In the default_clickhouse key, include the values for connecting to ClickHouse. In the storage_dir add a path to the location where MindsDB will save some configuration as (metadata and .pickle files). The Running on http://127.0.0.1:47334/ (Press CTRL+C to quit) message will be displayed if MindsDB is successfully started. That means MindsDB server is running and listening on localhost:47334.First, when MindsDB starts it creates a database and tables inside the ClickHouse. The database created is of ENGINE type MySQL(connection), where 'connection' is established from the parameters provided inside the config.json. USE mindsdb; SHOW TABLES; The default table created inside mindsdb database will be predictors where MindsDB shall keep information about the predictors(ML models), training status, accuracy, target variable and additional training options. DESCRIBE TABLE predictors; When a user creates a new model or makes a query to any table, the query is sent by MySQL text protocol to MindsDB, where it hits the MindsDB\u2019s API\u2019s responsible for training, analyzing, querying the models. Now, we have everything ready to create a model. We are going to use the data inside the pollution_measurement table to predict the Sulfur Dioxide(SO2) in the air. Creating the model is as simple as writing the INSERT query, where we will provide values for the few required attributes. Before creating the predictor make sure mindsdb database is used: use mindsdb; INSERT INTO predictors(name, predict, select_data_query) VALUES ('airq_predictor', 'SO2', 'SELECT * FROM data.pollution_measurement where SO2 > 0 ORDER BY rand() LIMIT 10000'); The Predictor in MindsDB\u2019s words means the Machine Learning model. The columns values for creating the predictor(model) are: name (string) - the name of the predictor.predict (string) - the feature you want to predict, in this example it will be SO2. select_data_query (string) - the SELECT query that will ingest the data to train the model. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. In the background, the INSERT to predictors query will call mindsdb-native that will do a black-box analysis and start a process of extracting, analyzing, and transforming the data. It will take some time to train the model depending on the data size, columns, columns type etc, so to keep it faster we are using 5000 random columns by adding ORDERED BY rand() LIMIT 10000 to the SELECT query. You should see a message like: INSERT INTO predictors (name, predict_cols, select_data_query) VALUESOk.1 rows in set. Elapsed: 0.824 sec. To check if the training of the model successfully finished, you can run: SELECT * FROM predictors WHERE name='airq_predictor'\u200d Status complete means that the model training has finished successfully. Now, let\u2019s create predictive analytics from the data by querying the created predictor. The idea was to predict the value of Sulfur Dioxide in the Seoul air station depending on the different measured parameters as NO2, O3, CO, location etc. SELECT SO2 as predicted , SO2_confidence as confidence from airq_predictor WHERE NO2 = 0 . 005 AND CO = 1 . 2 AND PM10 = 5 ; Now you can see that MindsDB predicted that the value of Sulfur Dioxide is 0.00115645 with around 98% confidence.To get additional information about the predicted value and confidence, we should include the explain column. In that case, the MindsDB\u2019s explain functionality apart from confidence can provide additional information such as prediction quality, confidence interval, missing information for improving the prediction etc. We can extend the query and include an additional column for explanation information: SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) Now we get additional information: { \"predicted_value\": 0.001156540079952395, \"confidence\": 0.9869, \"prediction_quality\": \"very confident\", \"confidence_interval\": [0.003184904620383531, 0.013975553923630717], \"important_missing_information\": [\"Station code\", \"Latitude\", \u201cO3\u201d], \"confidence_composition\": { \"CO\": 0.006 }, \"extra_insights\": { \"if_missing\": [{ \"NO2\": 0.007549311956155897 }, { \"CO\": 0.005459383721227349 }, { \"PM10\": 0.003870252306568623 }] } } By looking at the new information we can see that MindsDB is very confident about the quality of this prediction. The range of values where the predicted value lies within is determined inside the confidence interval. Also, the extra insights are providing SO2 value in a case where some of the included features (in WHERE clause) are not provided. MindsDB thinks that Station code and Latitude and O3 are very important features for more precise prediction so those values shall be included in the WHERE clause. Let\u2019s try including Station code and see the new predictions: SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 Now the predicted value has changed by adding the feature that MindsDB thought is quite important for better prediction. Additionally we can try and predict the Sulfur Dioxide in the air in some future date. What we can do is just include the Measurement date value inside WHERE clause for the specific date we want to get prediction e.g : SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 AND ` Measurement date `= \u2019 2020 - 07 - 03 00 : 01 : 00 \u2019 Or few weeks later as: At the end, the whole flow was as simple as seeing MindsDB as a database inside ClickHouse and executing queries for INSERT and SELECT directly from it. If you follow up to this tutorial with your own data, we are happy to hear about how MindsDB has come in useful to you. Everything that we did in this tutorial will be available through the MindsDB\u2019s Graphical User Interface MindsDB Scout in the next release. That means with a few clicks on MindsDB Scout you can successfully train ML models from your ClickHouse database too.","title":"Machine Learning Models as Tables"},{"location":"tutorials/clickhouse/#machine-learning-models-as-tables","text":"We will start this article by raising one of the most asked questions regarding Machine Learning, What is the difference between Machine Learning and Artificial Intelligence? When we think about machine learning we can think about it as a subset of Artificial Intelligence. In simple words, the idea behind machine learning is to enable machines to learn by themselves, by using small to large datasets and finding common patterns inside the data. That said, data is the core of any machine learning algorithm and having access to the data is one of the crucial steps for machine learning success. This will bring us to the second question, Where data lives these days?","title":"Machine Learning Models as Tables"},{"location":"tutorials/clickhouse/#a-great-deal-in-databases","text":"With the increase of data in volume, variety, velocity in today's databases. Is there a better place to bring machine learning to, than being able to do machine learning right in the databases?We believe that database users meet the most important aspect of applied machine learning, which is to understand what predictive questions are important and what data is relevant to answer those questions. Additionally, adding the statistical analysis for creating the most appropriate model to that, will yield the best combination which is auto machine learning straight from the database. Bringing AutoML to those that know data best can significantly augment the capacity to solve important problems. That\u2019s why we decided to build a seamless integration with Clickhouse, in a way such that any ClickHouse user can create, train and test machine learning models with the same knowledge they have of Structured Query Language (SQL).","title":"A great deal in databases"},{"location":"tutorials/clickhouse/#how-can-we-achieve-this","text":"We make use of ClickHouse\u2019s neat capabilities of accessing external tables as if they were internal tables. As such, the integration of these models is painless and transparent allowing us to: Exposing machine learning models like tables that can be queried. You simply SELECT what you want to predict and you pass in the WHERE statement the conditions for the prediction. Automatically, build, test and train machine learning models with a simple INSERT statement, where you specify what you want to learn and from what query.","title":"How can we achieve this?"},{"location":"tutorials/clickhouse/#why-mindsdb","text":"MindsDB is a fast-growing Open Source AutoML framework with built-in explainability - a unique visualization capability that helps better understand and trust the accuracy of predictions. With MindsDB, developers can build, train and test Machine Learning models, without needing the help of a Data Scientist/Machine Learning Engineer. It is different from typical AutoML frameworks in that MindsDB has a strong focus on trustworthiness by means of explainability, allowing users to get valuable insights into why and how the model is reaching its predictions.","title":"Why MindsDB?"},{"location":"tutorials/clickhouse/#why-clickhouse","text":"Speed and efficiency are key to ClickHouse. ClickHouse can process queries up to 100 times faster than traditional databases and is the perfect solution for Digital advertising, E-commerce, Web and App analytics, Monitoring, Telecommunications analytics. In the rest of this article, we will try to describe in detail the above points with integration between MindsDB, as an Auto-Machine Learning framework and ClickHouse, as an OLAP Database Management System.","title":"Why ClickHouse?"},{"location":"tutorials/clickhouse/#how-to-install-mindsdb","text":"Installing MindsDB is as easy as any other Python package. All you need for installation are a Python version greater than 3.6.x and around 1 GB available disk space. Other than that you just use pip or pip3 to install it as: pip install mindsdb For more detailed installation instructions please check out installation docs. If you got an error or have any questions, please post them to our support forum community.mindsdb.com","title":"How to install MindsDB"},{"location":"tutorials/clickhouse/#how-to-install-clickhouse","text":"If you already have ClickHouse installed and your analytics data saved then you\u2019re ready to start playing with MindsDB, so just skip to the Connect MindsDB to ClickHouse section.If not, ClickHouse can run on any Linux or Mac OS X with x86_64 CPU architecture. Depending on your machine check out available installation options. Once the installation is done, you can start the server as a daemon: sudo service clickhouse-server start Starting the server will not display any output, so you can execute: sudo service clickhouse-server status to check that the ClickHouse is running successfully. Next, use clickhouse-client to connect to it:clickhouse-clientIf you get Code: 516. DB::Exception: Received from localhost:9000. DB::Exception: default: Authentication failed: error you will need to provide the default password that you added during the installation process: clickhouse-client --password ****** That\u2019s it. You have successfully connected to your local ClickHouse server.","title":"How to install ClickHouse"},{"location":"tutorials/clickhouse/#import-dataset-to-clickhouse","text":"As with any other database management system, ClickHouse also groups tables into databases. To list the available databases you can run a show databases query that will display the default databases: SHOW DATABASES; For storing the data, we will create a new database called data: CREATE DATABASE IF NOT EXISTS data; The dataset that we will use in this tutorial provides time-series air pollution measurement information from data.seoul. Let\u2019s create a table and store the data in ClickHouse. Note that you can follow up to this tutorial with different data, just edit the example queries in line with your data. CREATE TABLE pollution_measurement ( ` Measurement date ` DateTime , ` Station code ` String , Address String , Latitude Float32 , Longitude Float32 , SO2 Decimal32 ( 5 ), NO2 Decimal32 ( 5 ), O3 Decimal32 ( 5 ), CO Decimal32 ( 5 ), PM10 Decimal32 ( 1 ), ` PM2 . 5 ` Decimal32 ( 1 ) ) ENGINE = MergeTree () ORDER BY ( ` Station code ` , ` Measurement date ` ); Note that we need to use backticks to escape the special characters in the column name. The parameters added to the Decimal32(p) are the precision of the decimal digits for e.g Decimal32(5) can contain numbers from -99999.99999 to 99999.99999. The Engine = MergeTree, specify the type of the table in ClickHouse. To learn more about all of the available table engines head over to the table-engines documentation. Lastly, what we need to do is to import the data inside the pollution_measurement table: clickhouse-client --date_time_input_format=best_effort --query=\"INSERT INTO data.pollution_measurement FORMAT CSV\" < Measurement_summary.csv The --date_time_input_format=best_effort enables the datetime parser to parse the basic and all ISO 8601 date and time formats. The pollution_measurement data should be added to the data.pollution_measurement table. To make sure it is successfully added execute SELECT query: SELECT * FROM data.pollution_measurement LIMIT 5;\u200d We are halfway there! We have successfully installed MindsDB and ClickHouse and have the data saved in the database. Now, we will use MindsDB to connect to ClickHouse and train and query Machine Learning models from the air pollution measurement data. If you don\u2019t want to install ClickHouse locally, ClickHouse Docker image is a good solution.","title":"Import dataset to ClickHouse"},{"location":"tutorials/clickhouse/#connect-mindsdb-to-clickhouse","text":"Let\u2019s start MindsDB: python3 -m mindsdb --api=mysql --config=config.json The --api parameter specifies the type of API to use (mysql). The --config specifies the location of the configuration file. The minimum required configuration for connecting to ClickHouse is: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_clickhouse\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"type\" : \"clickhouse\" , \"user\" : \"default\" } }, \"storage_dir\" : \"/storage\" } In the default_clickhouse key, include the values for connecting to ClickHouse. In the storage_dir add a path to the location where MindsDB will save some configuration as (metadata and .pickle files). The Running on http://127.0.0.1:47334/ (Press CTRL+C to quit) message will be displayed if MindsDB is successfully started. That means MindsDB server is running and listening on localhost:47334.First, when MindsDB starts it creates a database and tables inside the ClickHouse. The database created is of ENGINE type MySQL(connection), where 'connection' is established from the parameters provided inside the config.json. USE mindsdb; SHOW TABLES; The default table created inside mindsdb database will be predictors where MindsDB shall keep information about the predictors(ML models), training status, accuracy, target variable and additional training options. DESCRIBE TABLE predictors; When a user creates a new model or makes a query to any table, the query is sent by MySQL text protocol to MindsDB, where it hits the MindsDB\u2019s API\u2019s responsible for training, analyzing, querying the models. Now, we have everything ready to create a model. We are going to use the data inside the pollution_measurement table to predict the Sulfur Dioxide(SO2) in the air. Creating the model is as simple as writing the INSERT query, where we will provide values for the few required attributes. Before creating the predictor make sure mindsdb database is used: use mindsdb; INSERT INTO predictors(name, predict, select_data_query) VALUES ('airq_predictor', 'SO2', 'SELECT * FROM data.pollution_measurement where SO2 > 0 ORDER BY rand() LIMIT 10000'); The Predictor in MindsDB\u2019s words means the Machine Learning model. The columns values for creating the predictor(model) are: name (string) - the name of the predictor.predict (string) - the feature you want to predict, in this example it will be SO2. select_data_query (string) - the SELECT query that will ingest the data to train the model. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. In the background, the INSERT to predictors query will call mindsdb-native that will do a black-box analysis and start a process of extracting, analyzing, and transforming the data. It will take some time to train the model depending on the data size, columns, columns type etc, so to keep it faster we are using 5000 random columns by adding ORDERED BY rand() LIMIT 10000 to the SELECT query. You should see a message like: INSERT INTO predictors (name, predict_cols, select_data_query) VALUESOk.1 rows in set. Elapsed: 0.824 sec. To check if the training of the model successfully finished, you can run: SELECT * FROM predictors WHERE name='airq_predictor'\u200d Status complete means that the model training has finished successfully. Now, let\u2019s create predictive analytics from the data by querying the created predictor. The idea was to predict the value of Sulfur Dioxide in the Seoul air station depending on the different measured parameters as NO2, O3, CO, location etc. SELECT SO2 as predicted , SO2_confidence as confidence from airq_predictor WHERE NO2 = 0 . 005 AND CO = 1 . 2 AND PM10 = 5 ; Now you can see that MindsDB predicted that the value of Sulfur Dioxide is 0.00115645 with around 98% confidence.To get additional information about the predicted value and confidence, we should include the explain column. In that case, the MindsDB\u2019s explain functionality apart from confidence can provide additional information such as prediction quality, confidence interval, missing information for improving the prediction etc. We can extend the query and include an additional column for explanation information: SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) Now we get additional information: { \"predicted_value\": 0.001156540079952395, \"confidence\": 0.9869, \"prediction_quality\": \"very confident\", \"confidence_interval\": [0.003184904620383531, 0.013975553923630717], \"important_missing_information\": [\"Station code\", \"Latitude\", \u201cO3\u201d], \"confidence_composition\": { \"CO\": 0.006 }, \"extra_insights\": { \"if_missing\": [{ \"NO2\": 0.007549311956155897 }, { \"CO\": 0.005459383721227349 }, { \"PM10\": 0.003870252306568623 }] } } By looking at the new information we can see that MindsDB is very confident about the quality of this prediction. The range of values where the predicted value lies within is determined inside the confidence interval. Also, the extra insights are providing SO2 value in a case where some of the included features (in WHERE clause) are not provided. MindsDB thinks that Station code and Latitude and O3 are very important features for more precise prediction so those values shall be included in the WHERE clause. Let\u2019s try including Station code and see the new predictions: SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 Now the predicted value has changed by adding the feature that MindsDB thought is quite important for better prediction. Additionally we can try and predict the Sulfur Dioxide in the air in some future date. What we can do is just include the Measurement date value inside WHERE clause for the specific date we want to get prediction e.g : SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 AND ` Measurement date `= \u2019 2020 - 07 - 03 00 : 01 : 00 \u2019 Or few weeks later as: At the end, the whole flow was as simple as seeing MindsDB as a database inside ClickHouse and executing queries for INSERT and SELECT directly from it. If you follow up to this tutorial with your own data, we are happy to hear about how MindsDB has come in useful to you. Everything that we did in this tutorial will be available through the MindsDB\u2019s Graphical User Interface MindsDB Scout in the next release. That means with a few clicks on MindsDB Scout you can successfully train ML models from your ClickHouse database too.","title":"Connect MindsDB to ClickHouse"},{"location":"tutorials/mariadb/","text":"AI-Tables in MariaDB Database users are the best to know what data is relevant for ML models. Virtual AI tables in MariaDB allows users to run Automated Machine Learning models directly from inside the database. This tutorial is an overview of this integration capability. Anyone that has dealt with Machine Learning (ML) understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn\u2019t it make sense to bring machine learning capabilities straight to the database itself? To do so, we have developed a concept called AITables. In this article we want to present to you what AITables are, how you can use them in MariaDB. Invite you to try it out, get involved and to join us in the journey of ML meets MariaDB. AiTables AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example. The used car price example Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. Also, you use MariaDB and in your database there is a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn\u2019t it be nice if you could simply tell your MariaDB server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked into MariaDB are here to do exactly that. Although further down in this article we will guide you step by step on how to run this example yourself, let us introduce you to what you can do and how it looks in standard SQL. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called \u2018used_cars_model\u2019. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated \u2018used_cars_model\u2019 AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions are key for your business and what data you want your ML to learn from to make such predictions. By now, you might be thinking, how do I get started?, what do I need? How can I run this example in my environment? As promised, in the rest of this article we will show you step-by-step instructions on how to integrate MindsDB into your MariaDB server, how to build, test and use Machine Learning Models as AI-Tables all without the need for specific machine learning skills and how to evaluate prediction results in an \u201cExplainable AI\u201d way. If you want to preview this tutorial visually go ahead to our youtube channel and follow up the Machine Learning in MariaDB with AI Tables video. MindsDB AITables in MariaDB With MindsDB any MariaDB user can train and test neural-networks based Machine Learning models with the same knowledge they have of SQL. MindsDB is an open-source ML framework that enables running Machine Learning Models as AI-Tables. On top of that it has an exciting \u201cExplainable AI\u201d feature that allows users to get insights into their Machine Prediction accuracy score and evaluate its dependencies. For example, users can estimate how adding or removing certain data would impact on the effectiveness of the prediction. The whole integration consists of two important parts: * The Machine Learning models are exposed as database tables (AI-Tables) that can be queried with the SELECT statements. * The ML model generation and training is done through a simple INSERT statement. This is possible thanks to MariaDBs CONNECT engine, which enables us to publish tables that live outside MariaDB. Since MindsDB supports the MySQL tcp-ip protocol, AI-Tables can be plugged as if they are external MariaDB tables. The following diagram illustrates this process. The resource intensive Machine Learning tasks like model training happen on a separate MindsDB server instance or in the cloud, so that the Database performance is not affected. In the following step-by-step tutorial you will learn how to install a MindsDB server with MariaDB and connect to the data, how to train the model and get predictions using an example dataset. So let\u2019s get started. How to install MariaDB? If you already have MariaDB installed you can skip this section. MariaDB is one of the most popular database servers in the world and works on the most widely used operating systems. You can find the installation binaries and packages on the mariadb download site. To check the full list of distributions which include MariaDB head over to list of distributions . How to install MindsDB? Before you install MindsDB you need a Python version greater than 3.6 and pip >=19.3 which comes pre-installed with newer Python versions. Also, you will need to have around 1GB free space on your machine for installing the MindsDB\u2019s dependencies. Other than that the installation is quite simple. Inside your virtual environment just run: pip install mindsdb To check if the installation was successful run: pip show mindsdb And you should be able to see the MindsDB information as name, version, summary, license etc: That\u2019s all. Let\u2019s set up the required configuration and start MindsDB. Example Dataset If you are following this tutorial with your own data, you can skip to the next section. For this example we will use the Used Car Price dataset from the 100k used cars scraped data. The dataset contains information on price, transmission, mileage, fuel type, road tax, miles per gallon (mpg), and engine size of the used cars in the UK. The idea is to predict the price depending on the above features. Add data to MariaDB The first thing we need to do is to create the table. Execute the below query: CREATE TABLE ` used_cars_data ` ( ` model ` VARCHAR ( 100 ) DEFAULT NULL , ` year ` INT ( 11 ) DEFAULT NULL , ` price ` INT ( 11 ) DEFAULT NULL , ` transmission ` VARCHAR ( 100 ) DEFAULT NULL , ` mileage ` INT ( 11 ) DEFAULT NULL , ` fueltype ` VARCHAR ( 100 ) DEFAULT NULL , ` tax ` INT ( 11 ) DEFAULT NULL , ` mpg ` FLOAT DEFAULT NULL , ` enginesize ` FLOAT DEFAULT NULL ) engine = innodb DEFAULT charset = latin1 The InnoDB is a general storage engine and the one offered as the best choice in most cases from the MariaDB team.To see the list with all available ENGINEs and for advice on which one to choose check out the MariaDB engine docs . After creating the table there are a few options that you can do to add the data inside MariaDB. If you are using graphical clients such as dbForge, DBeaver or another SQL client use the import option from the menu. Use the LOAD DATA statement that reads the local file from the location provided and sends the content to the MariaDB Server: LOAD DATA LOCAL INFILE 'data.csv' INTO TABLE used_cars_data FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; The TERMINATED BY specifies the separator in the data. The escape characters and new lines are manageable with the ENCLOSED BY and LINES TERMINATED BY clause. Using mysqlimport . This is the cli for the above, LOAD DATA statement. The arguments sent here correspond to the clauses of the LOAD DATA example: mysqlimport --local --fields-terminated-by=\",\" used_cars_data data.csv Let\u2019s select the data from used_cars_data table to make sure it is successfully imported: SELECT * FROM test . used_cars_data LIMIT 5 ; The data is inside MariaDB so the next step is to add the required configuration. Required Configuration The first thing we need to set up is the required configuration for MindsDB Server. Let\u2019s create a new file called config.json and add the following example: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_mariadb\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"storage_dir\" : \"/storage\" } This looks like a big configuration file but it is quite simple. What we have included are: api \u2013 The keys here are specifying the host and port for MindsDB REST Apis. mysql \u2013 All of the information for connecting to mysql and using the mysql protocol as host, user, password and port. Also the log level information and the path to your local SSL certificate. If certificate_path is left empty, MindsDB will automatically create one. integrations \u2013 Here, we are specifying the type of integration that we will use, default_mariadb. Also the required parameters for connecting to it as host, port, user, password. Other supported databases for integrations are ClickHouse, PostgreSQL, MySQL and Microsoft SQL Server. interface \u2013 The required keys added here are datastore and mindsdb_native, that contains the path to the storage location, which will be used by MindsDB to save some of the configuration files. And the last thing we need to do is to install the CONNECT storage engine plugin that we have added in the plugin-load-add variable. To check how to download it for your OS check installing docs. It should be quite simple as using the package manager, e.g: sudo apt - get install mariadb - plugin - connect That\u2019s pretty much everything related to the configuration required for successful integration with MariaDB. Let\u2019s jump to the interesting part where we will train the machine learning model and query it. AutoML inside MariaDB First, we need to start MindsDB: python3 -m mindsdb --api=mysql --config=config.json The flags added here are: \u2013api \u2013 This specifies the type of API we will use with MindsDB, in this case mysql. \u2013config \u2013 The path to the config.json file we have created before. If MindsDB was successfully started there should be a new database automatically created in MariaDB called mindsdb with two tables (commands and predictors). In these tables, MindsDB will keep information about the models, model accuracy, training status, target variable that we will predict and additional options used for model training. Create new predictor The main motto when we first started MindsDB was with one line of code, so now we will try to stick to it and present that in the databases with just one query. The Predictor in MindsDB\u2019s words means Machine Learning model, so creating one could be done with an INSERT statement inside mindsdb.predictors table. Execute the following query: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); What this query does is it creates a new model called \u2018used_cars_model\u2019 , specifies the column that we will try to predict as \u2018price\u2019 from the used_cars_data table. The required columns(parameters) for training the predictor are: name (string) \u2013 the name of the predictor. predict (string) \u2013 the feature you want to predict, in this example it will be price. select_data_query (string) \u2013 the SELECT query that will get the data from MariaDB. training_options (dictionary) \u2013 optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. You should see the message about the successful execution of the query and if you open up the console, the MindsDB logger shall display messages while training the model. Training could take some time depending on the data used, columns types, size etc. In our example not more than 2-3 min. To check that model was successfully trained run: SELECT * FROM mindsdb . predictors WHERE name = 'used_cars_model' The column status shall be complete and the model accuracy will be saved when the training finishes. The model has been trained successfully. It was quite simple because we didn\u2019t do any hyperparameters tuning or features engineering and leave that out to MindsDB as an AutoML framework to try and fit the best model. With the INSERT query, we just provided labeled data as an input. The next step is to query the trained model with SELECT and get the output from it (predict the price of the car). Query the predictor To get the prediction from the model is as easy as executing the SELECT statement where we will select the price and price confidence. The main idea is to predict the used car\u2019s price depending on the different features. The first thing that comes to mind when looking for the used car is the car model, fuel type, mileage, the year when the car was produced etc. We should include in the WHERE clause all of this information and leave it to MindsDB to make the predictions for them e.g: SELECT price AS predicted , price_confidence AS confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; You should see that MindsDB is quite confident that the car with all of the above characteristics as included in the WHERE clause shall cost around 13,111. To get additional information about the prediction include the explain column in the SELECT e.g: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; Note that to beautify the resultFormat you can add command line option format for particular session. Now MindsDB will display additional information in the info column as prediction quality, confidence interval, missing information for improving the prediction etc. { \"predicted_value\" : 13772 , \"confidence\" : 0.9922 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10795 , 31666 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.008 , \"year\" : 0.018 , \"transmission\" : 0.001 , \"mpg\" : 0.001 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 13661 }, { \"year\" : 17136 }, { \"transmission\" : 3405 }, { \"mileage\" : 15281 }, { \"fuelType\" : 7877 }, { \"tax\" : 13908 }, { \"mpg\" : 38858 }, { \"engineSize\" : 13772 }] } } The confidence_interval specifies the probability that the value of a price lies within the range of 10k to 30k. The important_missing_information in this case is empty, but if we omit some of the important values in the WHERE clause e.g price, year or mpg, MindsDB shall warn us that that column is important for the model. The if_missing in the extra_insights shows the price value if some of the mentioned columns are missing. Now, let\u2019s try and get the price prediction for different car models with different fuel type, mileage, engine size, transmission: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a1\" AND mileage = 122946 AND transmission = \"manual\" AND fueltype = \"petrol\" AND mpg = \"35.4\" AND enginesize = 1 . 4 AND year = 2014 AND tax = 30 ; Now, MindsDB thinks that this type of car would cost around 12k and price ranges to 23k. Let\u2019s sum up what we did and the simple steps we take to get the predictions: Install MariaDB and MindsDB. Setup the configuration. Train the model with an INSERT query. Get predictions from the model with a SELECT query. Quite simple right? This is a brand new feature that we have developed so we are happy to hear your opinions on it. You can play around and query the model with different values or train and query the model with different datasets. If you have some interesting results or you found some issues we are happy to help and talk with you. Join our community forum or reach out to us on GitHub .","title":"AI-Tables in MariaDB"},{"location":"tutorials/mariadb/#ai-tables-in-mariadb","text":"Database users are the best to know what data is relevant for ML models. Virtual AI tables in MariaDB allows users to run Automated Machine Learning models directly from inside the database. This tutorial is an overview of this integration capability. Anyone that has dealt with Machine Learning (ML) understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn\u2019t it make sense to bring machine learning capabilities straight to the database itself? To do so, we have developed a concept called AITables. In this article we want to present to you what AITables are, how you can use them in MariaDB. Invite you to try it out, get involved and to join us in the journey of ML meets MariaDB.","title":"AI-Tables in MariaDB"},{"location":"tutorials/mariadb/#aitables","text":"AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example.","title":"AiTables"},{"location":"tutorials/mariadb/#the-used-car-price-example","text":"Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. Also, you use MariaDB and in your database there is a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn\u2019t it be nice if you could simply tell your MariaDB server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked into MariaDB are here to do exactly that. Although further down in this article we will guide you step by step on how to run this example yourself, let us introduce you to what you can do and how it looks in standard SQL. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called \u2018used_cars_model\u2019. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated \u2018used_cars_model\u2019 AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions are key for your business and what data you want your ML to learn from to make such predictions. By now, you might be thinking, how do I get started?, what do I need? How can I run this example in my environment? As promised, in the rest of this article we will show you step-by-step instructions on how to integrate MindsDB into your MariaDB server, how to build, test and use Machine Learning Models as AI-Tables all without the need for specific machine learning skills and how to evaluate prediction results in an \u201cExplainable AI\u201d way. If you want to preview this tutorial visually go ahead to our youtube channel and follow up the Machine Learning in MariaDB with AI Tables video.","title":"The used car price example"},{"location":"tutorials/mariadb/#mindsdb-aitables-in-mariadb","text":"With MindsDB any MariaDB user can train and test neural-networks based Machine Learning models with the same knowledge they have of SQL. MindsDB is an open-source ML framework that enables running Machine Learning Models as AI-Tables. On top of that it has an exciting \u201cExplainable AI\u201d feature that allows users to get insights into their Machine Prediction accuracy score and evaluate its dependencies. For example, users can estimate how adding or removing certain data would impact on the effectiveness of the prediction. The whole integration consists of two important parts: * The Machine Learning models are exposed as database tables (AI-Tables) that can be queried with the SELECT statements. * The ML model generation and training is done through a simple INSERT statement. This is possible thanks to MariaDBs CONNECT engine, which enables us to publish tables that live outside MariaDB. Since MindsDB supports the MySQL tcp-ip protocol, AI-Tables can be plugged as if they are external MariaDB tables. The following diagram illustrates this process. The resource intensive Machine Learning tasks like model training happen on a separate MindsDB server instance or in the cloud, so that the Database performance is not affected. In the following step-by-step tutorial you will learn how to install a MindsDB server with MariaDB and connect to the data, how to train the model and get predictions using an example dataset. So let\u2019s get started.","title":"MindsDB AITables in MariaDB"},{"location":"tutorials/mariadb/#how-to-install-mariadb","text":"If you already have MariaDB installed you can skip this section. MariaDB is one of the most popular database servers in the world and works on the most widely used operating systems. You can find the installation binaries and packages on the mariadb download site. To check the full list of distributions which include MariaDB head over to list of distributions .","title":"How to install MariaDB?"},{"location":"tutorials/mariadb/#how-to-install-mindsdb","text":"Before you install MindsDB you need a Python version greater than 3.6 and pip >=19.3 which comes pre-installed with newer Python versions. Also, you will need to have around 1GB free space on your machine for installing the MindsDB\u2019s dependencies. Other than that the installation is quite simple. Inside your virtual environment just run: pip install mindsdb To check if the installation was successful run: pip show mindsdb And you should be able to see the MindsDB information as name, version, summary, license etc: That\u2019s all. Let\u2019s set up the required configuration and start MindsDB.","title":"How to install MindsDB?"},{"location":"tutorials/mariadb/#example-dataset","text":"If you are following this tutorial with your own data, you can skip to the next section. For this example we will use the Used Car Price dataset from the 100k used cars scraped data. The dataset contains information on price, transmission, mileage, fuel type, road tax, miles per gallon (mpg), and engine size of the used cars in the UK. The idea is to predict the price depending on the above features.","title":"Example Dataset"},{"location":"tutorials/mariadb/#add-data-to-mariadb","text":"The first thing we need to do is to create the table. Execute the below query: CREATE TABLE ` used_cars_data ` ( ` model ` VARCHAR ( 100 ) DEFAULT NULL , ` year ` INT ( 11 ) DEFAULT NULL , ` price ` INT ( 11 ) DEFAULT NULL , ` transmission ` VARCHAR ( 100 ) DEFAULT NULL , ` mileage ` INT ( 11 ) DEFAULT NULL , ` fueltype ` VARCHAR ( 100 ) DEFAULT NULL , ` tax ` INT ( 11 ) DEFAULT NULL , ` mpg ` FLOAT DEFAULT NULL , ` enginesize ` FLOAT DEFAULT NULL ) engine = innodb DEFAULT charset = latin1 The InnoDB is a general storage engine and the one offered as the best choice in most cases from the MariaDB team.To see the list with all available ENGINEs and for advice on which one to choose check out the MariaDB engine docs . After creating the table there are a few options that you can do to add the data inside MariaDB. If you are using graphical clients such as dbForge, DBeaver or another SQL client use the import option from the menu. Use the LOAD DATA statement that reads the local file from the location provided and sends the content to the MariaDB Server: LOAD DATA LOCAL INFILE 'data.csv' INTO TABLE used_cars_data FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; The TERMINATED BY specifies the separator in the data. The escape characters and new lines are manageable with the ENCLOSED BY and LINES TERMINATED BY clause. Using mysqlimport . This is the cli for the above, LOAD DATA statement. The arguments sent here correspond to the clauses of the LOAD DATA example: mysqlimport --local --fields-terminated-by=\",\" used_cars_data data.csv Let\u2019s select the data from used_cars_data table to make sure it is successfully imported: SELECT * FROM test . used_cars_data LIMIT 5 ; The data is inside MariaDB so the next step is to add the required configuration.","title":"Add data to MariaDB"},{"location":"tutorials/mariadb/#required-configuration","text":"The first thing we need to set up is the required configuration for MindsDB Server. Let\u2019s create a new file called config.json and add the following example: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_mariadb\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"storage_dir\" : \"/storage\" } This looks like a big configuration file but it is quite simple. What we have included are: api \u2013 The keys here are specifying the host and port for MindsDB REST Apis. mysql \u2013 All of the information for connecting to mysql and using the mysql protocol as host, user, password and port. Also the log level information and the path to your local SSL certificate. If certificate_path is left empty, MindsDB will automatically create one. integrations \u2013 Here, we are specifying the type of integration that we will use, default_mariadb. Also the required parameters for connecting to it as host, port, user, password. Other supported databases for integrations are ClickHouse, PostgreSQL, MySQL and Microsoft SQL Server. interface \u2013 The required keys added here are datastore and mindsdb_native, that contains the path to the storage location, which will be used by MindsDB to save some of the configuration files. And the last thing we need to do is to install the CONNECT storage engine plugin that we have added in the plugin-load-add variable. To check how to download it for your OS check installing docs. It should be quite simple as using the package manager, e.g: sudo apt - get install mariadb - plugin - connect That\u2019s pretty much everything related to the configuration required for successful integration with MariaDB. Let\u2019s jump to the interesting part where we will train the machine learning model and query it.","title":"Required Configuration"},{"location":"tutorials/mariadb/#automl-inside-mariadb","text":"First, we need to start MindsDB: python3 -m mindsdb --api=mysql --config=config.json The flags added here are: \u2013api \u2013 This specifies the type of API we will use with MindsDB, in this case mysql. \u2013config \u2013 The path to the config.json file we have created before. If MindsDB was successfully started there should be a new database automatically created in MariaDB called mindsdb with two tables (commands and predictors). In these tables, MindsDB will keep information about the models, model accuracy, training status, target variable that we will predict and additional options used for model training.","title":"AutoML inside MariaDB"},{"location":"tutorials/mariadb/#create-new-predictor","text":"The main motto when we first started MindsDB was with one line of code, so now we will try to stick to it and present that in the databases with just one query. The Predictor in MindsDB\u2019s words means Machine Learning model, so creating one could be done with an INSERT statement inside mindsdb.predictors table. Execute the following query: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); What this query does is it creates a new model called \u2018used_cars_model\u2019 , specifies the column that we will try to predict as \u2018price\u2019 from the used_cars_data table. The required columns(parameters) for training the predictor are: name (string) \u2013 the name of the predictor. predict (string) \u2013 the feature you want to predict, in this example it will be price. select_data_query (string) \u2013 the SELECT query that will get the data from MariaDB. training_options (dictionary) \u2013 optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. You should see the message about the successful execution of the query and if you open up the console, the MindsDB logger shall display messages while training the model. Training could take some time depending on the data used, columns types, size etc. In our example not more than 2-3 min. To check that model was successfully trained run: SELECT * FROM mindsdb . predictors WHERE name = 'used_cars_model' The column status shall be complete and the model accuracy will be saved when the training finishes. The model has been trained successfully. It was quite simple because we didn\u2019t do any hyperparameters tuning or features engineering and leave that out to MindsDB as an AutoML framework to try and fit the best model. With the INSERT query, we just provided labeled data as an input. The next step is to query the trained model with SELECT and get the output from it (predict the price of the car).","title":"Create new predictor"},{"location":"tutorials/mariadb/#query-the-predictor","text":"To get the prediction from the model is as easy as executing the SELECT statement where we will select the price and price confidence. The main idea is to predict the used car\u2019s price depending on the different features. The first thing that comes to mind when looking for the used car is the car model, fuel type, mileage, the year when the car was produced etc. We should include in the WHERE clause all of this information and leave it to MindsDB to make the predictions for them e.g: SELECT price AS predicted , price_confidence AS confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; You should see that MindsDB is quite confident that the car with all of the above characteristics as included in the WHERE clause shall cost around 13,111. To get additional information about the prediction include the explain column in the SELECT e.g: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; Note that to beautify the resultFormat you can add command line option format for particular session. Now MindsDB will display additional information in the info column as prediction quality, confidence interval, missing information for improving the prediction etc. { \"predicted_value\" : 13772 , \"confidence\" : 0.9922 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10795 , 31666 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.008 , \"year\" : 0.018 , \"transmission\" : 0.001 , \"mpg\" : 0.001 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 13661 }, { \"year\" : 17136 }, { \"transmission\" : 3405 }, { \"mileage\" : 15281 }, { \"fuelType\" : 7877 }, { \"tax\" : 13908 }, { \"mpg\" : 38858 }, { \"engineSize\" : 13772 }] } } The confidence_interval specifies the probability that the value of a price lies within the range of 10k to 30k. The important_missing_information in this case is empty, but if we omit some of the important values in the WHERE clause e.g price, year or mpg, MindsDB shall warn us that that column is important for the model. The if_missing in the extra_insights shows the price value if some of the mentioned columns are missing. Now, let\u2019s try and get the price prediction for different car models with different fuel type, mileage, engine size, transmission: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a1\" AND mileage = 122946 AND transmission = \"manual\" AND fueltype = \"petrol\" AND mpg = \"35.4\" AND enginesize = 1 . 4 AND year = 2014 AND tax = 30 ; Now, MindsDB thinks that this type of car would cost around 12k and price ranges to 23k. Let\u2019s sum up what we did and the simple steps we take to get the predictions: Install MariaDB and MindsDB. Setup the configuration. Train the model with an INSERT query. Get predictions from the model with a SELECT query. Quite simple right? This is a brand new feature that we have developed so we are happy to hear your opinions on it. You can play around and query the model with different values or train and query the model with different datasets. If you have some interesting results or you found some issues we are happy to help and talk with you. Join our community forum or reach out to us on GitHub .","title":"Query the predictor"},{"location":"tutorials/microsoft-sql-server/","text":"Automated Machine Learning in Microsoft SQL Server Data is the main ingredient for machine learning. Nowadays most enterprise structured data lives inside a database, so moving the machine learning inside the database layer can bring great benefits. This is one of the main reasons we decided to build an integration with the most widely used database systems, so that any database user can create, train and query machine learning models with minimal knowledge of SQL. Using the MindsDB AI Tables feature, users can automate model training workflows and reduce deployment time and complexity by adding the predictive layer to the database. AI Tables allow database users to query predictive models in seconds as if they were data that existed on the table. Simply explained, AI Tables allow users to use machine learning models as if they were normal database tables. In plain SQL, that looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > MindsDB acts like an AutoML platform and interacts with your SQL database through an ODBC driver. AI Tables accelerate development speed, reduce costs and eliminate the complexity of machine learning workflows. Train models from the data inside the SQL database Get predictions through simple SQL queries Get insights into model accuracy Move from idea to production in minutes In this tutorial, you will get step-by-step instructions on how to enable AI Tables in Microsoft SQL Server. Based on a medical insurance example dataset, you will see how to train and query machine learning models by using only SQL statements. How to install MindsDB If you already have MindsDB installed, you can skip this section. MindsDB is a cross-platform tool and can be installed on the most widely used OSs: Windows, Linux and macOS. Also, there are few other options to get MindsDB, such as through Docker, installing from source code or using PyPi. To get started, check the following list, and use the option that works best for you: Windows Linux MacOS Docker Source install How to install Microsoft SQL Server Head over to the official SQL Server installation guide on Microsoft docs for Windows. If you want to use the cloud version, check out this documentation . Prerequisite For this integration to work, you must make sure that the Microsoft SQL Server has MSDASQL installed, which currently only works on Windows machines. If not, you can download it here . Example dataset In this tutorial, we will use the Medical Cost Personal dataset from Kaggle. The data contains medical information and costs billed by health insurance companies. There are 1338 observations and 7 variables in this dataset: age: age of the primary beneficiary sex: insurance contractor gender \u2013 female, male bmi: body mass index - an objective index of body weight (kg / m2) using the ratio of height to weight, ideally 18.5 to 24.9 children: number of children covered by health insurance / number of dependents smoker: smoker? region: the beneficiary's residential area in the US \u2013 northeast, southeast, southwest, northwest charges: individual medical costs billed by the health insurance. The end result that we will try to achieve is to accurately predict insurance costs per beneficiary. Import dataset to Microsoft SQL Server If you are following this tutorial with your own data, you can skip this section. The first thing we need to do before importing the dataset is to create a new table called insurance : CREATE TABLE insurance ( age int NULL , sex varchar ( 100 ) COLLATE SQL_Latin1_General_CP1_CI_AS NULL , bmi numeric ( 18 , 0 ) NULL , children int NULL , smoker varchar ( 100 ) COLLATE SQL_Latin1_General_CP1_CI_AS NULL , region varchar ( 100 ) COLLATE SQL_Latin1_General_CP1_CI_AS NULL , charges numeric ( 18 , 0 ) NULL ) GO ; To import the dataset there are a few options available. If you are using SQL Management Studio or any other database management GUI, use the import from CSV option. If you are following this tutorial using a SQL client, you can use the BULK INSERT statement: BULK INSERT insurance FROM 'insurance.csv' WITH ( FORMAT = 'CSV' , FIELDQUOTE = '\"' , FIRSTROW = 2 , FIELDTERMINATOR = ',' , -- the insurance.csv delimiter ROWTERMINATOR = '\\n' , TABLOCK ) Now, the data should be inside the insurance table. To make sure it was successfully imported, SELECT one row from the created table: SELECT TOP ( 1 ) * FROM insurance ; Connect MindsDB and Microsoft SQL Server We have installed MindsDB and imported the data, so the next step is to create the integration. When you start MindsDB, it should automatically run the MindsDB Studio (Graphical User Interface) at the http://127.0.0.1:47334/ address in your browser. We can use the Studio to connect MindsDB and Microsoft SQL server: From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select Microsoft SQL Server as the Supported Database. Add the Database name. Add the Host name. Add Port. Add SQL Server user. Add Password for SQL Server user. Click on check connection to test the connection. Click on CONNECT . The next step is to create a new datasource by selecting the data from the insurance table. Click on the NEW DATASOURCE button, and add the SELECT query in the Query input: SELECT * FROM insurance; We have created a new datasource InsuranceData . What we did through the MindsDB Studio can also be done by extending the default configuration of the MindsDB Server. To learn more about how to do that, see the mssql-client docs. Train new model The connection between the MindsDB and Microsoft SQL Server is done, so the next step is to train the model. Training a new model is done by inserting into the mindsdb.predictors table. The required parameters that the INSERT query should have are: name (string) -- The name of the machine learning model that we will train. predict (string) -- The column (feature) that we want to get predictions for. select_data_query (string) -- The SELECT query that will select the data used for training. In this example we will train new model called insurance_model that predicts the charges from the insurance table, so the model training query looks like this: exec ( 'INSERT INTO mindsdb.predictors (name, predict, select_data_query) VALUES (\"insurance_model\", \"charges\", \"SELECT * FROM insurance\")' ) AT mindsdb ; If you are wondering why we are using the exec command to wrap the INSERT statement, that's because it's one of the ways to query a linked server from Microsoft SQL Server. An alternate option is to use openquery , which will execute the specified INSERT query on the server. In this example the query looks like this: INSERT openquery ( mindsdb , 'SELECT name, predict, select_data_query FROM mindsdb.predictors WHERE 1=0' ) VALUES ( 'insurance_model' , 'charges' , 'SELECT * FROM insurance' ); Now, in the background, the training has started. MindsDB will do a black-box analysis and start a process of extracting, analyzing, and transforming the data. The time it takes to train a model differs depending on dataset size, the number of features, feature types, etc. To check if the model training has successfully finished, we can SELECT from the mindsdb.predictors table as: exec ( 'SELECT * FROM mindsdb.predictors' ) AT mindsdb ; The query will return the status of the model as complete , or if the training is still running as training . Make predictions (Query the model) Once the model training finishes we can query that model. Querying the model can be done by executing the SELECT statement from the models AI Table . Let's try and query the insurance_model that we have created. The prediction that we will do is to get insurance for the beneficiary that is 30 years old, has 27.9 as a body mass index and has 1 child. All of these parameters should be added inside the WHERE clause. exec ( 'SELECT charges AS predicted, charges_confidence AS confidence, charges_explain AS info FROM mindsdb.insurance_model WHERE age=30 AND bmi=27.9 AND children=1' ) AT mindsdb ; To get the insurance cost, confidence in that cost, and additional information from MindsDB, we did a SELECT for charges , charges_confidence , and charges_explain . The response from MindsDB should look like this: predic te d | 9421.233985124825 co nf ide n ce | 0.86 i nf o | { \"predicted_value\" : 9421.233985124825 , \"confidence\" : 0.86 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 6603.107461750912 , 12239.36050849874 ], \"important_missing_information\" : [ \"smoker\" ] } MindsDB thinks that for this beneficiary, the insurance cost should be 9,421, with 86% confidence for this prediction. The charges cost interval could vary from 6,603 to 12,239, depending on the beneficiary. As the important information for improving this prediction, MindsDB thinks that providing information about smoker is very important. Conclusion We have seen how easy it is to train and query models directly from a Microsoft SQL database. In short, the flow that we covered was: Connect to Microsoft SQL Server using MindsDB Studio integrations dashboard. Create a new datasource by SELECT ing from the table. Train new model by INSERT ing in mindsdb.predictors table. Get predictions by SELECT ing from the model. If you want to try AI Tables in a different database, check out the other tutorials: AI Tables in MySQL AI Tables in MariaDB AI Tables in PostgreSQL AI Tables in ClickHouse","title":"Automated Machine Learning in Microsoft SQL Server"},{"location":"tutorials/microsoft-sql-server/#automated-machine-learning-in-microsoft-sql-server","text":"Data is the main ingredient for machine learning. Nowadays most enterprise structured data lives inside a database, so moving the machine learning inside the database layer can bring great benefits. This is one of the main reasons we decided to build an integration with the most widely used database systems, so that any database user can create, train and query machine learning models with minimal knowledge of SQL. Using the MindsDB AI Tables feature, users can automate model training workflows and reduce deployment time and complexity by adding the predictive layer to the database. AI Tables allow database users to query predictive models in seconds as if they were data that existed on the table. Simply explained, AI Tables allow users to use machine learning models as if they were normal database tables. In plain SQL, that looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > MindsDB acts like an AutoML platform and interacts with your SQL database through an ODBC driver. AI Tables accelerate development speed, reduce costs and eliminate the complexity of machine learning workflows. Train models from the data inside the SQL database Get predictions through simple SQL queries Get insights into model accuracy Move from idea to production in minutes In this tutorial, you will get step-by-step instructions on how to enable AI Tables in Microsoft SQL Server. Based on a medical insurance example dataset, you will see how to train and query machine learning models by using only SQL statements.","title":"Automated Machine Learning in Microsoft SQL Server"},{"location":"tutorials/microsoft-sql-server/#how-to-install-mindsdb","text":"If you already have MindsDB installed, you can skip this section. MindsDB is a cross-platform tool and can be installed on the most widely used OSs: Windows, Linux and macOS. Also, there are few other options to get MindsDB, such as through Docker, installing from source code or using PyPi. To get started, check the following list, and use the option that works best for you: Windows Linux MacOS Docker Source install","title":"How to install MindsDB"},{"location":"tutorials/microsoft-sql-server/#how-to-install-microsoft-sql-server","text":"Head over to the official SQL Server installation guide on Microsoft docs for Windows. If you want to use the cloud version, check out this documentation .","title":"How to install Microsoft SQL Server"},{"location":"tutorials/microsoft-sql-server/#prerequisite","text":"For this integration to work, you must make sure that the Microsoft SQL Server has MSDASQL installed, which currently only works on Windows machines. If not, you can download it here .","title":"Prerequisite"},{"location":"tutorials/microsoft-sql-server/#example-dataset","text":"In this tutorial, we will use the Medical Cost Personal dataset from Kaggle. The data contains medical information and costs billed by health insurance companies. There are 1338 observations and 7 variables in this dataset: age: age of the primary beneficiary sex: insurance contractor gender \u2013 female, male bmi: body mass index - an objective index of body weight (kg / m2) using the ratio of height to weight, ideally 18.5 to 24.9 children: number of children covered by health insurance / number of dependents smoker: smoker? region: the beneficiary's residential area in the US \u2013 northeast, southeast, southwest, northwest charges: individual medical costs billed by the health insurance. The end result that we will try to achieve is to accurately predict insurance costs per beneficiary.","title":"Example dataset"},{"location":"tutorials/microsoft-sql-server/#import-dataset-to-microsoft-sql-server","text":"If you are following this tutorial with your own data, you can skip this section. The first thing we need to do before importing the dataset is to create a new table called insurance : CREATE TABLE insurance ( age int NULL , sex varchar ( 100 ) COLLATE SQL_Latin1_General_CP1_CI_AS NULL , bmi numeric ( 18 , 0 ) NULL , children int NULL , smoker varchar ( 100 ) COLLATE SQL_Latin1_General_CP1_CI_AS NULL , region varchar ( 100 ) COLLATE SQL_Latin1_General_CP1_CI_AS NULL , charges numeric ( 18 , 0 ) NULL ) GO ; To import the dataset there are a few options available. If you are using SQL Management Studio or any other database management GUI, use the import from CSV option. If you are following this tutorial using a SQL client, you can use the BULK INSERT statement: BULK INSERT insurance FROM 'insurance.csv' WITH ( FORMAT = 'CSV' , FIELDQUOTE = '\"' , FIRSTROW = 2 , FIELDTERMINATOR = ',' , -- the insurance.csv delimiter ROWTERMINATOR = '\\n' , TABLOCK ) Now, the data should be inside the insurance table. To make sure it was successfully imported, SELECT one row from the created table: SELECT TOP ( 1 ) * FROM insurance ;","title":"Import dataset to Microsoft SQL Server"},{"location":"tutorials/microsoft-sql-server/#connect-mindsdb-and-microsoft-sql-server","text":"We have installed MindsDB and imported the data, so the next step is to create the integration. When you start MindsDB, it should automatically run the MindsDB Studio (Graphical User Interface) at the http://127.0.0.1:47334/ address in your browser. We can use the Studio to connect MindsDB and Microsoft SQL server: From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select Microsoft SQL Server as the Supported Database. Add the Database name. Add the Host name. Add Port. Add SQL Server user. Add Password for SQL Server user. Click on check connection to test the connection. Click on CONNECT . The next step is to create a new datasource by selecting the data from the insurance table. Click on the NEW DATASOURCE button, and add the SELECT query in the Query input: SELECT * FROM insurance; We have created a new datasource InsuranceData . What we did through the MindsDB Studio can also be done by extending the default configuration of the MindsDB Server. To learn more about how to do that, see the mssql-client docs.","title":"Connect MindsDB and Microsoft SQL Server"},{"location":"tutorials/microsoft-sql-server/#train-new-model","text":"The connection between the MindsDB and Microsoft SQL Server is done, so the next step is to train the model. Training a new model is done by inserting into the mindsdb.predictors table. The required parameters that the INSERT query should have are: name (string) -- The name of the machine learning model that we will train. predict (string) -- The column (feature) that we want to get predictions for. select_data_query (string) -- The SELECT query that will select the data used for training. In this example we will train new model called insurance_model that predicts the charges from the insurance table, so the model training query looks like this: exec ( 'INSERT INTO mindsdb.predictors (name, predict, select_data_query) VALUES (\"insurance_model\", \"charges\", \"SELECT * FROM insurance\")' ) AT mindsdb ; If you are wondering why we are using the exec command to wrap the INSERT statement, that's because it's one of the ways to query a linked server from Microsoft SQL Server. An alternate option is to use openquery , which will execute the specified INSERT query on the server. In this example the query looks like this: INSERT openquery ( mindsdb , 'SELECT name, predict, select_data_query FROM mindsdb.predictors WHERE 1=0' ) VALUES ( 'insurance_model' , 'charges' , 'SELECT * FROM insurance' ); Now, in the background, the training has started. MindsDB will do a black-box analysis and start a process of extracting, analyzing, and transforming the data. The time it takes to train a model differs depending on dataset size, the number of features, feature types, etc. To check if the model training has successfully finished, we can SELECT from the mindsdb.predictors table as: exec ( 'SELECT * FROM mindsdb.predictors' ) AT mindsdb ; The query will return the status of the model as complete , or if the training is still running as training .","title":"Train new model"},{"location":"tutorials/microsoft-sql-server/#make-predictions-query-the-model","text":"Once the model training finishes we can query that model. Querying the model can be done by executing the SELECT statement from the models AI Table . Let's try and query the insurance_model that we have created. The prediction that we will do is to get insurance for the beneficiary that is 30 years old, has 27.9 as a body mass index and has 1 child. All of these parameters should be added inside the WHERE clause. exec ( 'SELECT charges AS predicted, charges_confidence AS confidence, charges_explain AS info FROM mindsdb.insurance_model WHERE age=30 AND bmi=27.9 AND children=1' ) AT mindsdb ; To get the insurance cost, confidence in that cost, and additional information from MindsDB, we did a SELECT for charges , charges_confidence , and charges_explain . The response from MindsDB should look like this: predic te d | 9421.233985124825 co nf ide n ce | 0.86 i nf o | { \"predicted_value\" : 9421.233985124825 , \"confidence\" : 0.86 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 6603.107461750912 , 12239.36050849874 ], \"important_missing_information\" : [ \"smoker\" ] } MindsDB thinks that for this beneficiary, the insurance cost should be 9,421, with 86% confidence for this prediction. The charges cost interval could vary from 6,603 to 12,239, depending on the beneficiary. As the important information for improving this prediction, MindsDB thinks that providing information about smoker is very important.","title":"Make predictions (Query the model)"},{"location":"tutorials/microsoft-sql-server/#conclusion","text":"We have seen how easy it is to train and query models directly from a Microsoft SQL database. In short, the flow that we covered was: Connect to Microsoft SQL Server using MindsDB Studio integrations dashboard. Create a new datasource by SELECT ing from the table. Train new model by INSERT ing in mindsdb.predictors table. Get predictions by SELECT ing from the model. If you want to try AI Tables in a different database, check out the other tutorials: AI Tables in MySQL AI Tables in MariaDB AI Tables in PostgreSQL AI Tables in ClickHouse","title":"Conclusion"},{"location":"tutorials/mysql/","text":"How to enable Automated Machine Learning in MySQL A database is surely the best place for Machine Learning - because data is the main ingredient of it. And now you can build, train, test & query Machine Learning models using standard SQL queries within a MySQL database! This doesn't require hardcore data science knowledge - the whole Machine Learning workflow is automated. This solution is called AI-Tables and is available in MySQL thanks to integration with an open-source predictive engine from MindsDB. AI-Tables look like normal database tables and return predictions upon being queried as if they were data that exists in the table. In plain SQL, it looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > This video explains how it works: In this tutorial below, you will get step-by-step instructions on how to enable AI-Tables in a MySQL database. Based on a churn prediction example, you see how to build, train and query Machine Learning models by only using SQL statements with MindsDB! How to install MySQL? If you don\u2019t have MySQL installed, you can download the installers for various platforms from the official documentation . Example dataset In this tutorial, we will use the Churn Modelling Dataset . If you have other datasets in your MySQL database, please skip this section. This data set contains details of a bank's customers, and the target variable is a binary variable reflecting whether the customer left the bank (closed their account) or is still a customer. Import dataset to MySQL The first thing we need to do is to import the dataset in MySQL. Create a new table called bank_churn: -- test.bank_churn definition CREATE TABLE test . bank_churn ( CreditScore NUMERIC NULL , Geography varchar ( 100 ) NULL , Gender varchar ( 100 ) NULL , Age NUMERIC NULL , Tenure NUMERIC NULL , Balance NUMERIC NULL , NumOfProducts NUMERIC NULL , HasCrCard NUMERIC NULL , IsActiveMember NUMERIC NULL , EstimatedSalary NUMERIC NULL , Exited NUMERIC NULL ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ; Next, we need to import the data inside the table. There are a few options to do that: Using the Load Data statement: LOAD DATA LOCAL INFILE 'data.csv' INTO bank_churn FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; Using MySQL import: mysqlimport --local --fields-terminated-by=\",\" bank_churn data.csv Using pgAdmin, DBeaver or another SQL client -- just use the \u201cimport from CSV\u201d file option from the navigation menu. Let\u2019s select some data from the bank_churn table to check that the data was successfully imported to MySQL: SELECT * FROM bank_churn LIMIT 1 ; Add Configuration As a prerequisite for using MySQL , we need to enable the Federated Storage engine. Check out the official MySQL documentation to see how to do that. The last step is to create the MindsDB configuration file. MindsDB will try to use the default configuration options like host, port or username for MySQL. If you want to extend these or change the default values, you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_mysql\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"storage_dir\" : \"/storage\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations[default_mysql] -- This key specifies the integration type in this case default_mysql. The required keys are: user(default root) - The MySQL user name. host(default localhost) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsDB will store models and configuration files. Now, we have successfully set up all of the requirements for AI Tables in MySQL. AutoML with AI Tables in MySQL If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that, start the MindsDB server: python3 -m mindsdb --api=mysql --config=config.json The arguments sent to MindsDB are: --api - This tells MindsDB which API should be started (HTTP or MySQL). --config - The path to the configuration file that we have created. If everything works as expected, you should see the following message: Upon successful setup, MindsDB should create a new database called mindsdb. In the mindsDB database, two new tables should be created, called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained models. Train new Machine Learning Model Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict if the bank's customer has left the bank from the bank_churn table, so let\u2019s run an INSERT query as follows: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'churn_model' , 'Exited' , 'SELECT * FROM test.bank_churn' ); This query will create a new model called 'churn_model' and a new table 'churn_model' inside mindsdb database. The required columns (parameters) added in the INSERT for training the predictor are: * name (string) - the name of the predictor. * predict (string) - the feature you want to predict, in this example it will be Exited. * select_data_query (string) - the SELECT query that will get the data from MySQL. To check that the training successfully finished, we can SELECT from the mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; A status of \u201ccomplete\u201d means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as if it were a standard database table. To get the prediction, we need to execute a SELECT query and in the WHERE clause include the when_data as a JSON string that includes feature values, such as CreditScore, EstimatedSalary, Gender, Balance, etc. SELECT * FROM mindsdb . churn_model WHERE when_data = '{\"CreditScore\": \"619\",\"Geography\": \"France\",\"Gender\": \"Female\", \"EstimatedSalary\": 100000, \"Balance\": 0.0, \"Age\":42, \"Tenure\": 2}' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the above customer closed their account with the bank (predicted_value 1) with around 98% confidence. Information in JSON format in the explain column: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"NumOfProducts\" ] } The important_missing_information shows the list of features that MindsDB thinks are quite important for better prediction, in this case, the \"NumOfProducts\". Congratulations, you have successfully trained and queried a Machine Learning Model by only using SQL Statements. Note that even if you used MySQL to build the model, you can still query the same model from the other databases too.","title":"How to enable Automated Machine Learning in MySQL"},{"location":"tutorials/mysql/#how-to-enable-automated-machine-learning-in-mysql","text":"A database is surely the best place for Machine Learning - because data is the main ingredient of it. And now you can build, train, test & query Machine Learning models using standard SQL queries within a MySQL database! This doesn't require hardcore data science knowledge - the whole Machine Learning workflow is automated. This solution is called AI-Tables and is available in MySQL thanks to integration with an open-source predictive engine from MindsDB. AI-Tables look like normal database tables and return predictions upon being queried as if they were data that exists in the table. In plain SQL, it looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > This video explains how it works: In this tutorial below, you will get step-by-step instructions on how to enable AI-Tables in a MySQL database. Based on a churn prediction example, you see how to build, train and query Machine Learning models by only using SQL statements with MindsDB!","title":"How to enable Automated Machine Learning in MySQL"},{"location":"tutorials/mysql/#how-to-install-mysql","text":"If you don\u2019t have MySQL installed, you can download the installers for various platforms from the official documentation .","title":"How to install MySQL?"},{"location":"tutorials/mysql/#example-dataset","text":"In this tutorial, we will use the Churn Modelling Dataset . If you have other datasets in your MySQL database, please skip this section. This data set contains details of a bank's customers, and the target variable is a binary variable reflecting whether the customer left the bank (closed their account) or is still a customer.","title":"Example dataset"},{"location":"tutorials/mysql/#import-dataset-to-mysql","text":"The first thing we need to do is to import the dataset in MySQL. Create a new table called bank_churn: -- test.bank_churn definition CREATE TABLE test . bank_churn ( CreditScore NUMERIC NULL , Geography varchar ( 100 ) NULL , Gender varchar ( 100 ) NULL , Age NUMERIC NULL , Tenure NUMERIC NULL , Balance NUMERIC NULL , NumOfProducts NUMERIC NULL , HasCrCard NUMERIC NULL , IsActiveMember NUMERIC NULL , EstimatedSalary NUMERIC NULL , Exited NUMERIC NULL ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ; Next, we need to import the data inside the table. There are a few options to do that: Using the Load Data statement: LOAD DATA LOCAL INFILE 'data.csv' INTO bank_churn FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; Using MySQL import: mysqlimport --local --fields-terminated-by=\",\" bank_churn data.csv Using pgAdmin, DBeaver or another SQL client -- just use the \u201cimport from CSV\u201d file option from the navigation menu. Let\u2019s select some data from the bank_churn table to check that the data was successfully imported to MySQL: SELECT * FROM bank_churn LIMIT 1 ;","title":"Import dataset to MySQL"},{"location":"tutorials/mysql/#add-configuration","text":"As a prerequisite for using MySQL , we need to enable the Federated Storage engine. Check out the official MySQL documentation to see how to do that. The last step is to create the MindsDB configuration file. MindsDB will try to use the default configuration options like host, port or username for MySQL. If you want to extend these or change the default values, you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_mysql\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"storage_dir\" : \"/storage\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations[default_mysql] -- This key specifies the integration type in this case default_mysql. The required keys are: user(default root) - The MySQL user name. host(default localhost) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsDB will store models and configuration files. Now, we have successfully set up all of the requirements for AI Tables in MySQL.","title":"Add Configuration"},{"location":"tutorials/mysql/#automl-with-ai-tables-in-mysql","text":"If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that, start the MindsDB server: python3 -m mindsdb --api=mysql --config=config.json The arguments sent to MindsDB are: --api - This tells MindsDB which API should be started (HTTP or MySQL). --config - The path to the configuration file that we have created. If everything works as expected, you should see the following message: Upon successful setup, MindsDB should create a new database called mindsdb. In the mindsDB database, two new tables should be created, called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained models.","title":"AutoML with AI Tables in MySQL"},{"location":"tutorials/mysql/#train-new-machine-learning-model","text":"Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict if the bank's customer has left the bank from the bank_churn table, so let\u2019s run an INSERT query as follows: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'churn_model' , 'Exited' , 'SELECT * FROM test.bank_churn' ); This query will create a new model called 'churn_model' and a new table 'churn_model' inside mindsdb database. The required columns (parameters) added in the INSERT for training the predictor are: * name (string) - the name of the predictor. * predict (string) - the feature you want to predict, in this example it will be Exited. * select_data_query (string) - the SELECT query that will get the data from MySQL. To check that the training successfully finished, we can SELECT from the mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; A status of \u201ccomplete\u201d means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as if it were a standard database table. To get the prediction, we need to execute a SELECT query and in the WHERE clause include the when_data as a JSON string that includes feature values, such as CreditScore, EstimatedSalary, Gender, Balance, etc. SELECT * FROM mindsdb . churn_model WHERE when_data = '{\"CreditScore\": \"619\",\"Geography\": \"France\",\"Gender\": \"Female\", \"EstimatedSalary\": 100000, \"Balance\": 0.0, \"Age\":42, \"Tenure\": 2}' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the above customer closed their account with the bank (predicted_value 1) with around 98% confidence. Information in JSON format in the explain column: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"NumOfProducts\" ] } The important_missing_information shows the list of features that MindsDB thinks are quite important for better prediction, in this case, the \"NumOfProducts\". Congratulations, you have successfully trained and queried a Machine Learning Model by only using SQL Statements. Note that even if you used MySQL to build the model, you can still query the same model from the other databases too.","title":"Train new Machine Learning Model"},{"location":"tutorials/postgresql/","text":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries Anyone that has dealt with Machine Learning understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn't it make sense to bring machine learning capabilities straight to the database itself? Bringing Machine Learning to those who know their data best can significantly augment the capacity to solve important problems. To do so, we have developed a concept called AI-Tables. What are AI Tables? AI-Tables differ from normal tables in that they can generate predictions upon being queried and can return such predictions as if they were data that existed in the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > Now, in this tutorial, you will get step-by-step instructions on how to enable AI-Tables in your database and how to build, train and query a Machine Learning model by only using SQL statements! How to install PostgreSQL If you don\u2019t have PostgreSQL installed, you can download the installers for various platforms from the official documentation. Example dataset In this tutorial, we will use the Airline Passenger Satisfaction dataset . If you have other datasets in your PostgreSQL database, please skip this section. This dataset contains airline passenger satisfaction survey data, and we will try to predict passenger satisfaction based on the other factors in the data. Import dataset to PostgreSQL First, let's create a us_consumption table. -- public.airline_passenger_satisfaction definition CREATE TABLE public . airline_passenger_satisfaction ( id numeric NULL , gender varchar NULL , \"Customer Type\" varchar NULL , age numeric NULL , \"Type of Travel\" varchar NULL , \"Class\" varchar NULL , \"Flight Distance\" numeric NULL , \"Inflight wifi service\" numeric NULL , \"Departure/Arrival time convenient\" numeric NULL , \"Ease of Online booking\" numeric NULL , \"Gate location\" numeric NULL , \"Food and drink\" numeric NULL , \"Online boarding\" numeric NULL , \"Seat comfort\" numeric NULL , \"Inflight entertainment\" numeric NULL , \"On-board service\" numeric NULL , \"Leg room service\" numeric NULL , \"Baggage handling\" numeric NULL , \"Checkin service\" numeric NULL , \"Inflight service\" numeric NULL , cleanliness numeric NULL , \"Departure Delay in Minutes\" numeric NULL , \"Arrival Delay in Minutes\" numeric NULL , satisfaction varchar NULL ); After you create the table, you can use the \\copy command to import the data from the CSV file to PostgreSQL: \\copy airline_passenger_satisfaction FROM '/path/to/csv/data.csv' DELIMITER ',' CSV Or, if you are using pgAdmin, DBeaver or another SQL client, just use the import from CSV file option from the navigation menu. To check if the data was successfully imported, execute a SELECT query: SELECT * FROM airline_passenger_satisfaction LIMIT 10 ; Add Configuration We have the data inside PostgreSQL, so the next step is to install the PostgreSQL foreign data wrapper for MySQL. Please check the EnterpriseDB documentation on how to do that. The last step is to create the MindsDB configuration file. MindsDB will try to use the default configuration options like host, port, and username for the PostgreSQL integration. If you want to extend these or change the default values, you need to add a config.json file. Create a new file config.json, and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"publish\" : true , \"host\" : \"127.0.0.1\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"storage_dir\" : \"/storage\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type, in this case default_postgres. The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsDB will store models and configuration. That\u2019s all for setting up the AI Tables in PostgreSQL. AutoML with AI Tables in PostgreSQL If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that, start the MindsDB server: python3 -m mindsdb --api=http,mysql --config=config.json The arguments sent to MindsDB are: --api - This tells MindsDB which API should be started (HTTP, MySQL or both). --config - The path to the configuration file that we have created. If everything works as expected, you should see the following message: Upon successful setup, MindsDB should create a new schema called mindsdb. In the mindsdb schema, two new tables should be created, called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained and in-training models. Train new Machine Learning Model Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict the consumption from the us_consumption table, so let\u2019s run the following INSERT query: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'passenger_satisfaction_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This query will create a new model called 'passenger_satisfaction_model' and a new table 'airline_passenger_satisfaction' inside the mindsdb schema. The required columns (parameters) added in the INSERT for training the predictor are: name (string) - the name of the predictor. predict (string) - the feature you want to predict, in this example it will be satisfaction. select_data_query (string) - the SELECT query that will get the data from PostgreSQL. training_options (dictionary) - optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . To check that the training finished successfully, we can SELECT from the mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'passenger_satisfaction_model' ; A status of \u201ccomplete\u201d means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as if it were a standard database table. To get the prediction, we need to execute a SELECT query andinclude the other features values required, such as Customer Type, Type of Travel, Seat comfort, etc., in the WHERE clause. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND \"Inflight wifi service\" = 5 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the value for consumption rate is around 0.87 with high confidence (97%). There is additional information that we can get back from MindsDB by selecting the explain column from the model: SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; Now, apart from the predicted and confidence values, MindsDB will return additional Information in JSON format in the explain column: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Flight Distance\" , \"Inflight wifi service\" , \"Online boarding\" , \"Baggage handling\" ], \"confidence_composition\" : { \"age\" : 0.935 }, \"extra_insights\" : { \"if_missing\" : [{ \"Customer Type\" : \"satisfied\" }, { \"Type of Travel\" : \"satisfied\" }, { \"Class\" : \"satisfied\" }] } } The confidence_interval shows a possible range of values in which consumption lies. The important_missing_information shows the list of features that MindsDB thinks are quite important for better prediction, in this case, the \u201cFlight Distance\u201d, \"Inflight wifi service\", \"Online boarding\" and \"Baggage handling\". The extra_insights shows a list of rows that we have included in the WHERE clause and shows what the consumption value would be if some of those were missing. Congratulations, you have successfully trained and queried a Machine Learning Model by only using SQL Statements. Note that even if we used PostgreSQL to build the model, you can still query the same model from the other databases too.","title":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries"},{"location":"tutorials/postgresql/#ai-tables-in-postgresql-get-neural-network-based-predictions-using-simple-sql-queries","text":"Anyone that has dealt with Machine Learning understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn't it make sense to bring machine learning capabilities straight to the database itself? Bringing Machine Learning to those who know their data best can significantly augment the capacity to solve important problems. To do so, we have developed a concept called AI-Tables.","title":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries"},{"location":"tutorials/postgresql/#what-are-ai-tables","text":"AI-Tables differ from normal tables in that they can generate predictions upon being queried and can return such predictions as if they were data that existed in the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > Now, in this tutorial, you will get step-by-step instructions on how to enable AI-Tables in your database and how to build, train and query a Machine Learning model by only using SQL statements!","title":"What are AI Tables?"},{"location":"tutorials/postgresql/#how-to-install-postgresql","text":"If you don\u2019t have PostgreSQL installed, you can download the installers for various platforms from the official documentation.","title":"How to install PostgreSQL"},{"location":"tutorials/postgresql/#example-dataset","text":"In this tutorial, we will use the Airline Passenger Satisfaction dataset . If you have other datasets in your PostgreSQL database, please skip this section. This dataset contains airline passenger satisfaction survey data, and we will try to predict passenger satisfaction based on the other factors in the data.","title":"Example dataset"},{"location":"tutorials/postgresql/#import-dataset-to-postgresql","text":"First, let's create a us_consumption table. -- public.airline_passenger_satisfaction definition CREATE TABLE public . airline_passenger_satisfaction ( id numeric NULL , gender varchar NULL , \"Customer Type\" varchar NULL , age numeric NULL , \"Type of Travel\" varchar NULL , \"Class\" varchar NULL , \"Flight Distance\" numeric NULL , \"Inflight wifi service\" numeric NULL , \"Departure/Arrival time convenient\" numeric NULL , \"Ease of Online booking\" numeric NULL , \"Gate location\" numeric NULL , \"Food and drink\" numeric NULL , \"Online boarding\" numeric NULL , \"Seat comfort\" numeric NULL , \"Inflight entertainment\" numeric NULL , \"On-board service\" numeric NULL , \"Leg room service\" numeric NULL , \"Baggage handling\" numeric NULL , \"Checkin service\" numeric NULL , \"Inflight service\" numeric NULL , cleanliness numeric NULL , \"Departure Delay in Minutes\" numeric NULL , \"Arrival Delay in Minutes\" numeric NULL , satisfaction varchar NULL ); After you create the table, you can use the \\copy command to import the data from the CSV file to PostgreSQL: \\copy airline_passenger_satisfaction FROM '/path/to/csv/data.csv' DELIMITER ',' CSV Or, if you are using pgAdmin, DBeaver or another SQL client, just use the import from CSV file option from the navigation menu. To check if the data was successfully imported, execute a SELECT query: SELECT * FROM airline_passenger_satisfaction LIMIT 10 ;","title":"Import dataset to PostgreSQL"},{"location":"tutorials/postgresql/#add-configuration","text":"We have the data inside PostgreSQL, so the next step is to install the PostgreSQL foreign data wrapper for MySQL. Please check the EnterpriseDB documentation on how to do that. The last step is to create the MindsDB configuration file. MindsDB will try to use the default configuration options like host, port, and username for the PostgreSQL integration. If you want to extend these or change the default values, you need to add a config.json file. Create a new file config.json, and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"password\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"publish\" : true , \"host\" : \"127.0.0.1\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"storage_dir\" : \"/storage\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsDB server address. port(default 47334) - The mindsDB server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type, in this case default_postgres. The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsDB will store models and configuration. That\u2019s all for setting up the AI Tables in PostgreSQL.","title":"Add Configuration"},{"location":"tutorials/postgresql/#automl-with-ai-tables-in-postgresql","text":"If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that, start the MindsDB server: python3 -m mindsdb --api=http,mysql --config=config.json The arguments sent to MindsDB are: --api - This tells MindsDB which API should be started (HTTP, MySQL or both). --config - The path to the configuration file that we have created. If everything works as expected, you should see the following message: Upon successful setup, MindsDB should create a new schema called mindsdb. In the mindsdb schema, two new tables should be created, called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained and in-training models.","title":"AutoML with AI Tables in PostgreSQL"},{"location":"tutorials/postgresql/#train-new-machine-learning-model","text":"Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict the consumption from the us_consumption table, so let\u2019s run the following INSERT query: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'passenger_satisfaction_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This query will create a new model called 'passenger_satisfaction_model' and a new table 'airline_passenger_satisfaction' inside the mindsdb schema. The required columns (parameters) added in the INSERT for training the predictor are: name (string) - the name of the predictor. predict (string) - the feature you want to predict, in this example it will be satisfaction. select_data_query (string) - the SELECT query that will get the data from PostgreSQL. training_options (dictionary) - optional value that contains additional training parameters. For a full list of parameters, check the PredictorInterface . To check that the training finished successfully, we can SELECT from the mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'passenger_satisfaction_model' ; A status of \u201ccomplete\u201d means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as if it were a standard database table. To get the prediction, we need to execute a SELECT query andinclude the other features values required, such as Customer Type, Type of Travel, Seat comfort, etc., in the WHERE clause. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND \"Inflight wifi service\" = 5 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the value for consumption rate is around 0.87 with high confidence (97%). There is additional information that we can get back from MindsDB by selecting the explain column from the model: SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; Now, apart from the predicted and confidence values, MindsDB will return additional Information in JSON format in the explain column: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Flight Distance\" , \"Inflight wifi service\" , \"Online boarding\" , \"Baggage handling\" ], \"confidence_composition\" : { \"age\" : 0.935 }, \"extra_insights\" : { \"if_missing\" : [{ \"Customer Type\" : \"satisfied\" }, { \"Type of Travel\" : \"satisfied\" }, { \"Class\" : \"satisfied\" }] } } The confidence_interval shows a possible range of values in which consumption lies. The important_missing_information shows the list of features that MindsDB thinks are quite important for better prediction, in this case, the \u201cFlight Distance\u201d, \"Inflight wifi service\", \"Online boarding\" and \"Baggage handling\". The extra_insights shows a list of rows that we have included in the WHERE clause and shows what the consumption value would be if some of those were missing. Congratulations, you have successfully trained and queried a Machine Learning Model by only using SQL Statements. Note that even if we used PostgreSQL to build the model, you can still query the same model from the other databases too.","title":"Train new Machine Learning Model"}]}